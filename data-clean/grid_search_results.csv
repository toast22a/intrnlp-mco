mean_fit_time,std_fit_time,mean_score_time,std_score_time,param_clf,param_smp,param_vec,param_vec__max_df,param_vec__max_features,param_vec__min_df,params,split0_test_score,split1_test_score,split2_test_score,mean_test_score,std_test_score,rank_test_score,split0_train_score,split1_train_score,split2_train_score,mean_train_score,std_train_score
2.0500503381093345,0.0160764555328101,0.8499966462453207,0.05047453604694861,"MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)",,"CountVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=None, min_df=1,
        ngram_range=(1, 1), preprocessor=None, stop_words=None,
        strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)",1.0,,1,"{'clf': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), 'smp': None, 'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=None, min_df=1,
        ngram_range=(1, 1), preprocessor=None, stop_words=None,
        strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None), 'vec__max_df': 1.0, 'vec__max_features': None, 'vec__min_df': 1}",0.17938469376956648,0.175856850154026,0.177005125370341,0.17741555643131116,0.00146918580304064,63,0.36729782357811774,0.3717110844961206,0.3696582946891588,0.36955573425446575,0.0018031651735519974
1.9232792854309082,0.031751771048548226,0.7639682292938232,0.02371129259990366,"MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)",,"CountVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=None, min_df=1,
        ngram_range=(1, 1), preprocessor=None, stop_words=None,
        strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)",1.0,,0.05,"{'clf': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), 'smp': None, 'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=None, min_df=1,
        ngram_range=(1, 1), preprocessor=None, stop_words=None,
        strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None), 'vec__max_df': 1.0, 'vec__max_features': None, 'vec__min_df': 0.05}",0.1048582991065398,0.10480785829781701,0.10455292281321271,0.10473969340585651,0.0001336625340264031,113,0.10517750857635336,0.105192992259003,0.10474437289034232,0.10503829124189956,0.00020792776653394678
2.0364937782287598,0.20874586983703228,0.8062209288279215,0.009616198427404903,"MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)",,"CountVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=None, min_df=1,
        ngram_range=(1, 1), preprocessor=None, stop_words=None,
        strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)",1.0,500,1,"{'clf': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), 'smp': None, 'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=None, min_df=1,
        ngram_range=(1, 1), preprocessor=None, stop_words=None,
        strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None), 'vec__max_df': 1.0, 'vec__max_features': 500, 'vec__min_df': 1}",0.2186522355103499,0.22512192809745635,0.22225115879692076,0.22200844080157567,0.0026468112339018152,27,0.24898382709411404,0.24532011045416113,0.24232637929603507,0.2455434389481034,0.0027224755289003114
2.104231278101603,0.08410792772579856,0.7400030295054117,0.016402149324384996,"MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)",,"CountVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=None, min_df=1,
        ngram_range=(1, 1), preprocessor=None, stop_words=None,
        strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)",1.0,500,0.05,"{'clf': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), 'smp': None, 'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=None, min_df=1,
        ngram_range=(1, 1), preprocessor=None, stop_words=None,
        strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None), 'vec__max_df': 1.0, 'vec__max_features': 500, 'vec__min_df': 0.05}",0.1048582991065398,0.10480785829781701,0.10455292281321271,0.10473969340585651,0.0001336625340264031,113,0.10517750857635336,0.105192992259003,0.10474437289034232,0.10503829124189956,0.00020792776653394678
2.217850923538208,0.04422780376652583,0.8271067142486572,0.007235201663379061,"MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)",,"CountVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=None, min_df=1,
        ngram_range=(1, 1), preprocessor=None, stop_words=None,
        strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)",1.0,5000,1,"{'clf': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), 'smp': None, 'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=None, min_df=1,
        ngram_range=(1, 1), preprocessor=None, stop_words=None,
        strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None), 'vec__max_df': 1.0, 'vec__max_features': 5000, 'vec__min_df': 1}",0.25083051630305003,0.2544641857426304,0.25255753573144246,0.2526174125923743,0.0014840434243178068,11,0.3850227799481565,0.385238158220282,0.385659719999527,0.3853068860559885,0.0002645320318482358
1.9612342516581218,0.07342305592580388,0.7310121854146322,0.02515587342373667,"MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)",,"CountVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=None, min_df=1,
        ngram_range=(1, 1), preprocessor=None, stop_words=None,
        strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)",1.0,5000,0.05,"{'clf': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), 'smp': None, 'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=None, min_df=1,
        ngram_range=(1, 1), preprocessor=None, stop_words=None,
        strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None), 'vec__max_df': 1.0, 'vec__max_features': 5000, 'vec__min_df': 0.05}",0.1048582991065398,0.10480785829781701,0.10455292281321271,0.10473969340585651,0.0001336625340264031,113,0.10517750857635336,0.105192992259003,0.10474437289034232,0.10503829124189956,0.00020792776653394678
2.0097176233927407,0.05144424870460216,0.8561827341715494,0.021329737112001465,"MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)",,"CountVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=None, min_df=1,
        ngram_range=(1, 1), preprocessor=None, stop_words=None,
        strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)",1.0,50000,1,"{'clf': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), 'smp': None, 'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=None, min_df=1,
        ngram_range=(1, 1), preprocessor=None, stop_words=None,
        strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None), 'vec__max_df': 1.0, 'vec__max_features': 50000, 'vec__min_df': 1}",0.182782720808211,0.1791130570860372,0.1805283866306203,0.18080805484162282,0.0015111295310999777,57,0.36546257234729485,0.37032990849157116,0.36897877582768634,0.36825708555551745,0.0020515632936945203
1.8971044222513835,0.0356471761251728,0.7532913684844971,0.01605657209674312,"MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)",,"CountVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=None, min_df=1,
        ngram_range=(1, 1), preprocessor=None, stop_words=None,
        strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)",1.0,50000,0.05,"{'clf': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), 'smp': None, 'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=None, min_df=1,
        ngram_range=(1, 1), preprocessor=None, stop_words=None,
        strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None), 'vec__max_df': 1.0, 'vec__max_features': 50000, 'vec__min_df': 0.05}",0.1048582991065398,0.10480785829781701,0.10455292281321271,0.10473969340585651,0.0001336625340264031,113,0.10517750857635336,0.105192992259003,0.10474437289034232,0.10503829124189956,0.00020792776653394678
2.01787797609965,0.02016980904500725,0.8097012837727865,0.011419686363488264,"MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)",,"CountVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=None, min_df=1,
        ngram_range=(1, 1), preprocessor=None, stop_words=None,
        strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)",0.95,,1,"{'clf': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), 'smp': None, 'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=None, min_df=1,
        ngram_range=(1, 1), preprocessor=None, stop_words=None,
        strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None), 'vec__max_df': 0.95, 'vec__max_features': None, 'vec__min_df': 1}",0.17938469376956648,0.175856850154026,0.177005125370341,0.17741555643131116,0.00146918580304064,63,0.36729782357811774,0.3717110844961206,0.3696582946891588,0.36955573425446575,0.0018031651735519974
1.9563946723937988,0.06706714706096566,0.7389296690622965,0.022400612325403073,"MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)",,"CountVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=None, min_df=1,
        ngram_range=(1, 1), preprocessor=None, stop_words=None,
        strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)",0.95,,0.05,"{'clf': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), 'smp': None, 'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=None, min_df=1,
        ngram_range=(1, 1), preprocessor=None, stop_words=None,
        strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None), 'vec__max_df': 0.95, 'vec__max_features': None, 'vec__min_df': 0.05}",0.1048582991065398,0.10480785829781701,0.10455292281321271,0.10473969340585651,0.0001336625340264031,113,0.10517750857635336,0.105192992259003,0.10474437289034232,0.10503829124189956,0.00020792776653394678
1.9248392581939697,0.0632848995510436,0.7741453647613525,0.021890744781761173,"MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)",,"CountVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=None, min_df=1,
        ngram_range=(1, 1), preprocessor=None, stop_words=None,
        strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)",0.95,500,1,"{'clf': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), 'smp': None, 'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=None, min_df=1,
        ngram_range=(1, 1), preprocessor=None, stop_words=None,
        strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None), 'vec__max_df': 0.95, 'vec__max_features': 500, 'vec__min_df': 1}",0.2186522355103499,0.22512192809745635,0.22225115879692076,0.22200844080157567,0.0026468112339018152,27,0.24898382709411404,0.24532011045416113,0.24232637929603507,0.2455434389481034,0.0027224755289003114
1.8634052276611328,0.0183309883650583,0.7580092748006185,0.021560532470714534,"MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)",,"CountVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=None, min_df=1,
        ngram_range=(1, 1), preprocessor=None, stop_words=None,
        strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)",0.95,500,0.05,"{'clf': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), 'smp': None, 'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=None, min_df=1,
        ngram_range=(1, 1), preprocessor=None, stop_words=None,
        strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None), 'vec__max_df': 0.95, 'vec__max_features': 500, 'vec__min_df': 0.05}",0.1048582991065398,0.10480785829781701,0.10455292281321271,0.10473969340585651,0.0001336625340264031,113,0.10517750857635336,0.105192992259003,0.10474437289034232,0.10503829124189956,0.00020792776653394678
1.9365325768788655,0.020056112978840617,0.8116180102030436,0.009664340458172185,"MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)",,"CountVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=None, min_df=1,
        ngram_range=(1, 1), preprocessor=None, stop_words=None,
        strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)",0.95,5000,1,"{'clf': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), 'smp': None, 'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=None, min_df=1,
        ngram_range=(1, 1), preprocessor=None, stop_words=None,
        strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None), 'vec__max_df': 0.95, 'vec__max_features': 5000, 'vec__min_df': 1}",0.25083051630305003,0.2544641857426304,0.25255753573144246,0.2526174125923743,0.0014840434243178068,11,0.3850227799481565,0.385238158220282,0.385659719999527,0.3853068860559885,0.0002645320318482358
1.8702778816223145,0.024610461677650446,0.7319934368133545,0.01912266480560051,"MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)",,"CountVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=None, min_df=1,
        ngram_range=(1, 1), preprocessor=None, stop_words=None,
        strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)",0.95,5000,0.05,"{'clf': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), 'smp': None, 'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=None, min_df=1,
        ngram_range=(1, 1), preprocessor=None, stop_words=None,
        strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None), 'vec__max_df': 0.95, 'vec__max_features': 5000, 'vec__min_df': 0.05}",0.1048582991065398,0.10480785829781701,0.10455292281321271,0.10473969340585651,0.0001336625340264031,113,0.10517750857635336,0.105192992259003,0.10474437289034232,0.10503829124189956,0.00020792776653394678
1.9794090588887532,0.03362678311218732,0.8305361270904541,0.02519175917307405,"MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)",,"CountVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=None, min_df=1,
        ngram_range=(1, 1), preprocessor=None, stop_words=None,
        strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)",0.95,50000,1,"{'clf': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), 'smp': None, 'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=None, min_df=1,
        ngram_range=(1, 1), preprocessor=None, stop_words=None,
        strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None), 'vec__max_df': 0.95, 'vec__max_features': 50000, 'vec__min_df': 1}",0.182782720808211,0.1791130570860372,0.1805283866306203,0.18080805484162282,0.0015111295310999777,57,0.36546257234729485,0.37032990849157116,0.36897877582768634,0.36825708555551745,0.0020515632936945203
1.8659547170003254,0.019909478954454808,0.74220871925354,0.022368852065639648,"MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)",,"CountVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=None, min_df=1,
        ngram_range=(1, 1), preprocessor=None, stop_words=None,
        strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)",0.95,50000,0.05,"{'clf': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), 'smp': None, 'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=None, min_df=1,
        ngram_range=(1, 1), preprocessor=None, stop_words=None,
        strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None), 'vec__max_df': 0.95, 'vec__max_features': 50000, 'vec__min_df': 0.05}",0.1048582991065398,0.10480785829781701,0.10455292281321271,0.10473969340585651,0.0001336625340264031,113,0.10517750857635336,0.105192992259003,0.10474437289034232,0.10503829124189956,0.00020792776653394678
2.0307236512502036,0.01927826260437126,0.8430182933807373,0.005787441483295753,"MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)",,"TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=5000, min_df=1,
        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,
        stop_words=None, strip_accents=None, sublinear_tf=False,
        token_pattern='(?u)\\b\\w\\w+\\b', tokenizer=None, use_idf=True,
        vocabulary=None)",1.0,,1,"{'clf': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), 'smp': None, 'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=5000, min_df=1,
        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,
        stop_words=None, strip_accents=None, sublinear_tf=False,
        token_pattern='(?u)\\b\\w\\w+\\b', tokenizer=None, use_idf=True,
        vocabulary=None), 'vec__max_df': 1.0, 'vec__max_features': None, 'vec__min_df': 1}",0.1069663476675592,0.10316728571042515,0.10646853131595152,0.10553405489797862,0.001685853408088696,111,0.15025969115895854,0.14933045372108072,0.15537273646588187,0.1516542937819737,0.002656562007447598
1.9078527291615803,0.00806146692124218,0.7508805592854818,0.018502879487863146,"MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)",,"TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=5000, min_df=1,
        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,
        stop_words=None, strip_accents=None, sublinear_tf=False,
        token_pattern='(?u)\\b\\w\\w+\\b', tokenizer=None, use_idf=True,
        vocabulary=None)",1.0,,0.05,"{'clf': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), 'smp': None, 'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=5000, min_df=1,
        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,
        stop_words=None, strip_accents=None, sublinear_tf=False,
        token_pattern='(?u)\\b\\w\\w+\\b', tokenizer=None, use_idf=True,
        vocabulary=None), 'vec__max_df': 1.0, 'vec__max_features': None, 'vec__min_df': 0.05}",0.09240278471032384,0.08916384133792006,0.09299258767462816,0.09151973790762402,0.0016831821785995532,177,0.09039611907936279,0.09196161201828683,0.09450034158786887,0.09228602422850617,0.001691171795222969
1.884764035542806,0.015350647176710874,0.8519500096638998,0.06880488511886516,"MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)",,"TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=5000, min_df=1,
        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,
        stop_words=None, strip_accents=None, sublinear_tf=False,
        token_pattern='(?u)\\b\\w\\w+\\b', tokenizer=None, use_idf=True,
        vocabulary=None)",1.0,500,1,"{'clf': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), 'smp': None, 'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=5000, min_df=1,
        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,
        stop_words=None, strip_accents=None, sublinear_tf=False,
        token_pattern='(?u)\\b\\w\\w+\\b', tokenizer=None, use_idf=True,
        vocabulary=None), 'vec__max_df': 1.0, 'vec__max_features': 500, 'vec__min_df': 1}",0.18237725855069692,0.18355001437603852,0.18203095274235456,0.18265274188969666,0.0006500283913196811,47,0.19349059245883793,0.1924987862741578,0.18811938086648933,0.19136958653316172,0.0023336377230616127
1.8795783519744873,0.026513334372442018,0.7439024448394775,0.016252605770873254,"MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)",,"TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=5000, min_df=1,
        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,
        stop_words=None, strip_accents=None, sublinear_tf=False,
        token_pattern='(?u)\\b\\w\\w+\\b', tokenizer=None, use_idf=True,
        vocabulary=None)",1.0,500,0.05,"{'clf': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), 'smp': None, 'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=5000, min_df=1,
        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,
        stop_words=None, strip_accents=None, sublinear_tf=False,
        token_pattern='(?u)\\b\\w\\w+\\b', tokenizer=None, use_idf=True,
        vocabulary=None), 'vec__max_df': 1.0, 'vec__max_features': 500, 'vec__min_df': 0.05}",0.09240278471032384,0.08916384133792006,0.09299258767462816,0.09151973790762402,0.0016831821785995532,177,0.09039611907936279,0.09196161201828683,0.09450034158786887,0.09228602422850617,0.001691171795222969
1.947541316350301,0.004638336281739055,0.9132364590962728,0.043949651072086905,"MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)",,"TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=5000, min_df=1,
        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,
        stop_words=None, strip_accents=None, sublinear_tf=False,
        token_pattern='(?u)\\b\\w\\w+\\b', tokenizer=None, use_idf=True,
        vocabulary=None)",1.0,5000,1,"{'clf': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), 'smp': None, 'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=5000, min_df=1,
        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,
        stop_words=None, strip_accents=None, sublinear_tf=False,
        token_pattern='(?u)\\b\\w\\w+\\b', tokenizer=None, use_idf=True,
        vocabulary=None), 'vec__max_df': 1.0, 'vec__max_features': 5000, 'vec__min_df': 1}",0.18768449973645243,0.18695054227075228,0.18510570294319018,0.1865802483167983,0.0010848614149546413,37,0.2383978790718833,0.23683391483239394,0.23794241259436802,0.23772473549954842,0.0006567767372084258
1.9793545405069988,0.015346843902920038,0.769902229309082,0.04145838797867669,"MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)",,"TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=5000, min_df=1,
        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,
        stop_words=None, strip_accents=None, sublinear_tf=False,
        token_pattern='(?u)\\b\\w\\w+\\b', tokenizer=None, use_idf=True,
        vocabulary=None)",1.0,5000,0.05,"{'clf': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), 'smp': None, 'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=5000, min_df=1,
        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,
        stop_words=None, strip_accents=None, sublinear_tf=False,
        token_pattern='(?u)\\b\\w\\w+\\b', tokenizer=None, use_idf=True,
        vocabulary=None), 'vec__max_df': 1.0, 'vec__max_features': 5000, 'vec__min_df': 0.05}",0.09240278471032384,0.08916384133792006,0.09299258767462816,0.09151973790762402,0.0016831821785995532,177,0.09039611907936279,0.09196161201828683,0.09450034158786887,0.09228602422850617,0.001691171795222969
2.0841897328694663,0.021692137335603413,0.8490939935048422,0.019916180600478853,"MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)",,"TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=5000, min_df=1,
        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,
        stop_words=None, strip_accents=None, sublinear_tf=False,
        token_pattern='(?u)\\b\\w\\w+\\b', tokenizer=None, use_idf=True,
        vocabulary=None)",1.0,50000,1,"{'clf': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), 'smp': None, 'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=5000, min_df=1,
        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,
        stop_words=None, strip_accents=None, sublinear_tf=False,
        token_pattern='(?u)\\b\\w\\w+\\b', tokenizer=None, use_idf=True,
        vocabulary=None), 'vec__max_df': 1.0, 'vec__max_features': 50000, 'vec__min_df': 1}",0.108884918148705,0.10553360284698461,0.10795661028952859,0.10745837709507274,0.0014128000094209077,109,0.15214182827344055,0.15151136905825055,0.15707613584669683,0.15357644439279597,0.002488004543120784
1.860942284266154,0.019832422226447797,0.7609851360321045,0.015874586394556345,"MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)",,"TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=5000, min_df=1,
        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,
        stop_words=None, strip_accents=None, sublinear_tf=False,
        token_pattern='(?u)\\b\\w\\w+\\b', tokenizer=None, use_idf=True,
        vocabulary=None)",1.0,50000,0.05,"{'clf': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), 'smp': None, 'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=5000, min_df=1,
        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,
        stop_words=None, strip_accents=None, sublinear_tf=False,
        token_pattern='(?u)\\b\\w\\w+\\b', tokenizer=None, use_idf=True,
        vocabulary=None), 'vec__max_df': 1.0, 'vec__max_features': 50000, 'vec__min_df': 0.05}",0.09240278471032384,0.08916384133792006,0.09299258767462816,0.09151973790762402,0.0016831821785995532,177,0.09039611907936279,0.09196161201828683,0.09450034158786887,0.09228602422850617,0.001691171795222969
2.046044111251831,0.013533393833129723,0.838768720626831,0.011741917276088986,"MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)",,"TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=5000, min_df=1,
        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,
        stop_words=None, strip_accents=None, sublinear_tf=False,
        token_pattern='(?u)\\b\\w\\w+\\b', tokenizer=None, use_idf=True,
        vocabulary=None)",0.95,,1,"{'clf': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), 'smp': None, 'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=5000, min_df=1,
        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,
        stop_words=None, strip_accents=None, sublinear_tf=False,
        token_pattern='(?u)\\b\\w\\w+\\b', tokenizer=None, use_idf=True,
        vocabulary=None), 'vec__max_df': 0.95, 'vec__max_features': None, 'vec__min_df': 1}",0.1069663476675592,0.10316728571042515,0.10646853131595152,0.10553405489797862,0.001685853408088696,111,0.15025969115895854,0.14933045372108072,0.15537273646588187,0.1516542937819737,0.002656562007447598
1.8717984358469646,0.022232846963798826,0.7384495735168457,0.01670385359149847,"MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)",,"TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=5000, min_df=1,
        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,
        stop_words=None, strip_accents=None, sublinear_tf=False,
        token_pattern='(?u)\\b\\w\\w+\\b', tokenizer=None, use_idf=True,
        vocabulary=None)",0.95,,0.05,"{'clf': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), 'smp': None, 'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=5000, min_df=1,
        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,
        stop_words=None, strip_accents=None, sublinear_tf=False,
        token_pattern='(?u)\\b\\w\\w+\\b', tokenizer=None, use_idf=True,
        vocabulary=None), 'vec__max_df': 0.95, 'vec__max_features': None, 'vec__min_df': 0.05}",0.09240278471032384,0.08916384133792006,0.09299258767462816,0.09151973790762402,0.0016831821785995532,177,0.09039611907936279,0.09196161201828683,0.09450034158786887,0.09228602422850617,0.001691171795222969
1.9197537104288738,0.011746705630079445,0.8100330034891764,0.02773712629313541,"MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)",,"TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=5000, min_df=1,
        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,
        stop_words=None, strip_accents=None, sublinear_tf=False,
        token_pattern='(?u)\\b\\w\\w+\\b', tokenizer=None, use_idf=True,
        vocabulary=None)",0.95,500,1,"{'clf': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), 'smp': None, 'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=5000, min_df=1,
        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,
        stop_words=None, strip_accents=None, sublinear_tf=False,
        token_pattern='(?u)\\b\\w\\w+\\b', tokenizer=None, use_idf=True,
        vocabulary=None), 'vec__max_df': 0.95, 'vec__max_features': 500, 'vec__min_df': 1}",0.18237725855069692,0.18355001437603852,0.18203095274235456,0.18265274188969666,0.0006500283913196811,47,0.19349059245883793,0.1924987862741578,0.18811938086648933,0.19136958653316172,0.0023336377230616127
1.8867675463358562,0.008779306727500996,0.7466962337493896,0.019982667408760906,"MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)",,"TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=5000, min_df=1,
        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,
        stop_words=None, strip_accents=None, sublinear_tf=False,
        token_pattern='(?u)\\b\\w\\w+\\b', tokenizer=None, use_idf=True,
        vocabulary=None)",0.95,500,0.05,"{'clf': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), 'smp': None, 'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=5000, min_df=1,
        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,
        stop_words=None, strip_accents=None, sublinear_tf=False,
        token_pattern='(?u)\\b\\w\\w+\\b', tokenizer=None, use_idf=True,
        vocabulary=None), 'vec__max_df': 0.95, 'vec__max_features': 500, 'vec__min_df': 0.05}",0.09240278471032384,0.08916384133792006,0.09299258767462816,0.09151973790762402,0.0016831821785995532,177,0.09039611907936279,0.09196161201828683,0.09450034158786887,0.09228602422850617,0.001691171795222969
1.9359058539072673,0.019914609087358023,0.8167241414388021,0.018780927808751787,"MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)",,"TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=5000, min_df=1,
        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,
        stop_words=None, strip_accents=None, sublinear_tf=False,
        token_pattern='(?u)\\b\\w\\w+\\b', tokenizer=None, use_idf=True,
        vocabulary=None)",0.95,5000,1,"{'clf': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), 'smp': None, 'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=5000, min_df=1,
        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,
        stop_words=None, strip_accents=None, sublinear_tf=False,
        token_pattern='(?u)\\b\\w\\w+\\b', tokenizer=None, use_idf=True,
        vocabulary=None), 'vec__max_df': 0.95, 'vec__max_features': 5000, 'vec__min_df': 1}",0.18768449973645243,0.18695054227075228,0.18510570294319018,0.1865802483167983,0.0010848614149546413,37,0.2383978790718833,0.23683391483239394,0.23794241259436802,0.23772473549954842,0.0006567767372084258
1.9091544946034749,0.0287876351990332,0.7364927927652994,0.021908319955394814,"MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)",,"TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=5000, min_df=1,
        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,
        stop_words=None, strip_accents=None, sublinear_tf=False,
        token_pattern='(?u)\\b\\w\\w+\\b', tokenizer=None, use_idf=True,
        vocabulary=None)",0.95,5000,0.05,"{'clf': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), 'smp': None, 'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=5000, min_df=1,
        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,
        stop_words=None, strip_accents=None, sublinear_tf=False,
        token_pattern='(?u)\\b\\w\\w+\\b', tokenizer=None, use_idf=True,
        vocabulary=None), 'vec__max_df': 0.95, 'vec__max_features': 5000, 'vec__min_df': 0.05}",0.09240278471032384,0.08916384133792006,0.09299258767462816,0.09151973790762402,0.0016831821785995532,177,0.09039611907936279,0.09196161201828683,0.09450034158786887,0.09228602422850617,0.001691171795222969
2.041194756825765,0.012120369873371225,0.8614803949991862,0.008224224692486924,"MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)",,"TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=5000, min_df=1,
        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,
        stop_words=None, strip_accents=None, sublinear_tf=False,
        token_pattern='(?u)\\b\\w\\w+\\b', tokenizer=None, use_idf=True,
        vocabulary=None)",0.95,50000,1,"{'clf': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), 'smp': None, 'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=5000, min_df=1,
        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,
        stop_words=None, strip_accents=None, sublinear_tf=False,
        token_pattern='(?u)\\b\\w\\w+\\b', tokenizer=None, use_idf=True,
        vocabulary=None), 'vec__max_df': 0.95, 'vec__max_features': 50000, 'vec__min_df': 1}",0.108884918148705,0.10553360284698461,0.10795661028952859,0.10745837709507274,0.0014128000094209077,109,0.15214182827344055,0.15151136905825055,0.15707613584669683,0.15357644439279597,0.002488004543120784
1.8997543652852376,0.013021174786151663,0.7585623264312744,0.011874368254152317,"MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)",,"TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=5000, min_df=1,
        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,
        stop_words=None, strip_accents=None, sublinear_tf=False,
        token_pattern='(?u)\\b\\w\\w+\\b', tokenizer=None, use_idf=True,
        vocabulary=None)",0.95,50000,0.05,"{'clf': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), 'smp': None, 'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=5000, min_df=1,
        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,
        stop_words=None, strip_accents=None, sublinear_tf=False,
        token_pattern='(?u)\\b\\w\\w+\\b', tokenizer=None, use_idf=True,
        vocabulary=None), 'vec__max_df': 0.95, 'vec__max_features': 50000, 'vec__min_df': 0.05}",0.09240278471032384,0.08916384133792006,0.09299258767462816,0.09151973790762402,0.0016831821785995532,177,0.09039611907936279,0.09196161201828683,0.09450034158786887,0.09228602422850617,0.001691171795222969
2.03594708442688,0.015696004873972515,0.8382685979207357,0.009729417525611224,"MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)","RandomUnderSampler(random_state=1, ratio=None, replacement=False,
          return_indices=False, sampling_strategy='auto')","CountVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=None, min_df=1,
        ngram_range=(1, 1), preprocessor=None, stop_words=None,
        strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)",1.0,,1,"{'clf': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), 'smp': RandomUnderSampler(random_state=1, ratio=None, replacement=False,
          return_indices=False, sampling_strategy='auto'), 'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=None, min_df=1,
        ngram_range=(1, 1), preprocessor=None, stop_words=None,
        strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None), 'vec__max_df': 1.0, 'vec__max_features': None, 'vec__min_df': 1}",0.17954148896811825,0.18227313950857307,0.1817435320860132,0.18118605352090153,0.0011828117437448302,53,0.3702567854872361,0.3815195612027458,0.3813994878333712,0.37772527817445106,0.0052812493261692895
1.9038280646006267,0.0423705453185655,0.7490206559499105,0.009161110038956876,"MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)","RandomUnderSampler(random_state=1, ratio=None, replacement=False,
          return_indices=False, sampling_strategy='auto')","CountVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=None, min_df=1,
        ngram_range=(1, 1), preprocessor=None, stop_words=None,
        strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)",1.0,,0.05,"{'clf': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), 'smp': RandomUnderSampler(random_state=1, ratio=None, replacement=False,
          return_indices=False, sampling_strategy='auto'), 'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=None, min_df=1,
        ngram_range=(1, 1), preprocessor=None, stop_words=None,
        strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None), 'vec__max_df': 1.0, 'vec__max_features': None, 'vec__min_df': 0.05}",0.09990749258294142,0.09309523362612857,0.10606099771696112,0.0996879079753437,0.005295527840527396,145,0.10161163628883964,0.09763273112305537,0.1084022733776878,0.10254888026319427,0.004446315313018665
1.9144502480824788,0.03584989242367061,0.7878537972768148,0.014551913079461047,"MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)","RandomUnderSampler(random_state=1, ratio=None, replacement=False,
          return_indices=False, sampling_strategy='auto')","CountVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=None, min_df=1,
        ngram_range=(1, 1), preprocessor=None, stop_words=None,
        strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)",1.0,500,1,"{'clf': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), 'smp': RandomUnderSampler(random_state=1, ratio=None, replacement=False,
          return_indices=False, sampling_strategy='auto'), 'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=None, min_df=1,
        ngram_range=(1, 1), preprocessor=None, stop_words=None,
        strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None), 'vec__max_df': 1.0, 'vec__max_features': 500, 'vec__min_df': 1}",0.16462538427740275,0.17046138696189886,0.1661555390969413,0.16708077011208097,0.002470731491804768,79,0.19454753696201801,0.19438512711597386,0.1907798814455418,0.19323751517451124,0.001739073872084242
1.9079859256744385,0.03715164892130259,0.7474303245544434,0.01692401940587655,"MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)","RandomUnderSampler(random_state=1, ratio=None, replacement=False,
          return_indices=False, sampling_strategy='auto')","CountVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=None, min_df=1,
        ngram_range=(1, 1), preprocessor=None, stop_words=None,
        strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)",1.0,500,0.05,"{'clf': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), 'smp': RandomUnderSampler(random_state=1, ratio=None, replacement=False,
          return_indices=False, sampling_strategy='auto'), 'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=None, min_df=1,
        ngram_range=(1, 1), preprocessor=None, stop_words=None,
        strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None), 'vec__max_df': 1.0, 'vec__max_features': 500, 'vec__min_df': 0.05}",0.09990749258294142,0.09309523362612857,0.10606099771696112,0.0996879079753437,0.005295527840527396,145,0.10161163628883964,0.09763273112305537,0.1084022733776878,0.10254888026319427,0.004446315313018665
1.9365267753601074,0.03959063066358012,0.818783680597941,0.0033359751734075363,"MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)","RandomUnderSampler(random_state=1, ratio=None, replacement=False,
          return_indices=False, sampling_strategy='auto')","CountVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=None, min_df=1,
        ngram_range=(1, 1), preprocessor=None, stop_words=None,
        strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)",1.0,5000,1,"{'clf': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), 'smp': RandomUnderSampler(random_state=1, ratio=None, replacement=False,
          return_indices=False, sampling_strategy='auto'), 'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=None, min_df=1,
        ngram_range=(1, 1), preprocessor=None, stop_words=None,
        strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None), 'vec__max_df': 1.0, 'vec__max_features': 5000, 'vec__min_df': 1}",0.1860901581795898,0.18707835227978822,0.18489948851092905,0.18602266632343567,0.000890796719720478,39,0.29657403763040413,0.30049917541712895,0.2975869096341051,0.2982200408938794,0.001663794469532677
1.9232715765635173,0.02189098468348099,0.7552047570546468,0.011911594697898119,"MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)","RandomUnderSampler(random_state=1, ratio=None, replacement=False,
          return_indices=False, sampling_strategy='auto')","CountVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=None, min_df=1,
        ngram_range=(1, 1), preprocessor=None, stop_words=None,
        strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)",1.0,5000,0.05,"{'clf': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), 'smp': RandomUnderSampler(random_state=1, ratio=None, replacement=False,
          return_indices=False, sampling_strategy='auto'), 'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=None, min_df=1,
        ngram_range=(1, 1), preprocessor=None, stop_words=None,
        strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None), 'vec__max_df': 1.0, 'vec__max_features': 5000, 'vec__min_df': 0.05}",0.09990749258294142,0.09309523362612857,0.10606099771696112,0.0996879079753437,0.005295527840527396,145,0.10161163628883964,0.09763273112305537,0.1084022733776878,0.10254888026319427,0.004446315313018665
2.0072964827219644,0.03267287772909728,0.8449602127075195,0.01827448968262413,"MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)","RandomUnderSampler(random_state=1, ratio=None, replacement=False,
          return_indices=False, sampling_strategy='auto')","CountVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=None, min_df=1,
        ngram_range=(1, 1), preprocessor=None, stop_words=None,
        strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)",1.0,50000,1,"{'clf': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), 'smp': RandomUnderSampler(random_state=1, ratio=None, replacement=False,
          return_indices=False, sampling_strategy='auto'), 'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=None, min_df=1,
        ngram_range=(1, 1), preprocessor=None, stop_words=None,
        strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None), 'vec__max_df': 1.0, 'vec__max_features': 50000, 'vec__min_df': 1}",0.1794728981333747,0.18210571462141947,0.18177724640917192,0.18111861972132204,0.0011714015822626626,55,0.3665056092595087,0.3780562105021281,0.3781285761506925,0.3742301319707764,0.005462142286346471
1.873961369196574,0.018780623075432164,0.7493369579315186,0.010906226721913315,"MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)","RandomUnderSampler(random_state=1, ratio=None, replacement=False,
          return_indices=False, sampling_strategy='auto')","CountVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=None, min_df=1,
        ngram_range=(1, 1), preprocessor=None, stop_words=None,
        strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)",1.0,50000,0.05,"{'clf': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), 'smp': RandomUnderSampler(random_state=1, ratio=None, replacement=False,
          return_indices=False, sampling_strategy='auto'), 'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=None, min_df=1,
        ngram_range=(1, 1), preprocessor=None, stop_words=None,
        strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None), 'vec__max_df': 1.0, 'vec__max_features': 50000, 'vec__min_df': 0.05}",0.09990749258294142,0.09309523362612857,0.10606099771696112,0.0996879079753437,0.005295527840527396,145,0.10161163628883964,0.09763273112305537,0.1084022733776878,0.10254888026319427,0.004446315313018665
2.022885322570801,0.016058406047400248,0.8270049889882406,0.011031217061659358,"MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)","RandomUnderSampler(random_state=1, ratio=None, replacement=False,
          return_indices=False, sampling_strategy='auto')","CountVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=None, min_df=1,
        ngram_range=(1, 1), preprocessor=None, stop_words=None,
        strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)",0.95,,1,"{'clf': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), 'smp': RandomUnderSampler(random_state=1, ratio=None, replacement=False,
          return_indices=False, sampling_strategy='auto'), 'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=None, min_df=1,
        ngram_range=(1, 1), preprocessor=None, stop_words=None,
        strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None), 'vec__max_df': 0.95, 'vec__max_features': None, 'vec__min_df': 1}",0.17954148896811825,0.18227313950857307,0.1817435320860132,0.18118605352090153,0.0011828117437448302,53,0.3702567854872361,0.3815195612027458,0.3813994878333712,0.37772527817445106,0.0052812493261692895
1.9250400066375732,0.012240017644525297,0.7523113091786703,0.021680412412883213,"MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)","RandomUnderSampler(random_state=1, ratio=None, replacement=False,
          return_indices=False, sampling_strategy='auto')","CountVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=None, min_df=1,
        ngram_range=(1, 1), preprocessor=None, stop_words=None,
        strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)",0.95,,0.05,"{'clf': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), 'smp': RandomUnderSampler(random_state=1, ratio=None, replacement=False,
          return_indices=False, sampling_strategy='auto'), 'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=None, min_df=1,
        ngram_range=(1, 1), preprocessor=None, stop_words=None,
        strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None), 'vec__max_df': 0.95, 'vec__max_features': None, 'vec__min_df': 0.05}",0.09990749258294142,0.09309523362612857,0.10606099771696112,0.0996879079753437,0.005295527840527396,145,0.10161163628883964,0.09763273112305537,0.1084022733776878,0.10254888026319427,0.004446315313018665
1.9175678094228108,0.023766391088740763,0.8219872315724691,0.0347047494012997,"MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)","RandomUnderSampler(random_state=1, ratio=None, replacement=False,
          return_indices=False, sampling_strategy='auto')","CountVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=None, min_df=1,
        ngram_range=(1, 1), preprocessor=None, stop_words=None,
        strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)",0.95,500,1,"{'clf': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), 'smp': RandomUnderSampler(random_state=1, ratio=None, replacement=False,
          return_indices=False, sampling_strategy='auto'), 'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=None, min_df=1,
        ngram_range=(1, 1), preprocessor=None, stop_words=None,
        strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None), 'vec__max_df': 0.95, 'vec__max_features': 500, 'vec__min_df': 1}",0.16462538427740275,0.17046138696189886,0.1661555390969413,0.16708077011208097,0.002470731491804768,79,0.19454753696201801,0.19438512711597386,0.1907798814455418,0.19323751517451124,0.001739073872084242
1.9170348644256592,0.028237538247409995,0.7502254645029703,0.015602573017328347,"MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)","RandomUnderSampler(random_state=1, ratio=None, replacement=False,
          return_indices=False, sampling_strategy='auto')","CountVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=None, min_df=1,
        ngram_range=(1, 1), preprocessor=None, stop_words=None,
        strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)",0.95,500,0.05,"{'clf': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), 'smp': RandomUnderSampler(random_state=1, ratio=None, replacement=False,
          return_indices=False, sampling_strategy='auto'), 'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=None, min_df=1,
        ngram_range=(1, 1), preprocessor=None, stop_words=None,
        strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None), 'vec__max_df': 0.95, 'vec__max_features': 500, 'vec__min_df': 0.05}",0.09990749258294142,0.09309523362612857,0.10606099771696112,0.0996879079753437,0.005295527840527396,145,0.10161163628883964,0.09763273112305537,0.1084022733776878,0.10254888026319427,0.004446315313018665
2.0149384339650473,0.012438762034303155,0.9494746526082357,0.025462625979407787,"MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)","RandomUnderSampler(random_state=1, ratio=None, replacement=False,
          return_indices=False, sampling_strategy='auto')","CountVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=None, min_df=1,
        ngram_range=(1, 1), preprocessor=None, stop_words=None,
        strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)",0.95,5000,1,"{'clf': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), 'smp': RandomUnderSampler(random_state=1, ratio=None, replacement=False,
          return_indices=False, sampling_strategy='auto'), 'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=None, min_df=1,
        ngram_range=(1, 1), preprocessor=None, stop_words=None,
        strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None), 'vec__max_df': 0.95, 'vec__max_features': 5000, 'vec__min_df': 1}",0.1860901581795898,0.18707835227978822,0.18489948851092905,0.18602266632343567,0.000890796719720478,39,0.29657403763040413,0.30049917541712895,0.2975869096341051,0.2982200408938794,0.001663794469532677
1.9838303724924724,0.03366856023925103,0.7627219359079996,0.021341321221259684,"MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)","RandomUnderSampler(random_state=1, ratio=None, replacement=False,
          return_indices=False, sampling_strategy='auto')","CountVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=None, min_df=1,
        ngram_range=(1, 1), preprocessor=None, stop_words=None,
        strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)",0.95,5000,0.05,"{'clf': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), 'smp': RandomUnderSampler(random_state=1, ratio=None, replacement=False,
          return_indices=False, sampling_strategy='auto'), 'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=None, min_df=1,
        ngram_range=(1, 1), preprocessor=None, stop_words=None,
        strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None), 'vec__max_df': 0.95, 'vec__max_features': 5000, 'vec__min_df': 0.05}",0.09990749258294142,0.09309523362612857,0.10606099771696112,0.0996879079753437,0.005295527840527396,145,0.10161163628883964,0.09763273112305537,0.1084022733776878,0.10254888026319427,0.004446315313018665
2.0375332832336426,0.018115995416116002,0.8356673717498779,0.029099730049541927,"MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)","RandomUnderSampler(random_state=1, ratio=None, replacement=False,
          return_indices=False, sampling_strategy='auto')","CountVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=None, min_df=1,
        ngram_range=(1, 1), preprocessor=None, stop_words=None,
        strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)",0.95,50000,1,"{'clf': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), 'smp': RandomUnderSampler(random_state=1, ratio=None, replacement=False,
          return_indices=False, sampling_strategy='auto'), 'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=None, min_df=1,
        ngram_range=(1, 1), preprocessor=None, stop_words=None,
        strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None), 'vec__max_df': 0.95, 'vec__max_features': 50000, 'vec__min_df': 1}",0.1794728981333747,0.18210571462141947,0.18177724640917192,0.18111861972132204,0.0011714015822626626,55,0.3665056092595087,0.3780562105021281,0.3781285761506925,0.3742301319707764,0.005462142286346471
1.9007553259531658,0.01809680851581834,0.755845308303833,0.023702573813449132,"MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)","RandomUnderSampler(random_state=1, ratio=None, replacement=False,
          return_indices=False, sampling_strategy='auto')","CountVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=None, min_df=1,
        ngram_range=(1, 1), preprocessor=None, stop_words=None,
        strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)",0.95,50000,0.05,"{'clf': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), 'smp': RandomUnderSampler(random_state=1, ratio=None, replacement=False,
          return_indices=False, sampling_strategy='auto'), 'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=None, min_df=1,
        ngram_range=(1, 1), preprocessor=None, stop_words=None,
        strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None), 'vec__max_df': 0.95, 'vec__max_features': 50000, 'vec__min_df': 0.05}",0.09990749258294142,0.09309523362612857,0.10606099771696112,0.0996879079753437,0.005295527840527396,145,0.10161163628883964,0.09763273112305537,0.1084022733776878,0.10254888026319427,0.004446315313018665
2.09476645787557,0.03251684505359411,0.8875822226206461,0.005712209251235799,"MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)","RandomUnderSampler(random_state=1, ratio=None, replacement=False,
          return_indices=False, sampling_strategy='auto')","TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=5000, min_df=1,
        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,
        stop_words=None, strip_accents=None, sublinear_tf=False,
        token_pattern='(?u)\\b\\w\\w+\\b', tokenizer=None, use_idf=True,
        vocabulary=None)",1.0,,1,"{'clf': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), 'smp': RandomUnderSampler(random_state=1, ratio=None, replacement=False,
          return_indices=False, sampling_strategy='auto'), 'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=5000, min_df=1,
        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,
        stop_words=None, strip_accents=None, sublinear_tf=False,
        token_pattern='(?u)\\b\\w\\w+\\b', tokenizer=None, use_idf=True,
        vocabulary=None), 'vec__max_df': 1.0, 'vec__max_features': None, 'vec__min_df': 1}",0.18043134156792945,0.18525297340682995,0.18504021202963702,0.1835748423347988,0.0022244871550370793,45,0.3880435450852455,0.4025466765792009,0.40040534503708974,0.3969985222338454,0.006392184654835382
1.9736737410227458,0.013096296360164099,0.8250676790873209,0.01631390417177862,"MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)","RandomUnderSampler(random_state=1, ratio=None, replacement=False,
          return_indices=False, sampling_strategy='auto')","TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=5000, min_df=1,
        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,
        stop_words=None, strip_accents=None, sublinear_tf=False,
        token_pattern='(?u)\\b\\w\\w+\\b', tokenizer=None, use_idf=True,
        vocabulary=None)",1.0,,0.05,"{'clf': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), 'smp': RandomUnderSampler(random_state=1, ratio=None, replacement=False,
          return_indices=False, sampling_strategy='auto'), 'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=5000, min_df=1,
        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,
        stop_words=None, strip_accents=None, sublinear_tf=False,
        token_pattern='(?u)\\b\\w\\w+\\b', tokenizer=None, use_idf=True,
        vocabulary=None), 'vec__max_df': 1.0, 'vec__max_features': None, 'vec__min_df': 0.05}",0.09654976281655568,0.08119435692489634,0.10320548010392323,0.09364986661512507,0.009216992798987368,161,0.09913509648857988,0.08572041236436952,0.1037317033069519,0.09619573738663377,0.007641183342137205
1.9723777770996094,0.02552210686451872,0.8825228214263916,0.08929312655496395,"MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)","RandomUnderSampler(random_state=1, ratio=None, replacement=False,
          return_indices=False, sampling_strategy='auto')","TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=5000, min_df=1,
        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,
        stop_words=None, strip_accents=None, sublinear_tf=False,
        token_pattern='(?u)\\b\\w\\w+\\b', tokenizer=None, use_idf=True,
        vocabulary=None)",1.0,500,1,"{'clf': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), 'smp': RandomUnderSampler(random_state=1, ratio=None, replacement=False,
          return_indices=False, sampling_strategy='auto'), 'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=5000, min_df=1,
        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,
        stop_words=None, strip_accents=None, sublinear_tf=False,
        token_pattern='(?u)\\b\\w\\w+\\b', tokenizer=None, use_idf=True,
        vocabulary=None), 'vec__max_df': 1.0, 'vec__max_features': 500, 'vec__min_df': 1}",0.16091711854813745,0.16940295077294815,0.16397452266086165,0.1647648639939824,0.0035091135351669883,87,0.19156483449526107,0.1920849816524466,0.18867044126067498,0.19077341913612755,0.0015021152252217152
1.8951857089996338,0.0182458906140506,0.7571773529052734,0.012548078046002538,"MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)","RandomUnderSampler(random_state=1, ratio=None, replacement=False,
          return_indices=False, sampling_strategy='auto')","TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=5000, min_df=1,
        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,
        stop_words=None, strip_accents=None, sublinear_tf=False,
        token_pattern='(?u)\\b\\w\\w+\\b', tokenizer=None, use_idf=True,
        vocabulary=None)",1.0,500,0.05,"{'clf': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), 'smp': RandomUnderSampler(random_state=1, ratio=None, replacement=False,
          return_indices=False, sampling_strategy='auto'), 'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=5000, min_df=1,
        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,
        stop_words=None, strip_accents=None, sublinear_tf=False,
        token_pattern='(?u)\\b\\w\\w+\\b', tokenizer=None, use_idf=True,
        vocabulary=None), 'vec__max_df': 1.0, 'vec__max_features': 500, 'vec__min_df': 0.05}",0.09654976281655568,0.08119435692489634,0.10320548010392323,0.09364986661512507,0.009216992798987368,161,0.09913509648857988,0.08572041236436952,0.1037317033069519,0.09619573738663377,0.007641183342137205
1.9654429753621419,0.010381729256312758,0.8255489667256674,0.00823320594101523,"MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)","RandomUnderSampler(random_state=1, ratio=None, replacement=False,
          return_indices=False, sampling_strategy='auto')","TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=5000, min_df=1,
        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,
        stop_words=None, strip_accents=None, sublinear_tf=False,
        token_pattern='(?u)\\b\\w\\w+\\b', tokenizer=None, use_idf=True,
        vocabulary=None)",1.0,5000,1,"{'clf': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), 'smp': RandomUnderSampler(random_state=1, ratio=None, replacement=False,
          return_indices=False, sampling_strategy='auto'), 'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=5000, min_df=1,
        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,
        stop_words=None, strip_accents=None, sublinear_tf=False,
        token_pattern='(?u)\\b\\w\\w+\\b', tokenizer=None, use_idf=True,
        vocabulary=None), 'vec__max_df': 1.0, 'vec__max_features': 5000, 'vec__min_df': 1}",0.18509134893357118,0.1883202932558495,0.18214378588645244,0.1851851426919577,0.0025224206327637308,41,0.29911837225446214,0.3039742120442461,0.30039387235905485,0.3011621522192544,0.0020554781234063844
1.913427432378133,0.0037808390125815797,0.7475190162658691,0.01837288114936807,"MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)","RandomUnderSampler(random_state=1, ratio=None, replacement=False,
          return_indices=False, sampling_strategy='auto')","TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=5000, min_df=1,
        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,
        stop_words=None, strip_accents=None, sublinear_tf=False,
        token_pattern='(?u)\\b\\w\\w+\\b', tokenizer=None, use_idf=True,
        vocabulary=None)",1.0,5000,0.05,"{'clf': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), 'smp': RandomUnderSampler(random_state=1, ratio=None, replacement=False,
          return_indices=False, sampling_strategy='auto'), 'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=5000, min_df=1,
        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,
        stop_words=None, strip_accents=None, sublinear_tf=False,
        token_pattern='(?u)\\b\\w\\w+\\b', tokenizer=None, use_idf=True,
        vocabulary=None), 'vec__max_df': 1.0, 'vec__max_features': 5000, 'vec__min_df': 0.05}",0.09654976281655568,0.08119435692489634,0.10320548010392323,0.09364986661512507,0.009216992798987368,161,0.09913509648857988,0.08572041236436952,0.1037317033069519,0.09619573738663377,0.007641183342137205
2.0687511761983237,0.023657836634428605,0.8456683953603109,0.025666302655398987,"MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)","RandomUnderSampler(random_state=1, ratio=None, replacement=False,
          return_indices=False, sampling_strategy='auto')","TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=5000, min_df=1,
        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,
        stop_words=None, strip_accents=None, sublinear_tf=False,
        token_pattern='(?u)\\b\\w\\w+\\b', tokenizer=None, use_idf=True,
        vocabulary=None)",1.0,50000,1,"{'clf': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), 'smp': RandomUnderSampler(random_state=1, ratio=None, replacement=False,
          return_indices=False, sampling_strategy='auto'), 'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=5000, min_df=1,
        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,
        stop_words=None, strip_accents=None, sublinear_tf=False,
        token_pattern='(?u)\\b\\w\\w+\\b', tokenizer=None, use_idf=True,
        vocabulary=None), 'vec__max_df': 1.0, 'vec__max_features': 50000, 'vec__min_df': 1}",0.18100652691227945,0.18579723630962594,0.18463425142247883,0.18381267154812808,0.002040256397271293,43,0.38381174403212265,0.39886431701570185,0.3973876312220993,0.39335456408997455,0.006774669055910312
1.9097516536712646,0.026740493691278802,0.7467319965362549,0.020611434079763395,"MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)","RandomUnderSampler(random_state=1, ratio=None, replacement=False,
          return_indices=False, sampling_strategy='auto')","TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=5000, min_df=1,
        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,
        stop_words=None, strip_accents=None, sublinear_tf=False,
        token_pattern='(?u)\\b\\w\\w+\\b', tokenizer=None, use_idf=True,
        vocabulary=None)",1.0,50000,0.05,"{'clf': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), 'smp': RandomUnderSampler(random_state=1, ratio=None, replacement=False,
          return_indices=False, sampling_strategy='auto'), 'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=5000, min_df=1,
        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,
        stop_words=None, strip_accents=None, sublinear_tf=False,
        token_pattern='(?u)\\b\\w\\w+\\b', tokenizer=None, use_idf=True,
        vocabulary=None), 'vec__max_df': 1.0, 'vec__max_features': 50000, 'vec__min_df': 0.05}",0.09654976281655568,0.08119435692489634,0.10320548010392323,0.09364986661512507,0.009216992798987368,161,0.09913509648857988,0.08572041236436952,0.1037317033069519,0.09619573738663377,0.007641183342137205
2.0616699854532876,0.020245054604967574,0.8579544226328532,0.020401382326566856,"MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)","RandomUnderSampler(random_state=1, ratio=None, replacement=False,
          return_indices=False, sampling_strategy='auto')","TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=5000, min_df=1,
        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,
        stop_words=None, strip_accents=None, sublinear_tf=False,
        token_pattern='(?u)\\b\\w\\w+\\b', tokenizer=None, use_idf=True,
        vocabulary=None)",0.95,,1,"{'clf': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), 'smp': RandomUnderSampler(random_state=1, ratio=None, replacement=False,
          return_indices=False, sampling_strategy='auto'), 'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=5000, min_df=1,
        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,
        stop_words=None, strip_accents=None, sublinear_tf=False,
        token_pattern='(?u)\\b\\w\\w+\\b', tokenizer=None, use_idf=True,
        vocabulary=None), 'vec__max_df': 0.95, 'vec__max_features': None, 'vec__min_df': 1}",0.18043134156792945,0.18525297340682995,0.18504021202963702,0.1835748423347988,0.0022244871550370793,45,0.3880435450852455,0.4025466765792009,0.40040534503708974,0.3969985222338454,0.006392184654835382
1.9145995775858562,0.0074643201341985065,0.7418700853983561,0.027813914115468262,"MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)","RandomUnderSampler(random_state=1, ratio=None, replacement=False,
          return_indices=False, sampling_strategy='auto')","TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=5000, min_df=1,
        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,
        stop_words=None, strip_accents=None, sublinear_tf=False,
        token_pattern='(?u)\\b\\w\\w+\\b', tokenizer=None, use_idf=True,
        vocabulary=None)",0.95,,0.05,"{'clf': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), 'smp': RandomUnderSampler(random_state=1, ratio=None, replacement=False,
          return_indices=False, sampling_strategy='auto'), 'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=5000, min_df=1,
        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,
        stop_words=None, strip_accents=None, sublinear_tf=False,
        token_pattern='(?u)\\b\\w\\w+\\b', tokenizer=None, use_idf=True,
        vocabulary=None), 'vec__max_df': 0.95, 'vec__max_features': None, 'vec__min_df': 0.05}",0.09654976281655568,0.08119435692489634,0.10320548010392323,0.09364986661512507,0.009216992798987368,161,0.09913509648857988,0.08572041236436952,0.1037317033069519,0.09619573738663377,0.007641183342137205
1.9451313018798828,0.021606187516137864,0.7913185755411783,0.031596884724892194,"MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)","RandomUnderSampler(random_state=1, ratio=None, replacement=False,
          return_indices=False, sampling_strategy='auto')","TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=5000, min_df=1,
        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,
        stop_words=None, strip_accents=None, sublinear_tf=False,
        token_pattern='(?u)\\b\\w\\w+\\b', tokenizer=None, use_idf=True,
        vocabulary=None)",0.95,500,1,"{'clf': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), 'smp': RandomUnderSampler(random_state=1, ratio=None, replacement=False,
          return_indices=False, sampling_strategy='auto'), 'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=5000, min_df=1,
        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,
        stop_words=None, strip_accents=None, sublinear_tf=False,
        token_pattern='(?u)\\b\\w\\w+\\b', tokenizer=None, use_idf=True,
        vocabulary=None), 'vec__max_df': 0.95, 'vec__max_features': 500, 'vec__min_df': 1}",0.16091711854813745,0.16940295077294815,0.16397452266086165,0.1647648639939824,0.0035091135351669883,87,0.19156483449526107,0.1920849816524466,0.18867044126067498,0.19077341913612755,0.0015021152252217152
1.8889133135477703,0.030833146719969003,0.7525790532430013,0.02597204553992088,"MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)","RandomUnderSampler(random_state=1, ratio=None, replacement=False,
          return_indices=False, sampling_strategy='auto')","TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=5000, min_df=1,
        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,
        stop_words=None, strip_accents=None, sublinear_tf=False,
        token_pattern='(?u)\\b\\w\\w+\\b', tokenizer=None, use_idf=True,
        vocabulary=None)",0.95,500,0.05,"{'clf': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), 'smp': RandomUnderSampler(random_state=1, ratio=None, replacement=False,
          return_indices=False, sampling_strategy='auto'), 'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=5000, min_df=1,
        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,
        stop_words=None, strip_accents=None, sublinear_tf=False,
        token_pattern='(?u)\\b\\w\\w+\\b', tokenizer=None, use_idf=True,
        vocabulary=None), 'vec__max_df': 0.95, 'vec__max_features': 500, 'vec__min_df': 0.05}",0.09654976281655568,0.08119435692489634,0.10320548010392323,0.09364986661512507,0.009216992798987368,161,0.09913509648857988,0.08572041236436952,0.1037317033069519,0.09619573738663377,0.007641183342137205
1.9438703854878743,0.03892404020327013,0.8095324039459229,0.011066369870527828,"MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)","RandomUnderSampler(random_state=1, ratio=None, replacement=False,
          return_indices=False, sampling_strategy='auto')","TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=5000, min_df=1,
        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,
        stop_words=None, strip_accents=None, sublinear_tf=False,
        token_pattern='(?u)\\b\\w\\w+\\b', tokenizer=None, use_idf=True,
        vocabulary=None)",0.95,5000,1,"{'clf': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), 'smp': RandomUnderSampler(random_state=1, ratio=None, replacement=False,
          return_indices=False, sampling_strategy='auto'), 'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=5000, min_df=1,
        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,
        stop_words=None, strip_accents=None, sublinear_tf=False,
        token_pattern='(?u)\\b\\w\\w+\\b', tokenizer=None, use_idf=True,
        vocabulary=None), 'vec__max_df': 0.95, 'vec__max_features': 5000, 'vec__min_df': 1}",0.18509134893357118,0.1883202932558495,0.18214378588645244,0.1851851426919577,0.0025224206327637308,41,0.29911837225446214,0.3039742120442461,0.30039387235905485,0.3011621522192544,0.0020554781234063844
1.9138163725535076,0.018678112472831666,0.7606436411539713,0.019636053996224017,"MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)","RandomUnderSampler(random_state=1, ratio=None, replacement=False,
          return_indices=False, sampling_strategy='auto')","TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=5000, min_df=1,
        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,
        stop_words=None, strip_accents=None, sublinear_tf=False,
        token_pattern='(?u)\\b\\w\\w+\\b', tokenizer=None, use_idf=True,
        vocabulary=None)",0.95,5000,0.05,"{'clf': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), 'smp': RandomUnderSampler(random_state=1, ratio=None, replacement=False,
          return_indices=False, sampling_strategy='auto'), 'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=5000, min_df=1,
        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,
        stop_words=None, strip_accents=None, sublinear_tf=False,
        token_pattern='(?u)\\b\\w\\w+\\b', tokenizer=None, use_idf=True,
        vocabulary=None), 'vec__max_df': 0.95, 'vec__max_features': 5000, 'vec__min_df': 0.05}",0.09654976281655568,0.08119435692489634,0.10320548010392323,0.09364986661512507,0.009216992798987368,161,0.09913509648857988,0.08572041236436952,0.1037317033069519,0.09619573738663377,0.007641183342137205
2.0358407497406006,0.05957210089399413,0.8609773317972819,0.007145101280534024,"MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)","RandomUnderSampler(random_state=1, ratio=None, replacement=False,
          return_indices=False, sampling_strategy='auto')","TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=5000, min_df=1,
        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,
        stop_words=None, strip_accents=None, sublinear_tf=False,
        token_pattern='(?u)\\b\\w\\w+\\b', tokenizer=None, use_idf=True,
        vocabulary=None)",0.95,50000,1,"{'clf': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), 'smp': RandomUnderSampler(random_state=1, ratio=None, replacement=False,
          return_indices=False, sampling_strategy='auto'), 'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=5000, min_df=1,
        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,
        stop_words=None, strip_accents=None, sublinear_tf=False,
        token_pattern='(?u)\\b\\w\\w+\\b', tokenizer=None, use_idf=True,
        vocabulary=None), 'vec__max_df': 0.95, 'vec__max_features': 50000, 'vec__min_df': 1}",0.18100652691227945,0.18579723630962594,0.18463425142247883,0.18381267154812808,0.002040256397271293,43,0.38381174403212265,0.39886431701570185,0.3973876312220993,0.39335456408997455,0.006774669055910312
1.9009629885355632,0.024246454881908772,0.781562884648641,0.018231233219459318,"MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)","RandomUnderSampler(random_state=1, ratio=None, replacement=False,
          return_indices=False, sampling_strategy='auto')","TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=5000, min_df=1,
        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,
        stop_words=None, strip_accents=None, sublinear_tf=False,
        token_pattern='(?u)\\b\\w\\w+\\b', tokenizer=None, use_idf=True,
        vocabulary=None)",0.95,50000,0.05,"{'clf': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), 'smp': RandomUnderSampler(random_state=1, ratio=None, replacement=False,
          return_indices=False, sampling_strategy='auto'), 'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=5000, min_df=1,
        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,
        stop_words=None, strip_accents=None, sublinear_tf=False,
        token_pattern='(?u)\\b\\w\\w+\\b', tokenizer=None, use_idf=True,
        vocabulary=None), 'vec__max_df': 0.95, 'vec__max_features': 50000, 'vec__min_df': 0.05}",0.09654976281655568,0.08119435692489634,0.10320548010392323,0.09364986661512507,0.009216992798987368,161,0.09913509648857988,0.08572041236436952,0.1037317033069519,0.09619573738663377,0.007641183342137205
2.031364838282267,0.026161111461164132,0.8279741605122884,0.015844975938890744,"ComplementNB(alpha=1.0, class_prior=None, fit_prior=True, norm=False)",,"CountVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=None, min_df=1,
        ngram_range=(1, 1), preprocessor=None, stop_words=None,
        strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)",1.0,,1,"{'clf': ComplementNB(alpha=1.0, class_prior=None, fit_prior=True, norm=False), 'smp': None, 'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=None, min_df=1,
        ngram_range=(1, 1), preprocessor=None, stop_words=None,
        strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None), 'vec__max_df': 1.0, 'vec__max_features': None, 'vec__min_df': 1}",0.250357052816445,0.2520791610317009,0.2479176744331392,0.25011796276042836,0.00170731089117101,15,0.7518859325619682,0.753215150123358,0.7534496811916059,0.7528502546256441,0.0006885680733049803
1.8907178243001301,0.008283601768459823,0.7576401233673096,0.009471746040159679,"ComplementNB(alpha=1.0, class_prior=None, fit_prior=True, norm=False)",,"CountVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=None, min_df=1,
        ngram_range=(1, 1), preprocessor=None, stop_words=None,
        strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)",1.0,,0.05,"{'clf': ComplementNB(alpha=1.0, class_prior=None, fit_prior=True, norm=False), 'smp': None, 'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=None, min_df=1,
        ngram_range=(1, 1), preprocessor=None, stop_words=None,
        strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None), 'vec__max_df': 1.0, 'vec__max_features': None, 'vec__min_df': 0.05}",0.12239545925828445,0.12470695585916099,0.12446771088226888,0.12385670866657145,0.0010378654102391554,101,0.12429502426197964,0.12569078918141668,0.12686211153994947,0.1256159749944486,0.0010493433327903754
1.8916326363881428,0.03352909585502606,0.7924490769704183,0.006731580428597523,"ComplementNB(alpha=1.0, class_prior=None, fit_prior=True, norm=False)",,"CountVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=None, min_df=1,
        ngram_range=(1, 1), preprocessor=None, stop_words=None,
        strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)",1.0,500,1,"{'clf': ComplementNB(alpha=1.0, class_prior=None, fit_prior=True, norm=False), 'smp': None, 'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=None, min_df=1,
        ngram_range=(1, 1), preprocessor=None, stop_words=None,
        strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None), 'vec__max_df': 1.0, 'vec__max_features': 500, 'vec__min_df': 1}",0.21543554239974935,0.2216704088018267,0.22105735326350506,0.21938776815502703,0.002805830309525378,29,0.23471064459803906,0.23501359889198184,0.23084306809101857,0.23352243719367982,0.0018986327391531741
1.9217272599538167,0.013967956626332613,0.7523413499196371,0.020997725845922716,"ComplementNB(alpha=1.0, class_prior=None, fit_prior=True, norm=False)",,"CountVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=None, min_df=1,
        ngram_range=(1, 1), preprocessor=None, stop_words=None,
        strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)",1.0,500,0.05,"{'clf': ComplementNB(alpha=1.0, class_prior=None, fit_prior=True, norm=False), 'smp': None, 'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=None, min_df=1,
        ngram_range=(1, 1), preprocessor=None, stop_words=None,
        strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None), 'vec__max_df': 1.0, 'vec__max_features': 500, 'vec__min_df': 0.05}",0.12239545925828445,0.12470695585916099,0.12446771088226888,0.12385670866657145,0.0010378654102391554,101,0.12429502426197964,0.12569078918141668,0.12686211153994947,0.1256159749944486,0.0010493433327903754
1.914324680964152,0.028902489119945956,0.8024623394012451,0.018092420506914893,"ComplementNB(alpha=1.0, class_prior=None, fit_prior=True, norm=False)",,"CountVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=None, min_df=1,
        ngram_range=(1, 1), preprocessor=None, stop_words=None,
        strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)",1.0,5000,1,"{'clf': ComplementNB(alpha=1.0, class_prior=None, fit_prior=True, norm=False), 'smp': None, 'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=None, min_df=1,
        ngram_range=(1, 1), preprocessor=None, stop_words=None,
        strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None), 'vec__max_df': 1.0, 'vec__max_features': 5000, 'vec__min_df': 1}",0.25473489849153,0.258625848873113,0.25750937936278845,0.2569567089091438,0.0016358396805583807,3,0.37096320954309864,0.37058388147128685,0.36691938185878054,0.3694888242910554,0.0018234579340905767
1.8722487290700276,0.015518886105230433,0.7373645305633545,0.013746819763093417,"ComplementNB(alpha=1.0, class_prior=None, fit_prior=True, norm=False)",,"CountVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=None, min_df=1,
        ngram_range=(1, 1), preprocessor=None, stop_words=None,
        strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)",1.0,5000,0.05,"{'clf': ComplementNB(alpha=1.0, class_prior=None, fit_prior=True, norm=False), 'smp': None, 'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=None, min_df=1,
        ngram_range=(1, 1), preprocessor=None, stop_words=None,
        strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None), 'vec__max_df': 1.0, 'vec__max_features': 5000, 'vec__min_df': 0.05}",0.12239545925828445,0.12470695585916099,0.12446771088226888,0.12385670866657145,0.0010378654102391554,101,0.12429502426197964,0.12569078918141668,0.12686211153994947,0.1256159749944486,0.0010493433327903754
2.006272315979004,0.033898784962636645,0.8375673294067383,0.006192973560139427,"ComplementNB(alpha=1.0, class_prior=None, fit_prior=True, norm=False)",,"CountVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=None, min_df=1,
        ngram_range=(1, 1), preprocessor=None, stop_words=None,
        strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)",1.0,50000,1,"{'clf': ComplementNB(alpha=1.0, class_prior=None, fit_prior=True, norm=False), 'smp': None, 'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=None, min_df=1,
        ngram_range=(1, 1), preprocessor=None, stop_words=None,
        strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None), 'vec__max_df': 1.0, 'vec__max_features': 50000, 'vec__min_df': 1}",0.25036397553732914,0.25322950254396576,0.2490714416139841,0.250888306565093,0.0017375384781095592,13,0.7368333571235921,0.7379319922250149,0.7378841927034172,0.7375498473506749,0.000507010770888185
1.8700836499532063,0.014252127516766012,0.7534540494283041,0.0073418112607647875,"ComplementNB(alpha=1.0, class_prior=None, fit_prior=True, norm=False)",,"CountVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=None, min_df=1,
        ngram_range=(1, 1), preprocessor=None, stop_words=None,
        strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)",1.0,50000,0.05,"{'clf': ComplementNB(alpha=1.0, class_prior=None, fit_prior=True, norm=False), 'smp': None, 'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=None, min_df=1,
        ngram_range=(1, 1), preprocessor=None, stop_words=None,
        strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None), 'vec__max_df': 1.0, 'vec__max_features': 50000, 'vec__min_df': 0.05}",0.12239545925828445,0.12470695585916099,0.12446771088226888,0.12385670866657145,0.0010378654102391554,101,0.12429502426197964,0.12569078918141668,0.12686211153994947,0.1256159749944486,0.0010493433327903754
2.1145500342051187,0.028211330018116758,0.8450533548990885,0.02044930721011554,"ComplementNB(alpha=1.0, class_prior=None, fit_prior=True, norm=False)",,"CountVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=None, min_df=1,
        ngram_range=(1, 1), preprocessor=None, stop_words=None,
        strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)",0.95,,1,"{'clf': ComplementNB(alpha=1.0, class_prior=None, fit_prior=True, norm=False), 'smp': None, 'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=None, min_df=1,
        ngram_range=(1, 1), preprocessor=None, stop_words=None,
        strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None), 'vec__max_df': 0.95, 'vec__max_features': None, 'vec__min_df': 1}",0.250357052816445,0.2520791610317009,0.2479176744331392,0.25011796276042836,0.00170731089117101,15,0.7518859325619682,0.753215150123358,0.7534496811916059,0.7528502546256441,0.0006885680733049803
1.9043219089508057,0.04907560667540376,0.739225705464681,0.004565115036436045,"ComplementNB(alpha=1.0, class_prior=None, fit_prior=True, norm=False)",,"CountVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=None, min_df=1,
        ngram_range=(1, 1), preprocessor=None, stop_words=None,
        strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)",0.95,,0.05,"{'clf': ComplementNB(alpha=1.0, class_prior=None, fit_prior=True, norm=False), 'smp': None, 'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=None, min_df=1,
        ngram_range=(1, 1), preprocessor=None, stop_words=None,
        strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None), 'vec__max_df': 0.95, 'vec__max_features': None, 'vec__min_df': 0.05}",0.12239545925828445,0.12470695585916099,0.12446771088226888,0.12385670866657145,0.0010378654102391554,101,0.12429502426197964,0.12569078918141668,0.12686211153994947,0.1256159749944486,0.0010493433327903754
1.907557487487793,0.016390285874640295,0.7922037442525228,0.011448730883379641,"ComplementNB(alpha=1.0, class_prior=None, fit_prior=True, norm=False)",,"CountVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=None, min_df=1,
        ngram_range=(1, 1), preprocessor=None, stop_words=None,
        strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)",0.95,500,1,"{'clf': ComplementNB(alpha=1.0, class_prior=None, fit_prior=True, norm=False), 'smp': None, 'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=None, min_df=1,
        ngram_range=(1, 1), preprocessor=None, stop_words=None,
        strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None), 'vec__max_df': 0.95, 'vec__max_features': 500, 'vec__min_df': 1}",0.21543554239974935,0.2216704088018267,0.22105735326350506,0.21938776815502703,0.002805830309525378,29,0.23471064459803906,0.23501359889198184,0.23084306809101857,0.23352243719367982,0.0018986327391531741
1.8775232632954915,0.028886057273939152,0.7607115109761556,0.029059309802210692,"ComplementNB(alpha=1.0, class_prior=None, fit_prior=True, norm=False)",,"CountVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=None, min_df=1,
        ngram_range=(1, 1), preprocessor=None, stop_words=None,
        strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)",0.95,500,0.05,"{'clf': ComplementNB(alpha=1.0, class_prior=None, fit_prior=True, norm=False), 'smp': None, 'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=None, min_df=1,
        ngram_range=(1, 1), preprocessor=None, stop_words=None,
        strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None), 'vec__max_df': 0.95, 'vec__max_features': 500, 'vec__min_df': 0.05}",0.12239545925828445,0.12470695585916099,0.12446771088226888,0.12385670866657145,0.0010378654102391554,101,0.12429502426197964,0.12569078918141668,0.12686211153994947,0.1256159749944486,0.0010493433327903754
1.906230370203654,0.019960658540215493,0.80875031153361,0.007424105029601111,"ComplementNB(alpha=1.0, class_prior=None, fit_prior=True, norm=False)",,"CountVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=None, min_df=1,
        ngram_range=(1, 1), preprocessor=None, stop_words=None,
        strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)",0.95,5000,1,"{'clf': ComplementNB(alpha=1.0, class_prior=None, fit_prior=True, norm=False), 'smp': None, 'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=None, min_df=1,
        ngram_range=(1, 1), preprocessor=None, stop_words=None,
        strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None), 'vec__max_df': 0.95, 'vec__max_features': 5000, 'vec__min_df': 1}",0.25473489849153,0.258625848873113,0.25750937936278845,0.2569567089091438,0.0016358396805583807,3,0.37096320954309864,0.37058388147128685,0.36691938185878054,0.3694888242910554,0.0018234579340905767
1.8914661407470703,0.010998610097382601,0.7945456504821777,0.010386352347172577,"ComplementNB(alpha=1.0, class_prior=None, fit_prior=True, norm=False)",,"CountVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=None, min_df=1,
        ngram_range=(1, 1), preprocessor=None, stop_words=None,
        strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)",0.95,5000,0.05,"{'clf': ComplementNB(alpha=1.0, class_prior=None, fit_prior=True, norm=False), 'smp': None, 'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=None, min_df=1,
        ngram_range=(1, 1), preprocessor=None, stop_words=None,
        strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None), 'vec__max_df': 0.95, 'vec__max_features': 5000, 'vec__min_df': 0.05}",0.12239545925828445,0.12470695585916099,0.12446771088226888,0.12385670866657145,0.0010378654102391554,101,0.12429502426197964,0.12569078918141668,0.12686211153994947,0.1256159749944486,0.0010493433327903754
2.0351269245147705,0.0074608256841999804,0.8373504479726156,0.0059847372728605,"ComplementNB(alpha=1.0, class_prior=None, fit_prior=True, norm=False)",,"CountVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=None, min_df=1,
        ngram_range=(1, 1), preprocessor=None, stop_words=None,
        strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)",0.95,50000,1,"{'clf': ComplementNB(alpha=1.0, class_prior=None, fit_prior=True, norm=False), 'smp': None, 'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=None, min_df=1,
        ngram_range=(1, 1), preprocessor=None, stop_words=None,
        strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None), 'vec__max_df': 0.95, 'vec__max_features': 50000, 'vec__min_df': 1}",0.25036397553732914,0.25322950254396576,0.2490714416139841,0.250888306565093,0.0017375384781095592,13,0.7368333571235921,0.7379319922250149,0.7378841927034172,0.7375498473506749,0.000507010770888185
1.887013037999471,0.029684231803636378,0.7527307669321696,0.005233254683603816,"ComplementNB(alpha=1.0, class_prior=None, fit_prior=True, norm=False)",,"CountVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=None, min_df=1,
        ngram_range=(1, 1), preprocessor=None, stop_words=None,
        strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)",0.95,50000,0.05,"{'clf': ComplementNB(alpha=1.0, class_prior=None, fit_prior=True, norm=False), 'smp': None, 'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=None, min_df=1,
        ngram_range=(1, 1), preprocessor=None, stop_words=None,
        strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None), 'vec__max_df': 0.95, 'vec__max_features': 50000, 'vec__min_df': 0.05}",0.12239545925828445,0.12470695585916099,0.12446771088226888,0.12385670866657145,0.0010378654102391554,101,0.12429502426197964,0.12569078918141668,0.12686211153994947,0.1256159749944486,0.0010493433327903754
2.0452771186828613,0.009834467553887184,0.8415029843648275,0.005773776431975338,"ComplementNB(alpha=1.0, class_prior=None, fit_prior=True, norm=False)",,"TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=5000, min_df=1,
        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,
        stop_words=None, strip_accents=None, sublinear_tf=False,
        token_pattern='(?u)\\b\\w\\w+\\b', tokenizer=None, use_idf=True,
        vocabulary=None)",1.0,,1,"{'clf': ComplementNB(alpha=1.0, class_prior=None, fit_prior=True, norm=False), 'smp': None, 'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=5000, min_df=1,
        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,
        stop_words=None, strip_accents=None, sublinear_tf=False,
        token_pattern='(?u)\\b\\w\\w+\\b', tokenizer=None, use_idf=True,
        vocabulary=None), 'vec__max_df': 1.0, 'vec__max_features': None, 'vec__min_df': 1}",0.24521369364463705,0.2458910495606618,0.24266774452279832,0.24459082924269906,0.0013876584327997025,21,0.7597195973486368,0.7593709372238514,0.7582322522124898,0.7591075955949927,0.0006351170488566242
1.9042924245198567,0.009575641658735187,0.7479313214619955,0.006016409839293294,"ComplementNB(alpha=1.0, class_prior=None, fit_prior=True, norm=False)",,"TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=5000, min_df=1,
        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,
        stop_words=None, strip_accents=None, sublinear_tf=False,
        token_pattern='(?u)\\b\\w\\w+\\b', tokenizer=None, use_idf=True,
        vocabulary=None)",1.0,,0.05,"{'clf': ComplementNB(alpha=1.0, class_prior=None, fit_prior=True, norm=False), 'smp': None, 'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=5000, min_df=1,
        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,
        stop_words=None, strip_accents=None, sublinear_tf=False,
        token_pattern='(?u)\\b\\w\\w+\\b', tokenizer=None, use_idf=True,
        vocabulary=None), 'vec__max_df': 1.0, 'vec__max_features': None, 'vec__min_df': 0.05}",0.12369557532342018,0.12445502445166648,0.12356081499013198,0.12390380492173955,0.00039363461440498145,93,0.12460402544770767,0.12434991332446153,0.12590798917366014,0.12495397598194312,0.000682519411334576
1.917082945505778,0.048269575390190216,0.7999958197275797,0.008126560638815724,"ComplementNB(alpha=1.0, class_prior=None, fit_prior=True, norm=False)",,"TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=5000, min_df=1,
        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,
        stop_words=None, strip_accents=None, sublinear_tf=False,
        token_pattern='(?u)\\b\\w\\w+\\b', tokenizer=None, use_idf=True,
        vocabulary=None)",1.0,500,1,"{'clf': ComplementNB(alpha=1.0, class_prior=None, fit_prior=True, norm=False), 'smp': None, 'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=5000, min_df=1,
        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,
        stop_words=None, strip_accents=None, sublinear_tf=False,
        token_pattern='(?u)\\b\\w\\w+\\b', tokenizer=None, use_idf=True,
        vocabulary=None), 'vec__max_df': 1.0, 'vec__max_features': 500, 'vec__min_df': 1}",0.21619827796956356,0.2196057975665228,0.2209613377415751,0.21892180442588716,0.0020037580626190176,31,0.2366873530085469,0.23709762566681936,0.23336502123298034,0.23571666663611554,0.0016712785505108877
1.8871433734893799,0.018590650056470835,0.7629083792368571,0.010014929295180042,"ComplementNB(alpha=1.0, class_prior=None, fit_prior=True, norm=False)",,"TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=5000, min_df=1,
        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,
        stop_words=None, strip_accents=None, sublinear_tf=False,
        token_pattern='(?u)\\b\\w\\w+\\b', tokenizer=None, use_idf=True,
        vocabulary=None)",1.0,500,0.05,"{'clf': ComplementNB(alpha=1.0, class_prior=None, fit_prior=True, norm=False), 'smp': None, 'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=5000, min_df=1,
        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,
        stop_words=None, strip_accents=None, sublinear_tf=False,
        token_pattern='(?u)\\b\\w\\w+\\b', tokenizer=None, use_idf=True,
        vocabulary=None), 'vec__max_df': 1.0, 'vec__max_features': 500, 'vec__min_df': 0.05}",0.12369557532342018,0.12445502445166648,0.12356081499013198,0.12390380492173955,0.00039363461440498145,93,0.12460402544770767,0.12434991332446153,0.12590798917366014,0.12495397598194312,0.000682519411334576
1.9266878763834636,0.031754255260688376,0.8229312896728516,0.006868455237343568,"ComplementNB(alpha=1.0, class_prior=None, fit_prior=True, norm=False)",,"TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=5000, min_df=1,
        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,
        stop_words=None, strip_accents=None, sublinear_tf=False,
        token_pattern='(?u)\\b\\w\\w+\\b', tokenizer=None, use_idf=True,
        vocabulary=None)",1.0,5000,1,"{'clf': ComplementNB(alpha=1.0, class_prior=None, fit_prior=True, norm=False), 'smp': None, 'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=5000, min_df=1,
        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,
        stop_words=None, strip_accents=None, sublinear_tf=False,
        token_pattern='(?u)\\b\\w\\w+\\b', tokenizer=None, use_idf=True,
        vocabulary=None), 'vec__max_df': 1.0, 'vec__max_features': 5000, 'vec__min_df': 1}",0.25287810878817896,0.2536489046892126,0.2538007254068631,0.25344257962808486,0.00040392481099420365,9,0.3825298753914954,0.3819307315673485,0.37901467242307907,0.38115842646064096,0.0015354704731071094
1.8940720558166504,0.015333084279700463,0.7483146985371908,0.002657206687845552,"ComplementNB(alpha=1.0, class_prior=None, fit_prior=True, norm=False)",,"TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=5000, min_df=1,
        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,
        stop_words=None, strip_accents=None, sublinear_tf=False,
        token_pattern='(?u)\\b\\w\\w+\\b', tokenizer=None, use_idf=True,
        vocabulary=None)",1.0,5000,0.05,"{'clf': ComplementNB(alpha=1.0, class_prior=None, fit_prior=True, norm=False), 'smp': None, 'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=5000, min_df=1,
        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,
        stop_words=None, strip_accents=None, sublinear_tf=False,
        token_pattern='(?u)\\b\\w\\w+\\b', tokenizer=None, use_idf=True,
        vocabulary=None), 'vec__max_df': 1.0, 'vec__max_features': 5000, 'vec__min_df': 0.05}",0.12369557532342018,0.12445502445166648,0.12356081499013198,0.12390380492173955,0.00039363461440498145,93,0.12460402544770767,0.12434991332446153,0.12590798917366014,0.12495397598194312,0.000682519411334576
2.051273981730143,0.026686804394076547,0.8563573360443115,0.005215775389562321,"ComplementNB(alpha=1.0, class_prior=None, fit_prior=True, norm=False)",,"TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=5000, min_df=1,
        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,
        stop_words=None, strip_accents=None, sublinear_tf=False,
        token_pattern='(?u)\\b\\w\\w+\\b', tokenizer=None, use_idf=True,
        vocabulary=None)",1.0,50000,1,"{'clf': ComplementNB(alpha=1.0, class_prior=None, fit_prior=True, norm=False), 'smp': None, 'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=5000, min_df=1,
        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,
        stop_words=None, strip_accents=None, sublinear_tf=False,
        token_pattern='(?u)\\b\\w\\w+\\b', tokenizer=None, use_idf=True,
        vocabulary=None), 'vec__max_df': 1.0, 'vec__max_features': 50000, 'vec__min_df': 1}",0.2462617042893488,0.24712328550511267,0.24292471470361418,0.2454365681660252,0.001810641950673668,19,0.7470959574040391,0.7459924128126115,0.7448729069928449,0.7459870924031652,0.0009075643274703917
1.877113660176595,0.014035238722978113,0.7774479389190674,0.011920229244360014,"ComplementNB(alpha=1.0, class_prior=None, fit_prior=True, norm=False)",,"TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=5000, min_df=1,
        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,
        stop_words=None, strip_accents=None, sublinear_tf=False,
        token_pattern='(?u)\\b\\w\\w+\\b', tokenizer=None, use_idf=True,
        vocabulary=None)",1.0,50000,0.05,"{'clf': ComplementNB(alpha=1.0, class_prior=None, fit_prior=True, norm=False), 'smp': None, 'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=5000, min_df=1,
        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,
        stop_words=None, strip_accents=None, sublinear_tf=False,
        token_pattern='(?u)\\b\\w\\w+\\b', tokenizer=None, use_idf=True,
        vocabulary=None), 'vec__max_df': 1.0, 'vec__max_features': 50000, 'vec__min_df': 0.05}",0.12369557532342018,0.12445502445166648,0.12356081499013198,0.12390380492173955,0.00039363461440498145,93,0.12460402544770767,0.12434991332446153,0.12590798917366014,0.12495397598194312,0.000682519411334576
2.056959788004557,0.014471974937559574,0.846166213353475,0.0029145502863604563,"ComplementNB(alpha=1.0, class_prior=None, fit_prior=True, norm=False)",,"TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=5000, min_df=1,
        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,
        stop_words=None, strip_accents=None, sublinear_tf=False,
        token_pattern='(?u)\\b\\w\\w+\\b', tokenizer=None, use_idf=True,
        vocabulary=None)",0.95,,1,"{'clf': ComplementNB(alpha=1.0, class_prior=None, fit_prior=True, norm=False), 'smp': None, 'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=5000, min_df=1,
        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,
        stop_words=None, strip_accents=None, sublinear_tf=False,
        token_pattern='(?u)\\b\\w\\w+\\b', tokenizer=None, use_idf=True,
        vocabulary=None), 'vec__max_df': 0.95, 'vec__max_features': None, 'vec__min_df': 1}",0.24521369364463705,0.2458910495606618,0.24266774452279832,0.24459082924269906,0.0013876584327997025,21,0.7597195973486368,0.7593709372238514,0.7582322522124898,0.7591075955949927,0.0006351170488566242
1.9052319526672363,0.027510423141381173,0.772911787033081,0.00309931034030578,"ComplementNB(alpha=1.0, class_prior=None, fit_prior=True, norm=False)",,"TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=5000, min_df=1,
        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,
        stop_words=None, strip_accents=None, sublinear_tf=False,
        token_pattern='(?u)\\b\\w\\w+\\b', tokenizer=None, use_idf=True,
        vocabulary=None)",0.95,,0.05,"{'clf': ComplementNB(alpha=1.0, class_prior=None, fit_prior=True, norm=False), 'smp': None, 'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=5000, min_df=1,
        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,
        stop_words=None, strip_accents=None, sublinear_tf=False,
        token_pattern='(?u)\\b\\w\\w+\\b', tokenizer=None, use_idf=True,
        vocabulary=None), 'vec__max_df': 0.95, 'vec__max_features': None, 'vec__min_df': 0.05}",0.12369557532342018,0.12445502445166648,0.12356081499013198,0.12390380492173955,0.00039363461440498145,93,0.12460402544770767,0.12434991332446153,0.12590798917366014,0.12495397598194312,0.000682519411334576
1.8931935628255208,0.03972372907844407,0.790027936299642,0.014163075513326615,"ComplementNB(alpha=1.0, class_prior=None, fit_prior=True, norm=False)",,"TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=5000, min_df=1,
        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,
        stop_words=None, strip_accents=None, sublinear_tf=False,
        token_pattern='(?u)\\b\\w\\w+\\b', tokenizer=None, use_idf=True,
        vocabulary=None)",0.95,500,1,"{'clf': ComplementNB(alpha=1.0, class_prior=None, fit_prior=True, norm=False), 'smp': None, 'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=5000, min_df=1,
        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,
        stop_words=None, strip_accents=None, sublinear_tf=False,
        token_pattern='(?u)\\b\\w\\w+\\b', tokenizer=None, use_idf=True,
        vocabulary=None), 'vec__max_df': 0.95, 'vec__max_features': 500, 'vec__min_df': 1}",0.21619827796956356,0.2196057975665228,0.2209613377415751,0.21892180442588716,0.0020037580626190176,31,0.2366873530085469,0.23709762566681936,0.23336502123298034,0.23571666663611554,0.0016712785505108877
1.899206558863322,0.02240217939996246,0.7417360146840414,0.0068488583668362794,"ComplementNB(alpha=1.0, class_prior=None, fit_prior=True, norm=False)",,"TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=5000, min_df=1,
        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,
        stop_words=None, strip_accents=None, sublinear_tf=False,
        token_pattern='(?u)\\b\\w\\w+\\b', tokenizer=None, use_idf=True,
        vocabulary=None)",0.95,500,0.05,"{'clf': ComplementNB(alpha=1.0, class_prior=None, fit_prior=True, norm=False), 'smp': None, 'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=5000, min_df=1,
        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,
        stop_words=None, strip_accents=None, sublinear_tf=False,
        token_pattern='(?u)\\b\\w\\w+\\b', tokenizer=None, use_idf=True,
        vocabulary=None), 'vec__max_df': 0.95, 'vec__max_features': 500, 'vec__min_df': 0.05}",0.12369557532342018,0.12445502445166648,0.12356081499013198,0.12390380492173955,0.00039363461440498145,93,0.12460402544770767,0.12434991332446153,0.12590798917366014,0.12495397598194312,0.000682519411334576
1.961666186650594,0.03449662639578951,0.8314088185628256,0.012781755919401065,"ComplementNB(alpha=1.0, class_prior=None, fit_prior=True, norm=False)",,"TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=5000, min_df=1,
        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,
        stop_words=None, strip_accents=None, sublinear_tf=False,
        token_pattern='(?u)\\b\\w\\w+\\b', tokenizer=None, use_idf=True,
        vocabulary=None)",0.95,5000,1,"{'clf': ComplementNB(alpha=1.0, class_prior=None, fit_prior=True, norm=False), 'smp': None, 'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=5000, min_df=1,
        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,
        stop_words=None, strip_accents=None, sublinear_tf=False,
        token_pattern='(?u)\\b\\w\\w+\\b', tokenizer=None, use_idf=True,
        vocabulary=None), 'vec__max_df': 0.95, 'vec__max_features': 5000, 'vec__min_df': 1}",0.25287810878817896,0.2536489046892126,0.2538007254068631,0.25344257962808486,0.00040392481099420365,9,0.3825298753914954,0.3819307315673485,0.37901467242307907,0.38115842646064096,0.0015354704731071094
1.892365535100301,0.02895628584883057,0.7492656707763672,0.013327344126823001,"ComplementNB(alpha=1.0, class_prior=None, fit_prior=True, norm=False)",,"TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=5000, min_df=1,
        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,
        stop_words=None, strip_accents=None, sublinear_tf=False,
        token_pattern='(?u)\\b\\w\\w+\\b', tokenizer=None, use_idf=True,
        vocabulary=None)",0.95,5000,0.05,"{'clf': ComplementNB(alpha=1.0, class_prior=None, fit_prior=True, norm=False), 'smp': None, 'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=5000, min_df=1,
        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,
        stop_words=None, strip_accents=None, sublinear_tf=False,
        token_pattern='(?u)\\b\\w\\w+\\b', tokenizer=None, use_idf=True,
        vocabulary=None), 'vec__max_df': 0.95, 'vec__max_features': 5000, 'vec__min_df': 0.05}",0.12369557532342018,0.12445502445166648,0.12356081499013198,0.12390380492173955,0.00039363461440498145,93,0.12460402544770767,0.12434991332446153,0.12590798917366014,0.12495397598194312,0.000682519411334576
2.041658322016398,0.02698831908251872,0.8525165716807047,0.017022516347618172,"ComplementNB(alpha=1.0, class_prior=None, fit_prior=True, norm=False)",,"TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=5000, min_df=1,
        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,
        stop_words=None, strip_accents=None, sublinear_tf=False,
        token_pattern='(?u)\\b\\w\\w+\\b', tokenizer=None, use_idf=True,
        vocabulary=None)",0.95,50000,1,"{'clf': ComplementNB(alpha=1.0, class_prior=None, fit_prior=True, norm=False), 'smp': None, 'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=5000, min_df=1,
        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,
        stop_words=None, strip_accents=None, sublinear_tf=False,
        token_pattern='(?u)\\b\\w\\w+\\b', tokenizer=None, use_idf=True,
        vocabulary=None), 'vec__max_df': 0.95, 'vec__max_features': 50000, 'vec__min_df': 1}",0.2462617042893488,0.24712328550511267,0.24292471470361418,0.2454365681660252,0.001810641950673668,19,0.7470959574040391,0.7459924128126115,0.7448729069928449,0.7459870924031652,0.0009075643274703917
1.8940409024556477,0.006689408917320386,0.7658663590749105,0.013455683651649421,"ComplementNB(alpha=1.0, class_prior=None, fit_prior=True, norm=False)",,"TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=5000, min_df=1,
        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,
        stop_words=None, strip_accents=None, sublinear_tf=False,
        token_pattern='(?u)\\b\\w\\w+\\b', tokenizer=None, use_idf=True,
        vocabulary=None)",0.95,50000,0.05,"{'clf': ComplementNB(alpha=1.0, class_prior=None, fit_prior=True, norm=False), 'smp': None, 'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=5000, min_df=1,
        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,
        stop_words=None, strip_accents=None, sublinear_tf=False,
        token_pattern='(?u)\\b\\w\\w+\\b', tokenizer=None, use_idf=True,
        vocabulary=None), 'vec__max_df': 0.95, 'vec__max_features': 50000, 'vec__min_df': 0.05}",0.12369557532342018,0.12445502445166648,0.12356081499013198,0.12390380492173955,0.00039363461440498145,93,0.12460402544770767,0.12434991332446153,0.12590798917366014,0.12495397598194312,0.000682519411334576
2.0156643390655518,0.04667788354431435,0.8387277921040853,0.010796806280311846,"ComplementNB(alpha=1.0, class_prior=None, fit_prior=True, norm=False)","RandomUnderSampler(random_state=1, ratio=None, replacement=False,
          return_indices=False, sampling_strategy='auto')","CountVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=None, min_df=1,
        ngram_range=(1, 1), preprocessor=None, stop_words=None,
        strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)",1.0,,1,"{'clf': ComplementNB(alpha=1.0, class_prior=None, fit_prior=True, norm=False), 'smp': RandomUnderSampler(random_state=1, ratio=None, replacement=False,
          return_indices=False, sampling_strategy='auto'), 'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=None, min_df=1,
        ngram_range=(1, 1), preprocessor=None, stop_words=None,
        strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None), 'vec__max_df': 1.0, 'vec__max_features': None, 'vec__min_df': 1}",0.16746835614449795,0.1742998665918077,0.16769391469206563,0.16982071247612376,0.003168578583385253,69,0.4177600574374484,0.4329707286346149,0.4311148396737155,0.4272818752485929,0.00677543808080674
1.9175020058949788,0.007798261947378986,0.7536263465881348,0.008020871873297796,"ComplementNB(alpha=1.0, class_prior=None, fit_prior=True, norm=False)","RandomUnderSampler(random_state=1, ratio=None, replacement=False,
          return_indices=False, sampling_strategy='auto')","CountVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=None, min_df=1,
        ngram_range=(1, 1), preprocessor=None, stop_words=None,
        strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)",1.0,,0.05,"{'clf': ComplementNB(alpha=1.0, class_prior=None, fit_prior=True, norm=False), 'smp': RandomUnderSampler(random_state=1, ratio=None, replacement=False,
          return_indices=False, sampling_strategy='auto'), 'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=None, min_df=1,
        ngram_range=(1, 1), preprocessor=None, stop_words=None,
        strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None), 'vec__max_df': 1.0, 'vec__max_features': None, 'vec__min_df': 0.05}",0.1033072768614791,0.09646191998928094,0.11058785728796419,0.10345235137957474,0.0057678020714672686,121,0.10604339614646077,0.09885918841660166,0.11267647170708257,0.10585968542338166,0.005642377844176542
1.9004626274108887,0.012314835299415308,0.7894360224405924,0.012720400865807306,"ComplementNB(alpha=1.0, class_prior=None, fit_prior=True, norm=False)","RandomUnderSampler(random_state=1, ratio=None, replacement=False,
          return_indices=False, sampling_strategy='auto')","CountVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=None, min_df=1,
        ngram_range=(1, 1), preprocessor=None, stop_words=None,
        strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)",1.0,500,1,"{'clf': ComplementNB(alpha=1.0, class_prior=None, fit_prior=True, norm=False), 'smp': RandomUnderSampler(random_state=1, ratio=None, replacement=False,
          return_indices=False, sampling_strategy='auto'), 'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=None, min_df=1,
        ngram_range=(1, 1), preprocessor=None, stop_words=None,
        strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None), 'vec__max_df': 1.0, 'vec__max_features': 500, 'vec__min_df': 1}",0.159070946105226,0.16447466008154163,0.16413537782069415,0.16256032800248724,0.002471250376069078,89,0.18359990643206414,0.18205982033960924,0.1812845320117637,0.18231475292781232,0.0009622829261505711
1.9233697255452473,0.018878880644839235,0.7566165924072266,0.03256186088252279,"ComplementNB(alpha=1.0, class_prior=None, fit_prior=True, norm=False)","RandomUnderSampler(random_state=1, ratio=None, replacement=False,
          return_indices=False, sampling_strategy='auto')","CountVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=None, min_df=1,
        ngram_range=(1, 1), preprocessor=None, stop_words=None,
        strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)",1.0,500,0.05,"{'clf': ComplementNB(alpha=1.0, class_prior=None, fit_prior=True, norm=False), 'smp': RandomUnderSampler(random_state=1, ratio=None, replacement=False,
          return_indices=False, sampling_strategy='auto'), 'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=None, min_df=1,
        ngram_range=(1, 1), preprocessor=None, stop_words=None,
        strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None), 'vec__max_df': 1.0, 'vec__max_features': 500, 'vec__min_df': 0.05}",0.1033072768614791,0.09646191998928094,0.11058785728796419,0.10345235137957474,0.0057678020714672686,121,0.10604339614646077,0.09885918841660166,0.11267647170708257,0.10585968542338166,0.005642377844176542
1.941686709721883,0.04594660750338874,0.8215300242106119,0.010074649620291195,"ComplementNB(alpha=1.0, class_prior=None, fit_prior=True, norm=False)","RandomUnderSampler(random_state=1, ratio=None, replacement=False,
          return_indices=False, sampling_strategy='auto')","CountVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=None, min_df=1,
        ngram_range=(1, 1), preprocessor=None, stop_words=None,
        strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)",1.0,5000,1,"{'clf': ComplementNB(alpha=1.0, class_prior=None, fit_prior=True, norm=False), 'smp': RandomUnderSampler(random_state=1, ratio=None, replacement=False,
          return_indices=False, sampling_strategy='auto'), 'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=None, min_df=1,
        ngram_range=(1, 1), preprocessor=None, stop_words=None,
        strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None), 'vec__max_df': 1.0, 'vec__max_features': 5000, 'vec__min_df': 1}",0.17899444315231977,0.18137088860960932,0.17846585844707333,0.17961039673633414,0.0012634210632395612,59,0.2889923154513366,0.29039814428900806,0.28889153173315696,0.2894273304911672,0.0006877009582762837
1.9038373629252117,0.006179057995072085,0.7584796746571859,0.015688970294836947,"ComplementNB(alpha=1.0, class_prior=None, fit_prior=True, norm=False)","RandomUnderSampler(random_state=1, ratio=None, replacement=False,
          return_indices=False, sampling_strategy='auto')","CountVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=None, min_df=1,
        ngram_range=(1, 1), preprocessor=None, stop_words=None,
        strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)",1.0,5000,0.05,"{'clf': ComplementNB(alpha=1.0, class_prior=None, fit_prior=True, norm=False), 'smp': RandomUnderSampler(random_state=1, ratio=None, replacement=False,
          return_indices=False, sampling_strategy='auto'), 'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=None, min_df=1,
        ngram_range=(1, 1), preprocessor=None, stop_words=None,
        strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None), 'vec__max_df': 1.0, 'vec__max_features': 5000, 'vec__min_df': 0.05}",0.1033072768614791,0.09646191998928094,0.11058785728796419,0.10345235137957474,0.0057678020714672686,121,0.10604339614646077,0.09885918841660166,0.11267647170708257,0.10585968542338166,0.005642377844176542
2.050981601079305,0.04428966239804627,0.853929360707601,0.006369312105359745,"ComplementNB(alpha=1.0, class_prior=None, fit_prior=True, norm=False)","RandomUnderSampler(random_state=1, ratio=None, replacement=False,
          return_indices=False, sampling_strategy='auto')","CountVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=None, min_df=1,
        ngram_range=(1, 1), preprocessor=None, stop_words=None,
        strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)",1.0,50000,1,"{'clf': ComplementNB(alpha=1.0, class_prior=None, fit_prior=True, norm=False), 'smp': RandomUnderSampler(random_state=1, ratio=None, replacement=False,
          return_indices=False, sampling_strategy='auto'), 'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=None, min_df=1,
        ngram_range=(1, 1), preprocessor=None, stop_words=None,
        strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None), 'vec__max_df': 1.0, 'vec__max_features': 50000, 'vec__min_df': 1}",0.16739743796721201,0.17481484779504614,0.16800039878742987,0.170070894849896,0.0033635009139316324,67,0.4135711522094789,0.42917495795082194,0.4264811055138701,0.42307573855805697,0.006810143487533903
1.9061519304911296,0.008358143062143218,0.763447125752767,0.018547291382194422,"ComplementNB(alpha=1.0, class_prior=None, fit_prior=True, norm=False)","RandomUnderSampler(random_state=1, ratio=None, replacement=False,
          return_indices=False, sampling_strategy='auto')","CountVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=None, min_df=1,
        ngram_range=(1, 1), preprocessor=None, stop_words=None,
        strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)",1.0,50000,0.05,"{'clf': ComplementNB(alpha=1.0, class_prior=None, fit_prior=True, norm=False), 'smp': RandomUnderSampler(random_state=1, ratio=None, replacement=False,
          return_indices=False, sampling_strategy='auto'), 'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=None, min_df=1,
        ngram_range=(1, 1), preprocessor=None, stop_words=None,
        strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None), 'vec__max_df': 1.0, 'vec__max_features': 50000, 'vec__min_df': 0.05}",0.1033072768614791,0.09646191998928094,0.11058785728796419,0.10345235137957474,0.0057678020714672686,121,0.10604339614646077,0.09885918841660166,0.11267647170708257,0.10585968542338166,0.005642377844176542
2.0123112996419272,0.011379339899187369,0.8398682276407877,0.010393598966950586,"ComplementNB(alpha=1.0, class_prior=None, fit_prior=True, norm=False)","RandomUnderSampler(random_state=1, ratio=None, replacement=False,
          return_indices=False, sampling_strategy='auto')","CountVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=None, min_df=1,
        ngram_range=(1, 1), preprocessor=None, stop_words=None,
        strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)",0.95,,1,"{'clf': ComplementNB(alpha=1.0, class_prior=None, fit_prior=True, norm=False), 'smp': RandomUnderSampler(random_state=1, ratio=None, replacement=False,
          return_indices=False, sampling_strategy='auto'), 'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=None, min_df=1,
        ngram_range=(1, 1), preprocessor=None, stop_words=None,
        strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None), 'vec__max_df': 0.95, 'vec__max_features': None, 'vec__min_df': 1}",0.16746835614449795,0.1742998665918077,0.16769391469206563,0.16982071247612376,0.003168578583385253,69,0.4177600574374484,0.4329707286346149,0.4311148396737155,0.4272818752485929,0.00677543808080674
1.9219815731048584,0.020272581229708337,0.7655854225158691,0.024809157571743057,"ComplementNB(alpha=1.0, class_prior=None, fit_prior=True, norm=False)","RandomUnderSampler(random_state=1, ratio=None, replacement=False,
          return_indices=False, sampling_strategy='auto')","CountVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=None, min_df=1,
        ngram_range=(1, 1), preprocessor=None, stop_words=None,
        strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)",0.95,,0.05,"{'clf': ComplementNB(alpha=1.0, class_prior=None, fit_prior=True, norm=False), 'smp': RandomUnderSampler(random_state=1, ratio=None, replacement=False,
          return_indices=False, sampling_strategy='auto'), 'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=None, min_df=1,
        ngram_range=(1, 1), preprocessor=None, stop_words=None,
        strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None), 'vec__max_df': 0.95, 'vec__max_features': None, 'vec__min_df': 0.05}",0.1033072768614791,0.09646191998928094,0.11058785728796419,0.10345235137957474,0.0057678020714672686,121,0.10604339614646077,0.09885918841660166,0.11267647170708257,0.10585968542338166,0.005642377844176542
1.936829964319865,0.025303893809754955,0.8028457164764404,0.0077062402153344495,"ComplementNB(alpha=1.0, class_prior=None, fit_prior=True, norm=False)","RandomUnderSampler(random_state=1, ratio=None, replacement=False,
          return_indices=False, sampling_strategy='auto')","CountVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=None, min_df=1,
        ngram_range=(1, 1), preprocessor=None, stop_words=None,
        strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)",0.95,500,1,"{'clf': ComplementNB(alpha=1.0, class_prior=None, fit_prior=True, norm=False), 'smp': RandomUnderSampler(random_state=1, ratio=None, replacement=False,
          return_indices=False, sampling_strategy='auto'), 'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=None, min_df=1,
        ngram_range=(1, 1), preprocessor=None, stop_words=None,
        strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None), 'vec__max_df': 0.95, 'vec__max_features': 500, 'vec__min_df': 1}",0.159070946105226,0.16447466008154163,0.16413537782069415,0.16256032800248724,0.002471250376069078,89,0.18359990643206414,0.18205982033960924,0.1812845320117637,0.18231475292781232,0.0009622829261505711
1.9054829279581706,0.024310277518562243,0.8530903657277426,0.07254680943174291,"ComplementNB(alpha=1.0, class_prior=None, fit_prior=True, norm=False)","RandomUnderSampler(random_state=1, ratio=None, replacement=False,
          return_indices=False, sampling_strategy='auto')","CountVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=None, min_df=1,
        ngram_range=(1, 1), preprocessor=None, stop_words=None,
        strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)",0.95,500,0.05,"{'clf': ComplementNB(alpha=1.0, class_prior=None, fit_prior=True, norm=False), 'smp': RandomUnderSampler(random_state=1, ratio=None, replacement=False,
          return_indices=False, sampling_strategy='auto'), 'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=None, min_df=1,
        ngram_range=(1, 1), preprocessor=None, stop_words=None,
        strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None), 'vec__max_df': 0.95, 'vec__max_features': 500, 'vec__min_df': 0.05}",0.1033072768614791,0.09646191998928094,0.11058785728796419,0.10345235137957474,0.0057678020714672686,121,0.10604339614646077,0.09885918841660166,0.11267647170708257,0.10585968542338166,0.005642377844176542
1.996321678161621,0.0513954751259362,0.830743153889974,0.0033763508519142524,"ComplementNB(alpha=1.0, class_prior=None, fit_prior=True, norm=False)","RandomUnderSampler(random_state=1, ratio=None, replacement=False,
          return_indices=False, sampling_strategy='auto')","CountVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=None, min_df=1,
        ngram_range=(1, 1), preprocessor=None, stop_words=None,
        strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)",0.95,5000,1,"{'clf': ComplementNB(alpha=1.0, class_prior=None, fit_prior=True, norm=False), 'smp': RandomUnderSampler(random_state=1, ratio=None, replacement=False,
          return_indices=False, sampling_strategy='auto'), 'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=None, min_df=1,
        ngram_range=(1, 1), preprocessor=None, stop_words=None,
        strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None), 'vec__max_df': 0.95, 'vec__max_features': 5000, 'vec__min_df': 1}",0.17899444315231977,0.18137088860960932,0.17846585844707333,0.17961039673633414,0.0012634210632395612,59,0.2889923154513366,0.29039814428900806,0.28889153173315696,0.2894273304911672,0.0006877009582762837
1.9563236236572266,0.028366231768482457,0.7607216040293375,0.014713987574668849,"ComplementNB(alpha=1.0, class_prior=None, fit_prior=True, norm=False)","RandomUnderSampler(random_state=1, ratio=None, replacement=False,
          return_indices=False, sampling_strategy='auto')","CountVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=None, min_df=1,
        ngram_range=(1, 1), preprocessor=None, stop_words=None,
        strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)",0.95,5000,0.05,"{'clf': ComplementNB(alpha=1.0, class_prior=None, fit_prior=True, norm=False), 'smp': RandomUnderSampler(random_state=1, ratio=None, replacement=False,
          return_indices=False, sampling_strategy='auto'), 'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=None, min_df=1,
        ngram_range=(1, 1), preprocessor=None, stop_words=None,
        strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None), 'vec__max_df': 0.95, 'vec__max_features': 5000, 'vec__min_df': 0.05}",0.1033072768614791,0.09646191998928094,0.11058785728796419,0.10345235137957474,0.0057678020714672686,121,0.10604339614646077,0.09885918841660166,0.11267647170708257,0.10585968542338166,0.005642377844176542
2.0402611096700034,0.023436238261899854,0.9217245578765869,0.031644389897689675,"ComplementNB(alpha=1.0, class_prior=None, fit_prior=True, norm=False)","RandomUnderSampler(random_state=1, ratio=None, replacement=False,
          return_indices=False, sampling_strategy='auto')","CountVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=None, min_df=1,
        ngram_range=(1, 1), preprocessor=None, stop_words=None,
        strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)",0.95,50000,1,"{'clf': ComplementNB(alpha=1.0, class_prior=None, fit_prior=True, norm=False), 'smp': RandomUnderSampler(random_state=1, ratio=None, replacement=False,
          return_indices=False, sampling_strategy='auto'), 'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=None, min_df=1,
        ngram_range=(1, 1), preprocessor=None, stop_words=None,
        strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None), 'vec__max_df': 0.95, 'vec__max_features': 50000, 'vec__min_df': 1}",0.16739743796721201,0.17481484779504614,0.16800039878742987,0.170070894849896,0.0033635009139316324,67,0.4135711522094789,0.42917495795082194,0.4264811055138701,0.42307573855805697,0.006810143487533903
1.9509156545003254,0.02689623263543116,0.7650852203369141,0.011921893797611544,"ComplementNB(alpha=1.0, class_prior=None, fit_prior=True, norm=False)","RandomUnderSampler(random_state=1, ratio=None, replacement=False,
          return_indices=False, sampling_strategy='auto')","CountVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=None, min_df=1,
        ngram_range=(1, 1), preprocessor=None, stop_words=None,
        strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)",0.95,50000,0.05,"{'clf': ComplementNB(alpha=1.0, class_prior=None, fit_prior=True, norm=False), 'smp': RandomUnderSampler(random_state=1, ratio=None, replacement=False,
          return_indices=False, sampling_strategy='auto'), 'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=None, min_df=1,
        ngram_range=(1, 1), preprocessor=None, stop_words=None,
        strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None), 'vec__max_df': 0.95, 'vec__max_features': 50000, 'vec__min_df': 0.05}",0.1033072768614791,0.09646191998928094,0.11058785728796419,0.10345235137957474,0.0057678020714672686,121,0.10604339614646077,0.09885918841660166,0.11267647170708257,0.10585968542338166,0.005642377844176542
2.071880658467611,0.015084939469151007,0.8538282712300619,0.005643689532029793,"ComplementNB(alpha=1.0, class_prior=None, fit_prior=True, norm=False)","RandomUnderSampler(random_state=1, ratio=None, replacement=False,
          return_indices=False, sampling_strategy='auto')","TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=5000, min_df=1,
        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,
        stop_words=None, strip_accents=None, sublinear_tf=False,
        token_pattern='(?u)\\b\\w\\w+\\b', tokenizer=None, use_idf=True,
        vocabulary=None)",1.0,,1,"{'clf': ComplementNB(alpha=1.0, class_prior=None, fit_prior=True, norm=False), 'smp': RandomUnderSampler(random_state=1, ratio=None, replacement=False,
          return_indices=False, sampling_strategy='auto'), 'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=5000, min_df=1,
        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,
        stop_words=None, strip_accents=None, sublinear_tf=False,
        token_pattern='(?u)\\b\\w\\w+\\b', tokenizer=None, use_idf=True,
        vocabulary=None), 'vec__max_df': 1.0, 'vec__max_features': None, 'vec__min_df': 1}",0.16212474783554645,0.17137630373716906,0.1650793760656427,0.16619347587945274,0.0038582150835639586,83,0.4242088414311599,0.4412854102387994,0.4377153803653837,0.43440321067844767,0.007354370654552085
1.9083022276560466,0.013341159211739749,0.7596689860026041,0.011925757610109026,"ComplementNB(alpha=1.0, class_prior=None, fit_prior=True, norm=False)","RandomUnderSampler(random_state=1, ratio=None, replacement=False,
          return_indices=False, sampling_strategy='auto')","TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=5000, min_df=1,
        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,
        stop_words=None, strip_accents=None, sublinear_tf=False,
        token_pattern='(?u)\\b\\w\\w+\\b', tokenizer=None, use_idf=True,
        vocabulary=None)",1.0,,0.05,"{'clf': ComplementNB(alpha=1.0, class_prior=None, fit_prior=True, norm=False), 'smp': RandomUnderSampler(random_state=1, ratio=None, replacement=False,
          return_indices=False, sampling_strategy='auto'), 'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=5000, min_df=1,
        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,
        stop_words=None, strip_accents=None, sublinear_tf=False,
        token_pattern='(?u)\\b\\w\\w+\\b', tokenizer=None, use_idf=True,
        vocabulary=None), 'vec__max_df': 1.0, 'vec__max_features': None, 'vec__min_df': 0.05}",0.09797414360333398,0.07601759555353166,0.10640770685388994,0.09346648200358519,0.012809605147061282,169,0.1000967410743705,0.07892564208114629,0.10772310610807197,0.09558182975452957,0.012182276792265074
1.9220969676971436,0.033089504152825834,0.7948576609293619,0.0182023672623212,"ComplementNB(alpha=1.0, class_prior=None, fit_prior=True, norm=False)","RandomUnderSampler(random_state=1, ratio=None, replacement=False,
          return_indices=False, sampling_strategy='auto')","TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=5000, min_df=1,
        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,
        stop_words=None, strip_accents=None, sublinear_tf=False,
        token_pattern='(?u)\\b\\w\\w+\\b', tokenizer=None, use_idf=True,
        vocabulary=None)",1.0,500,1,"{'clf': ComplementNB(alpha=1.0, class_prior=None, fit_prior=True, norm=False), 'smp': RandomUnderSampler(random_state=1, ratio=None, replacement=False,
          return_indices=False, sampling_strategy='auto'), 'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=5000, min_df=1,
        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,
        stop_words=None, strip_accents=None, sublinear_tf=False,
        token_pattern='(?u)\\b\\w\\w+\\b', tokenizer=None, use_idf=True,
        vocabulary=None), 'vec__max_df': 1.0, 'vec__max_features': 500, 'vec__min_df': 1}",0.15608882148494235,0.15975020568098738,0.1577005885019332,0.15784653855595432,0.0014983122997701236,91,0.1769607500669556,0.1801427315630858,0.17629747673690513,0.17780031945564886,0.00167832331775684
1.9140143394470215,0.03174538634267198,0.7655189832051595,0.016979920520052056,"ComplementNB(alpha=1.0, class_prior=None, fit_prior=True, norm=False)","RandomUnderSampler(random_state=1, ratio=None, replacement=False,
          return_indices=False, sampling_strategy='auto')","TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=5000, min_df=1,
        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,
        stop_words=None, strip_accents=None, sublinear_tf=False,
        token_pattern='(?u)\\b\\w\\w+\\b', tokenizer=None, use_idf=True,
        vocabulary=None)",1.0,500,0.05,"{'clf': ComplementNB(alpha=1.0, class_prior=None, fit_prior=True, norm=False), 'smp': RandomUnderSampler(random_state=1, ratio=None, replacement=False,
          return_indices=False, sampling_strategy='auto'), 'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=5000, min_df=1,
        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,
        stop_words=None, strip_accents=None, sublinear_tf=False,
        token_pattern='(?u)\\b\\w\\w+\\b', tokenizer=None, use_idf=True,
        vocabulary=None), 'vec__max_df': 1.0, 'vec__max_features': 500, 'vec__min_df': 0.05}",0.09797414360333398,0.07601759555353166,0.10640770685388994,0.09346648200358519,0.012809605147061282,169,0.1000967410743705,0.07892564208114629,0.10772310610807197,0.09558182975452957,0.012182276792265074
1.9676748116811116,0.01748046429553443,0.8511595726013184,0.03356244704414913,"ComplementNB(alpha=1.0, class_prior=None, fit_prior=True, norm=False)","RandomUnderSampler(random_state=1, ratio=None, replacement=False,
          return_indices=False, sampling_strategy='auto')","TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=5000, min_df=1,
        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,
        stop_words=None, strip_accents=None, sublinear_tf=False,
        token_pattern='(?u)\\b\\w\\w+\\b', tokenizer=None, use_idf=True,
        vocabulary=None)",1.0,5000,1,"{'clf': ComplementNB(alpha=1.0, class_prior=None, fit_prior=True, norm=False), 'smp': RandomUnderSampler(random_state=1, ratio=None, replacement=False,
          return_indices=False, sampling_strategy='auto'), 'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=5000, min_df=1,
        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,
        stop_words=None, strip_accents=None, sublinear_tf=False,
        token_pattern='(?u)\\b\\w\\w+\\b', tokenizer=None, use_idf=True,
        vocabulary=None), 'vec__max_df': 1.0, 'vec__max_features': 5000, 'vec__min_df': 1}",0.17341265701427655,0.17627014921257073,0.17102552206607724,0.17356944276430816,0.0021439783588761393,65,0.28914656739372835,0.29354332733072785,0.2899718560549493,0.29088725025980183,0.0019081115268958435
1.9909339745839436,0.0552777317356574,0.7944397926330566,0.0495918248482019,"ComplementNB(alpha=1.0, class_prior=None, fit_prior=True, norm=False)","RandomUnderSampler(random_state=1, ratio=None, replacement=False,
          return_indices=False, sampling_strategy='auto')","TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=5000, min_df=1,
        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,
        stop_words=None, strip_accents=None, sublinear_tf=False,
        token_pattern='(?u)\\b\\w\\w+\\b', tokenizer=None, use_idf=True,
        vocabulary=None)",1.0,5000,0.05,"{'clf': ComplementNB(alpha=1.0, class_prior=None, fit_prior=True, norm=False), 'smp': RandomUnderSampler(random_state=1, ratio=None, replacement=False,
          return_indices=False, sampling_strategy='auto'), 'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=5000, min_df=1,
        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,
        stop_words=None, strip_accents=None, sublinear_tf=False,
        token_pattern='(?u)\\b\\w\\w+\\b', tokenizer=None, use_idf=True,
        vocabulary=None), 'vec__max_df': 1.0, 'vec__max_features': 5000, 'vec__min_df': 0.05}",0.09797414360333398,0.07601759555353166,0.10640770685388994,0.09346648200358519,0.012809605147061282,169,0.1000967410743705,0.07892564208114629,0.10772310610807197,0.09558182975452957,0.012182276792265074
2.0793616771698,0.031768183684652666,0.882586399714152,0.027921110597151893,"ComplementNB(alpha=1.0, class_prior=None, fit_prior=True, norm=False)","RandomUnderSampler(random_state=1, ratio=None, replacement=False,
          return_indices=False, sampling_strategy='auto')","TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=5000, min_df=1,
        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,
        stop_words=None, strip_accents=None, sublinear_tf=False,
        token_pattern='(?u)\\b\\w\\w+\\b', tokenizer=None, use_idf=True,
        vocabulary=None)",1.0,50000,1,"{'clf': ComplementNB(alpha=1.0, class_prior=None, fit_prior=True, norm=False), 'smp': RandomUnderSampler(random_state=1, ratio=None, replacement=False,
          return_indices=False, sampling_strategy='auto'), 'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=5000, min_df=1,
        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,
        stop_words=None, strip_accents=None, sublinear_tf=False,
        token_pattern='(?u)\\b\\w\\w+\\b', tokenizer=None, use_idf=True,
        vocabulary=None), 'vec__max_df': 1.0, 'vec__max_features': 50000, 'vec__min_df': 1}",0.1622739104332211,0.1705768737219405,0.16481209204105712,0.16588762539873955,0.003473939055891386,85,0.4199687316297684,0.4373503839109756,0.43376209964185714,0.4303604050608671,0.007492623195425924
1.913472016652425,0.020706121114778903,0.7835866610209147,0.021520797916315386,"ComplementNB(alpha=1.0, class_prior=None, fit_prior=True, norm=False)","RandomUnderSampler(random_state=1, ratio=None, replacement=False,
          return_indices=False, sampling_strategy='auto')","TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=5000, min_df=1,
        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,
        stop_words=None, strip_accents=None, sublinear_tf=False,
        token_pattern='(?u)\\b\\w\\w+\\b', tokenizer=None, use_idf=True,
        vocabulary=None)",1.0,50000,0.05,"{'clf': ComplementNB(alpha=1.0, class_prior=None, fit_prior=True, norm=False), 'smp': RandomUnderSampler(random_state=1, ratio=None, replacement=False,
          return_indices=False, sampling_strategy='auto'), 'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=5000, min_df=1,
        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,
        stop_words=None, strip_accents=None, sublinear_tf=False,
        token_pattern='(?u)\\b\\w\\w+\\b', tokenizer=None, use_idf=True,
        vocabulary=None), 'vec__max_df': 1.0, 'vec__max_features': 50000, 'vec__min_df': 0.05}",0.09797414360333398,0.07601759555353166,0.10640770685388994,0.09346648200358519,0.012809605147061282,169,0.1000967410743705,0.07892564208114629,0.10772310610807197,0.09558182975452957,0.012182276792265074
2.0948341687520347,0.014935936768742686,0.8745323816935221,0.017979199032003467,"ComplementNB(alpha=1.0, class_prior=None, fit_prior=True, norm=False)","RandomUnderSampler(random_state=1, ratio=None, replacement=False,
          return_indices=False, sampling_strategy='auto')","TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=5000, min_df=1,
        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,
        stop_words=None, strip_accents=None, sublinear_tf=False,
        token_pattern='(?u)\\b\\w\\w+\\b', tokenizer=None, use_idf=True,
        vocabulary=None)",0.95,,1,"{'clf': ComplementNB(alpha=1.0, class_prior=None, fit_prior=True, norm=False), 'smp': RandomUnderSampler(random_state=1, ratio=None, replacement=False,
          return_indices=False, sampling_strategy='auto'), 'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=5000, min_df=1,
        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,
        stop_words=None, strip_accents=None, sublinear_tf=False,
        token_pattern='(?u)\\b\\w\\w+\\b', tokenizer=None, use_idf=True,
        vocabulary=None), 'vec__max_df': 0.95, 'vec__max_features': None, 'vec__min_df': 1}",0.16212474783554645,0.17137630373716906,0.1650793760656427,0.16619347587945274,0.0038582150835639586,83,0.4242088414311599,0.4412854102387994,0.4377153803653837,0.43440321067844767,0.007354370654552085
2.0715413093566895,0.09487396199079261,0.7667721112569174,0.014210016669334727,"ComplementNB(alpha=1.0, class_prior=None, fit_prior=True, norm=False)","RandomUnderSampler(random_state=1, ratio=None, replacement=False,
          return_indices=False, sampling_strategy='auto')","TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=5000, min_df=1,
        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,
        stop_words=None, strip_accents=None, sublinear_tf=False,
        token_pattern='(?u)\\b\\w\\w+\\b', tokenizer=None, use_idf=True,
        vocabulary=None)",0.95,,0.05,"{'clf': ComplementNB(alpha=1.0, class_prior=None, fit_prior=True, norm=False), 'smp': RandomUnderSampler(random_state=1, ratio=None, replacement=False,
          return_indices=False, sampling_strategy='auto'), 'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=5000, min_df=1,
        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,
        stop_words=None, strip_accents=None, sublinear_tf=False,
        token_pattern='(?u)\\b\\w\\w+\\b', tokenizer=None, use_idf=True,
        vocabulary=None), 'vec__max_df': 0.95, 'vec__max_features': None, 'vec__min_df': 0.05}",0.09797414360333398,0.07601759555353166,0.10640770685388994,0.09346648200358519,0.012809605147061282,169,0.1000967410743705,0.07892564208114629,0.10772310610807197,0.09558182975452957,0.012182276792265074
1.9372626145680745,0.03170766185328498,0.8114360173543295,0.005060671533183694,"ComplementNB(alpha=1.0, class_prior=None, fit_prior=True, norm=False)","RandomUnderSampler(random_state=1, ratio=None, replacement=False,
          return_indices=False, sampling_strategy='auto')","TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=5000, min_df=1,
        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,
        stop_words=None, strip_accents=None, sublinear_tf=False,
        token_pattern='(?u)\\b\\w\\w+\\b', tokenizer=None, use_idf=True,
        vocabulary=None)",0.95,500,1,"{'clf': ComplementNB(alpha=1.0, class_prior=None, fit_prior=True, norm=False), 'smp': RandomUnderSampler(random_state=1, ratio=None, replacement=False,
          return_indices=False, sampling_strategy='auto'), 'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=5000, min_df=1,
        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,
        stop_words=None, strip_accents=None, sublinear_tf=False,
        token_pattern='(?u)\\b\\w\\w+\\b', tokenizer=None, use_idf=True,
        vocabulary=None), 'vec__max_df': 0.95, 'vec__max_features': 500, 'vec__min_df': 1}",0.15608882148494235,0.15975020568098738,0.1577005885019332,0.15784653855595432,0.0014983122997701236,91,0.1769607500669556,0.1801427315630858,0.17629747673690513,0.17780031945564886,0.00167832331775684
1.9278464317321777,0.020651394418524465,0.7543695767720541,0.015951053387319045,"ComplementNB(alpha=1.0, class_prior=None, fit_prior=True, norm=False)","RandomUnderSampler(random_state=1, ratio=None, replacement=False,
          return_indices=False, sampling_strategy='auto')","TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=5000, min_df=1,
        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,
        stop_words=None, strip_accents=None, sublinear_tf=False,
        token_pattern='(?u)\\b\\w\\w+\\b', tokenizer=None, use_idf=True,
        vocabulary=None)",0.95,500,0.05,"{'clf': ComplementNB(alpha=1.0, class_prior=None, fit_prior=True, norm=False), 'smp': RandomUnderSampler(random_state=1, ratio=None, replacement=False,
          return_indices=False, sampling_strategy='auto'), 'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=5000, min_df=1,
        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,
        stop_words=None, strip_accents=None, sublinear_tf=False,
        token_pattern='(?u)\\b\\w\\w+\\b', tokenizer=None, use_idf=True,
        vocabulary=None), 'vec__max_df': 0.95, 'vec__max_features': 500, 'vec__min_df': 0.05}",0.09797414360333398,0.07601759555353166,0.10640770685388994,0.09346648200358519,0.012809605147061282,169,0.1000967410743705,0.07892564208114629,0.10772310610807197,0.09558182975452957,0.012182276792265074
1.9535848299662273,0.03809082050641497,0.8442067305246989,0.006102760466079605,"ComplementNB(alpha=1.0, class_prior=None, fit_prior=True, norm=False)","RandomUnderSampler(random_state=1, ratio=None, replacement=False,
          return_indices=False, sampling_strategy='auto')","TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=5000, min_df=1,
        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,
        stop_words=None, strip_accents=None, sublinear_tf=False,
        token_pattern='(?u)\\b\\w\\w+\\b', tokenizer=None, use_idf=True,
        vocabulary=None)",0.95,5000,1,"{'clf': ComplementNB(alpha=1.0, class_prior=None, fit_prior=True, norm=False), 'smp': RandomUnderSampler(random_state=1, ratio=None, replacement=False,
          return_indices=False, sampling_strategy='auto'), 'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=5000, min_df=1,
        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,
        stop_words=None, strip_accents=None, sublinear_tf=False,
        token_pattern='(?u)\\b\\w\\w+\\b', tokenizer=None, use_idf=True,
        vocabulary=None), 'vec__max_df': 0.95, 'vec__max_features': 5000, 'vec__min_df': 1}",0.17341265701427655,0.17627014921257073,0.17102552206607724,0.17356944276430816,0.0021439783588761393,65,0.28914656739372835,0.29354332733072785,0.2899718560549493,0.29088725025980183,0.0019081115268958435
1.9095979531606038,0.012133867135584503,0.7586590449015299,0.008365229734957038,"ComplementNB(alpha=1.0, class_prior=None, fit_prior=True, norm=False)","RandomUnderSampler(random_state=1, ratio=None, replacement=False,
          return_indices=False, sampling_strategy='auto')","TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=5000, min_df=1,
        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,
        stop_words=None, strip_accents=None, sublinear_tf=False,
        token_pattern='(?u)\\b\\w\\w+\\b', tokenizer=None, use_idf=True,
        vocabulary=None)",0.95,5000,0.05,"{'clf': ComplementNB(alpha=1.0, class_prior=None, fit_prior=True, norm=False), 'smp': RandomUnderSampler(random_state=1, ratio=None, replacement=False,
          return_indices=False, sampling_strategy='auto'), 'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=5000, min_df=1,
        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,
        stop_words=None, strip_accents=None, sublinear_tf=False,
        token_pattern='(?u)\\b\\w\\w+\\b', tokenizer=None, use_idf=True,
        vocabulary=None), 'vec__max_df': 0.95, 'vec__max_features': 5000, 'vec__min_df': 0.05}",0.09797414360333398,0.07601759555353166,0.10640770685388994,0.09346648200358519,0.012809605147061282,169,0.1000967410743705,0.07892564208114629,0.10772310610807197,0.09558182975452957,0.012182276792265074
2.05462114016215,0.024823672047629544,0.8759247461954752,0.0005367538227599448,"ComplementNB(alpha=1.0, class_prior=None, fit_prior=True, norm=False)","RandomUnderSampler(random_state=1, ratio=None, replacement=False,
          return_indices=False, sampling_strategy='auto')","TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=5000, min_df=1,
        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,
        stop_words=None, strip_accents=None, sublinear_tf=False,
        token_pattern='(?u)\\b\\w\\w+\\b', tokenizer=None, use_idf=True,
        vocabulary=None)",0.95,50000,1,"{'clf': ComplementNB(alpha=1.0, class_prior=None, fit_prior=True, norm=False), 'smp': RandomUnderSampler(random_state=1, ratio=None, replacement=False,
          return_indices=False, sampling_strategy='auto'), 'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=5000, min_df=1,
        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,
        stop_words=None, strip_accents=None, sublinear_tf=False,
        token_pattern='(?u)\\b\\w\\w+\\b', tokenizer=None, use_idf=True,
        vocabulary=None), 'vec__max_df': 0.95, 'vec__max_features': 50000, 'vec__min_df': 1}",0.1622739104332211,0.1705768737219405,0.16481209204105712,0.16588762539873955,0.003473939055891386,85,0.4199687316297684,0.4373503839109756,0.43376209964185714,0.4303604050608671,0.007492623195425924
1.9009993076324463,0.02067295018372337,0.7682700951894125,0.01164579088910096,"ComplementNB(alpha=1.0, class_prior=None, fit_prior=True, norm=False)","RandomUnderSampler(random_state=1, ratio=None, replacement=False,
          return_indices=False, sampling_strategy='auto')","TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=5000, min_df=1,
        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,
        stop_words=None, strip_accents=None, sublinear_tf=False,
        token_pattern='(?u)\\b\\w\\w+\\b', tokenizer=None, use_idf=True,
        vocabulary=None)",0.95,50000,0.05,"{'clf': ComplementNB(alpha=1.0, class_prior=None, fit_prior=True, norm=False), 'smp': RandomUnderSampler(random_state=1, ratio=None, replacement=False,
          return_indices=False, sampling_strategy='auto'), 'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=5000, min_df=1,
        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,
        stop_words=None, strip_accents=None, sublinear_tf=False,
        token_pattern='(?u)\\b\\w\\w+\\b', tokenizer=None, use_idf=True,
        vocabulary=None), 'vec__max_df': 0.95, 'vec__max_features': 50000, 'vec__min_df': 0.05}",0.09797414360333398,0.07601759555353166,0.10640770685388994,0.09346648200358519,0.012809605147061282,169,0.1000967410743705,0.07892564208114629,0.10772310610807197,0.09558182975452957,0.012182276792265074
116.36419566472371,1.1866536501928866,0.8880027135213217,0.02176565379607573,"LinearSVC(C=1.0, class_weight=None, dual=False, fit_intercept=True,
     intercept_scaling=1, loss='squared_hinge', max_iter=1000,
     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,
     verbose=0)",,"CountVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=None, min_df=1,
        ngram_range=(1, 1), preprocessor=None, stop_words=None,
        strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)",1.0,,1,"{'clf': LinearSVC(C=1.0, class_weight=None, dual=False, fit_intercept=True,
     intercept_scaling=1, loss='squared_hinge', max_iter=1000,
     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,
     verbose=0), 'smp': None, 'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=None, min_df=1,
        ngram_range=(1, 1), preprocessor=None, stop_words=None,
        strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None), 'vec__max_df': 1.0, 'vec__max_features': None, 'vec__min_df': 1}",0.23763887439254283,0.2392518544171735,0.23588521758013592,0.23759198212995078,0.001374823639924496,25,0.9233272829064312,0.9247390451698873,0.9237995094419187,0.9239552791727457,0.0005867801001574016
5.761036316553752,0.08285079261435536,0.7674838701883951,0.008351418339799742,"LinearSVC(C=1.0, class_weight=None, dual=False, fit_intercept=True,
     intercept_scaling=1, loss='squared_hinge', max_iter=1000,
     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,
     verbose=0)",,"CountVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=None, min_df=1,
        ngram_range=(1, 1), preprocessor=None, stop_words=None,
        strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)",1.0,,0.05,"{'clf': LinearSVC(C=1.0, class_weight=None, dual=False, fit_intercept=True,
     intercept_scaling=1, loss='squared_hinge', max_iter=1000,
     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,
     verbose=0), 'smp': None, 'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=None, min_df=1,
        ngram_range=(1, 1), preprocessor=None, stop_words=None,
        strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None), 'vec__max_df': 1.0, 'vec__max_features': None, 'vec__min_df': 0.05}",0.10108289561254614,0.10243065216413022,0.1015516944668543,0.10168841408117689,0.0005586478439290339,129,0.10163710890687327,0.10255489562264217,0.10185544755786233,0.10201581736245925,0.00039146900224692566
32.71045653025309,2.3510813993164703,0.8460013071695963,0.0493366017616632,"LinearSVC(C=1.0, class_weight=None, dual=False, fit_intercept=True,
     intercept_scaling=1, loss='squared_hinge', max_iter=1000,
     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,
     verbose=0)",,"CountVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=None, min_df=1,
        ngram_range=(1, 1), preprocessor=None, stop_words=None,
        strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)",1.0,500,1,"{'clf': LinearSVC(C=1.0, class_weight=None, dual=False, fit_intercept=True,
     intercept_scaling=1, loss='squared_hinge', max_iter=1000,
     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,
     verbose=0), 'smp': None, 'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=None, min_df=1,
        ngram_range=(1, 1), preprocessor=None, stop_words=None,
        strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None), 'vec__max_df': 1.0, 'vec__max_features': 500, 'vec__min_df': 1}",0.21158820105182363,0.21821665395121534,0.2173562802457023,0.21572037841624708,0.0029429268941622007,35,0.23517518909495627,0.23200376844819506,0.22873953913073725,0.2319728322246295,0.0026274341606637784
4.991060336430867,0.16583245797657883,0.8054733276367188,0.033784700548594955,"LinearSVC(C=1.0, class_weight=None, dual=False, fit_intercept=True,
     intercept_scaling=1, loss='squared_hinge', max_iter=1000,
     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,
     verbose=0)",,"CountVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=None, min_df=1,
        ngram_range=(1, 1), preprocessor=None, stop_words=None,
        strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)",1.0,500,0.05,"{'clf': LinearSVC(C=1.0, class_weight=None, dual=False, fit_intercept=True,
     intercept_scaling=1, loss='squared_hinge', max_iter=1000,
     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,
     verbose=0), 'smp': None, 'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=None, min_df=1,
        ngram_range=(1, 1), preprocessor=None, stop_words=None,
        strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None), 'vec__max_df': 1.0, 'vec__max_features': 500, 'vec__min_df': 0.05}",0.10108289561254614,0.10243065216413022,0.1015516944668543,0.10168841408117689,0.0005586478439290339,129,0.10163710890687327,0.10255489562264217,0.10185544755786233,0.10201581736245925,0.00039146900224692566
50.42854539553324,0.6076597286485285,0.8497058550516764,0.004654051241122628,"LinearSVC(C=1.0, class_weight=None, dual=False, fit_intercept=True,
     intercept_scaling=1, loss='squared_hinge', max_iter=1000,
     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,
     verbose=0)",,"CountVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=None, min_df=1,
        ngram_range=(1, 1), preprocessor=None, stop_words=None,
        strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)",1.0,5000,1,"{'clf': LinearSVC(C=1.0, class_weight=None, dual=False, fit_intercept=True,
     intercept_scaling=1, loss='squared_hinge', max_iter=1000,
     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,
     verbose=0), 'smp': None, 'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=None, min_df=1,
        ngram_range=(1, 1), preprocessor=None, stop_words=None,
        strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None), 'vec__max_df': 1.0, 'vec__max_features': 5000, 'vec__min_df': 1}",0.24767283506186105,0.24826014989905512,0.24743911580173963,0.24779070025421862,0.0003453919511856794,17,0.5403265340159439,0.5366259598327083,0.5342769727578367,0.5370764888688296,0.0024901847539475375
5.180753310521443,0.03095043463679918,0.7619663874308268,0.004499122130862575,"LinearSVC(C=1.0, class_weight=None, dual=False, fit_intercept=True,
     intercept_scaling=1, loss='squared_hinge', max_iter=1000,
     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,
     verbose=0)",,"CountVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=None, min_df=1,
        ngram_range=(1, 1), preprocessor=None, stop_words=None,
        strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)",1.0,5000,0.05,"{'clf': LinearSVC(C=1.0, class_weight=None, dual=False, fit_intercept=True,
     intercept_scaling=1, loss='squared_hinge', max_iter=1000,
     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,
     verbose=0), 'smp': None, 'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=None, min_df=1,
        ngram_range=(1, 1), preprocessor=None, stop_words=None,
        strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None), 'vec__max_df': 1.0, 'vec__max_features': 5000, 'vec__min_df': 0.05}",0.10108289561254614,0.10243065216413022,0.1015516944668543,0.10168841408117689,0.0005586478439290339,129,0.10163710890687327,0.10255489562264217,0.10185544755786233,0.10201581736245925,0.00039146900224692566
115.98685296376546,1.3757060637250373,0.8870770136515299,0.015478889504012565,"LinearSVC(C=1.0, class_weight=None, dual=False, fit_intercept=True,
     intercept_scaling=1, loss='squared_hinge', max_iter=1000,
     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,
     verbose=0)",,"CountVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=None, min_df=1,
        ngram_range=(1, 1), preprocessor=None, stop_words=None,
        strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)",1.0,50000,1,"{'clf': LinearSVC(C=1.0, class_weight=None, dual=False, fit_intercept=True,
     intercept_scaling=1, loss='squared_hinge', max_iter=1000,
     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,
     verbose=0), 'smp': None, 'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=None, min_df=1,
        ngram_range=(1, 1), preprocessor=None, stop_words=None,
        strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None), 'vec__max_df': 1.0, 'vec__max_features': 50000, 'vec__min_df': 1}",0.23617896538855349,0.2393222030359746,0.2374973303258895,0.23766616625013917,0.0012887629479541706,23,0.9158016320461541,0.9174956693190389,0.9156753961889398,0.9163242325180443,0.0008299325301714267
6.017127672831218,0.19048884096458418,0.7726767063140869,0.06221473387336673,"LinearSVC(C=1.0, class_weight=None, dual=False, fit_intercept=True,
     intercept_scaling=1, loss='squared_hinge', max_iter=1000,
     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,
     verbose=0)",,"CountVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=None, min_df=1,
        ngram_range=(1, 1), preprocessor=None, stop_words=None,
        strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)",1.0,50000,0.05,"{'clf': LinearSVC(C=1.0, class_weight=None, dual=False, fit_intercept=True,
     intercept_scaling=1, loss='squared_hinge', max_iter=1000,
     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,
     verbose=0), 'smp': None, 'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=None, min_df=1,
        ngram_range=(1, 1), preprocessor=None, stop_words=None,
        strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None), 'vec__max_df': 1.0, 'vec__max_features': 50000, 'vec__min_df': 0.05}",0.10108289561254614,0.10243065216413022,0.1015516944668543,0.10168841408117689,0.0005586478439290339,129,0.10163710890687327,0.10255489562264217,0.10185544755786233,0.10201581736245925,0.00039146900224692566
116.23350429534912,3.942753191021184,0.8623309135437012,0.011143212239725342,"LinearSVC(C=1.0, class_weight=None, dual=False, fit_intercept=True,
     intercept_scaling=1, loss='squared_hinge', max_iter=1000,
     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,
     verbose=0)",,"CountVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=None, min_df=1,
        ngram_range=(1, 1), preprocessor=None, stop_words=None,
        strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)",0.95,,1,"{'clf': LinearSVC(C=1.0, class_weight=None, dual=False, fit_intercept=True,
     intercept_scaling=1, loss='squared_hinge', max_iter=1000,
     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,
     verbose=0), 'smp': None, 'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=None, min_df=1,
        ngram_range=(1, 1), preprocessor=None, stop_words=None,
        strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None), 'vec__max_df': 0.95, 'vec__max_features': None, 'vec__min_df': 1}",0.23763887439254283,0.2392518544171735,0.23588521758013592,0.23759198212995078,0.001374823639924496,25,0.9233272829064312,0.9247390451698873,0.9237995094419187,0.9239552791727457,0.0005867801001574016
5.948367913564046,0.03697032724628411,0.7554730574289957,0.030419823294976927,"LinearSVC(C=1.0, class_weight=None, dual=False, fit_intercept=True,
     intercept_scaling=1, loss='squared_hinge', max_iter=1000,
     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,
     verbose=0)",,"CountVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=None, min_df=1,
        ngram_range=(1, 1), preprocessor=None, stop_words=None,
        strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)",0.95,,0.05,"{'clf': LinearSVC(C=1.0, class_weight=None, dual=False, fit_intercept=True,
     intercept_scaling=1, loss='squared_hinge', max_iter=1000,
     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,
     verbose=0), 'smp': None, 'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=None, min_df=1,
        ngram_range=(1, 1), preprocessor=None, stop_words=None,
        strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None), 'vec__max_df': 0.95, 'vec__max_features': None, 'vec__min_df': 0.05}",0.10108289561254614,0.10243065216413022,0.1015516944668543,0.10168841408117689,0.0005586478439290339,129,0.10163710890687327,0.10255489562264217,0.10185544755786233,0.10201581736245925,0.00039146900224692566
33.50765426953634,2.3316619350190004,0.7984246412913004,0.011857225844733602,"LinearSVC(C=1.0, class_weight=None, dual=False, fit_intercept=True,
     intercept_scaling=1, loss='squared_hinge', max_iter=1000,
     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,
     verbose=0)",,"CountVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=None, min_df=1,
        ngram_range=(1, 1), preprocessor=None, stop_words=None,
        strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)",0.95,500,1,"{'clf': LinearSVC(C=1.0, class_weight=None, dual=False, fit_intercept=True,
     intercept_scaling=1, loss='squared_hinge', max_iter=1000,
     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,
     verbose=0), 'smp': None, 'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=None, min_df=1,
        ngram_range=(1, 1), preprocessor=None, stop_words=None,
        strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None), 'vec__max_df': 0.95, 'vec__max_features': 500, 'vec__min_df': 1}",0.21158820105182363,0.21821665395121534,0.2173562802457023,0.21572037841624708,0.0029429268941622007,35,0.23517518909495627,0.23200376844819506,0.22873953913073725,0.2319728322246295,0.0026274341606637784
5.671282052993774,0.039661206874701385,0.7289683024088541,0.011388772554582625,"LinearSVC(C=1.0, class_weight=None, dual=False, fit_intercept=True,
     intercept_scaling=1, loss='squared_hinge', max_iter=1000,
     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,
     verbose=0)",,"CountVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=None, min_df=1,
        ngram_range=(1, 1), preprocessor=None, stop_words=None,
        strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)",0.95,500,0.05,"{'clf': LinearSVC(C=1.0, class_weight=None, dual=False, fit_intercept=True,
     intercept_scaling=1, loss='squared_hinge', max_iter=1000,
     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,
     verbose=0), 'smp': None, 'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=None, min_df=1,
        ngram_range=(1, 1), preprocessor=None, stop_words=None,
        strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None), 'vec__max_df': 0.95, 'vec__max_features': 500, 'vec__min_df': 0.05}",0.10108289561254614,0.10243065216413022,0.1015516944668543,0.10168841408117689,0.0005586478439290339,129,0.10163710890687327,0.10255489562264217,0.10185544755786233,0.10201581736245925,0.00039146900224692566
48.430281241734825,0.6429632658345379,0.8489324251810709,0.0010304465760438094,"LinearSVC(C=1.0, class_weight=None, dual=False, fit_intercept=True,
     intercept_scaling=1, loss='squared_hinge', max_iter=1000,
     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,
     verbose=0)",,"CountVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=None, min_df=1,
        ngram_range=(1, 1), preprocessor=None, stop_words=None,
        strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)",0.95,5000,1,"{'clf': LinearSVC(C=1.0, class_weight=None, dual=False, fit_intercept=True,
     intercept_scaling=1, loss='squared_hinge', max_iter=1000,
     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,
     verbose=0), 'smp': None, 'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=None, min_df=1,
        ngram_range=(1, 1), preprocessor=None, stop_words=None,
        strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None), 'vec__max_df': 0.95, 'vec__max_features': 5000, 'vec__min_df': 1}",0.24767283506186105,0.24826014989905512,0.24743911580173963,0.24779070025421862,0.0003453919511856794,17,0.5403265340159439,0.5366259598327083,0.5342769727578367,0.5370764888688296,0.0024901847539475375
5.008137067159017,0.04595083621139662,0.7395486831665039,0.006308227253834212,"LinearSVC(C=1.0, class_weight=None, dual=False, fit_intercept=True,
     intercept_scaling=1, loss='squared_hinge', max_iter=1000,
     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,
     verbose=0)",,"CountVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=None, min_df=1,
        ngram_range=(1, 1), preprocessor=None, stop_words=None,
        strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)",0.95,5000,0.05,"{'clf': LinearSVC(C=1.0, class_weight=None, dual=False, fit_intercept=True,
     intercept_scaling=1, loss='squared_hinge', max_iter=1000,
     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,
     verbose=0), 'smp': None, 'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=None, min_df=1,
        ngram_range=(1, 1), preprocessor=None, stop_words=None,
        strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None), 'vec__max_df': 0.95, 'vec__max_features': 5000, 'vec__min_df': 0.05}",0.10108289561254614,0.10243065216413022,0.1015516944668543,0.10168841408117689,0.0005586478439290339,129,0.10163710890687327,0.10255489562264217,0.10185544755786233,0.10201581736245925,0.00039146900224692566
110.45175997416179,1.1527636146675904,0.8878750006357828,0.02331318939827411,"LinearSVC(C=1.0, class_weight=None, dual=False, fit_intercept=True,
     intercept_scaling=1, loss='squared_hinge', max_iter=1000,
     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,
     verbose=0)",,"CountVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=None, min_df=1,
        ngram_range=(1, 1), preprocessor=None, stop_words=None,
        strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)",0.95,50000,1,"{'clf': LinearSVC(C=1.0, class_weight=None, dual=False, fit_intercept=True,
     intercept_scaling=1, loss='squared_hinge', max_iter=1000,
     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,
     verbose=0), 'smp': None, 'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=None, min_df=1,
        ngram_range=(1, 1), preprocessor=None, stop_words=None,
        strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None), 'vec__max_df': 0.95, 'vec__max_features': 50000, 'vec__min_df': 1}",0.23617896538855349,0.2393222030359746,0.2374973303258895,0.23766616625013917,0.0012887629479541706,23,0.9158016320461541,0.9174956693190389,0.9156753961889398,0.9163242325180443,0.0008299325301714267
5.955329259236653,0.1732357930156103,0.7468359470367432,0.0043234828202050575,"LinearSVC(C=1.0, class_weight=None, dual=False, fit_intercept=True,
     intercept_scaling=1, loss='squared_hinge', max_iter=1000,
     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,
     verbose=0)",,"CountVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=None, min_df=1,
        ngram_range=(1, 1), preprocessor=None, stop_words=None,
        strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)",0.95,50000,0.05,"{'clf': LinearSVC(C=1.0, class_weight=None, dual=False, fit_intercept=True,
     intercept_scaling=1, loss='squared_hinge', max_iter=1000,
     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,
     verbose=0), 'smp': None, 'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=None, min_df=1,
        ngram_range=(1, 1), preprocessor=None, stop_words=None,
        strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None), 'vec__max_df': 0.95, 'vec__max_features': 50000, 'vec__min_df': 0.05}",0.10108289561254614,0.10243065216413022,0.1015516944668543,0.10168841408117689,0.0005586478439290339,129,0.10163710890687327,0.10255489562264217,0.10185544755786233,0.10201581736245925,0.00039146900224692566
29.383270978927612,1.8373071108017784,0.917413075764974,0.02396009829155878,"LinearSVC(C=1.0, class_weight=None, dual=False, fit_intercept=True,
     intercept_scaling=1, loss='squared_hinge', max_iter=1000,
     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,
     verbose=0)",,"TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=5000, min_df=1,
        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,
        stop_words=None, strip_accents=None, sublinear_tf=False,
        token_pattern='(?u)\\b\\w\\w+\\b', tokenizer=None, use_idf=True,
        vocabulary=None)",1.0,,1,"{'clf': LinearSVC(C=1.0, class_weight=None, dual=False, fit_intercept=True,
     intercept_scaling=1, loss='squared_hinge', max_iter=1000,
     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,
     verbose=0), 'smp': None, 'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=5000, min_df=1,
        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,
        stop_words=None, strip_accents=None, sublinear_tf=False,
        token_pattern='(?u)\\b\\w\\w+\\b', tokenizer=None, use_idf=True,
        vocabulary=None), 'vec__max_df': 1.0, 'vec__max_features': None, 'vec__min_df': 1}",0.2543345584816768,0.25779769658752194,0.2537318365519675,0.25528803054038873,0.0017915795976805803,5,0.8766216148903059,0.878916854562793,0.875766401063092,0.8771016235053969,0.001330199362593396
4.704434315363566,0.272926215180468,0.7561842600504557,0.0036833406290989225,"LinearSVC(C=1.0, class_weight=None, dual=False, fit_intercept=True,
     intercept_scaling=1, loss='squared_hinge', max_iter=1000,
     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,
     verbose=0)",,"TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=5000, min_df=1,
        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,
        stop_words=None, strip_accents=None, sublinear_tf=False,
        token_pattern='(?u)\\b\\w\\w+\\b', tokenizer=None, use_idf=True,
        vocabulary=None)",1.0,,0.05,"{'clf': LinearSVC(C=1.0, class_weight=None, dual=False, fit_intercept=True,
     intercept_scaling=1, loss='squared_hinge', max_iter=1000,
     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,
     verbose=0), 'smp': None, 'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=5000, min_df=1,
        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,
        stop_words=None, strip_accents=None, sublinear_tf=False,
        token_pattern='(?u)\\b\\w\\w+\\b', tokenizer=None, use_idf=True,
        vocabulary=None), 'vec__max_df': 1.0, 'vec__max_features': None, 'vec__min_df': 0.05}",0.10267778640428495,0.10035929409658162,0.09964858164192007,0.10089522071426221,0.001293427843042775,137,0.10108374919704596,0.10103957873285632,0.10121653558779363,0.10111328783923197,7.520119974207232e-05
14.76440421740214,0.06453207624095299,0.8175614674886068,0.029978098707436715,"LinearSVC(C=1.0, class_weight=None, dual=False, fit_intercept=True,
     intercept_scaling=1, loss='squared_hinge', max_iter=1000,
     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,
     verbose=0)",,"TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=5000, min_df=1,
        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,
        stop_words=None, strip_accents=None, sublinear_tf=False,
        token_pattern='(?u)\\b\\w\\w+\\b', tokenizer=None, use_idf=True,
        vocabulary=None)",1.0,500,1,"{'clf': LinearSVC(C=1.0, class_weight=None, dual=False, fit_intercept=True,
     intercept_scaling=1, loss='squared_hinge', max_iter=1000,
     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,
     verbose=0), 'smp': None, 'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=5000, min_df=1,
        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,
        stop_words=None, strip_accents=None, sublinear_tf=False,
        token_pattern='(?u)\\b\\w\\w+\\b', tokenizer=None, use_idf=True,
        vocabulary=None), 'vec__max_df': 1.0, 'vec__max_features': 500, 'vec__min_df': 1}",0.21456071962917297,0.21938463633903313,0.21949289871003336,0.21781275155941315,0.0022999585419045993,33,0.23282674651041863,0.23105750861463384,0.22874948965225037,0.23087791492576762,0.001669370398619484
4.044908920923869,0.2375762275119065,0.73248291015625,0.030841119842556153,"LinearSVC(C=1.0, class_weight=None, dual=False, fit_intercept=True,
     intercept_scaling=1, loss='squared_hinge', max_iter=1000,
     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,
     verbose=0)",,"TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=5000, min_df=1,
        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,
        stop_words=None, strip_accents=None, sublinear_tf=False,
        token_pattern='(?u)\\b\\w\\w+\\b', tokenizer=None, use_idf=True,
        vocabulary=None)",1.0,500,0.05,"{'clf': LinearSVC(C=1.0, class_weight=None, dual=False, fit_intercept=True,
     intercept_scaling=1, loss='squared_hinge', max_iter=1000,
     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,
     verbose=0), 'smp': None, 'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=5000, min_df=1,
        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,
        stop_words=None, strip_accents=None, sublinear_tf=False,
        token_pattern='(?u)\\b\\w\\w+\\b', tokenizer=None, use_idf=True,
        vocabulary=None), 'vec__max_df': 1.0, 'vec__max_features': 500, 'vec__min_df': 0.05}",0.10267778640428495,0.10035929409658162,0.09964858164192007,0.10089522071426221,0.001293427843042775,137,0.10108374919704596,0.10103957873285632,0.10121653558779363,0.10111328783923197,7.520119974207232e-05
17.344868342081707,0.07548053150424763,0.8428537845611572,0.007691906596184885,"LinearSVC(C=1.0, class_weight=None, dual=False, fit_intercept=True,
     intercept_scaling=1, loss='squared_hinge', max_iter=1000,
     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,
     verbose=0)",,"TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=5000, min_df=1,
        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,
        stop_words=None, strip_accents=None, sublinear_tf=False,
        token_pattern='(?u)\\b\\w\\w+\\b', tokenizer=None, use_idf=True,
        vocabulary=None)",1.0,5000,1,"{'clf': LinearSVC(C=1.0, class_weight=None, dual=False, fit_intercept=True,
     intercept_scaling=1, loss='squared_hinge', max_iter=1000,
     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,
     verbose=0), 'smp': None, 'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=5000, min_df=1,
        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,
        stop_words=None, strip_accents=None, sublinear_tf=False,
        token_pattern='(?u)\\b\\w\\w+\\b', tokenizer=None, use_idf=True,
        vocabulary=None), 'vec__max_df': 1.0, 'vec__max_features': 5000, 'vec__min_df': 1}",0.25796895092382005,0.2578885571042492,0.25851875974178296,0.25812542258995075,0.0002800611573863987,1,0.4861921378194445,0.48044780988470726,0.47860635512556016,0.481748767609904,0.0032306235746648536
4.2402260303497314,0.33072774034406105,0.7419830163319906,0.012319332046700512,"LinearSVC(C=1.0, class_weight=None, dual=False, fit_intercept=True,
     intercept_scaling=1, loss='squared_hinge', max_iter=1000,
     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,
     verbose=0)",,"TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=5000, min_df=1,
        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,
        stop_words=None, strip_accents=None, sublinear_tf=False,
        token_pattern='(?u)\\b\\w\\w+\\b', tokenizer=None, use_idf=True,
        vocabulary=None)",1.0,5000,0.05,"{'clf': LinearSVC(C=1.0, class_weight=None, dual=False, fit_intercept=True,
     intercept_scaling=1, loss='squared_hinge', max_iter=1000,
     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,
     verbose=0), 'smp': None, 'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=5000, min_df=1,
        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,
        stop_words=None, strip_accents=None, sublinear_tf=False,
        token_pattern='(?u)\\b\\w\\w+\\b', tokenizer=None, use_idf=True,
        vocabulary=None), 'vec__max_df': 1.0, 'vec__max_features': 5000, 'vec__min_df': 0.05}",0.10267778640428495,0.10035929409658162,0.09964858164192007,0.10089522071426221,0.001293427843042775,137,0.10108374919704596,0.10103957873285632,0.10121653558779363,0.10111328783923197,7.520119974207232e-05
29.33319115638733,0.25418764679995026,1.1053425470987956,0.056790348167665906,"LinearSVC(C=1.0, class_weight=None, dual=False, fit_intercept=True,
     intercept_scaling=1, loss='squared_hinge', max_iter=1000,
     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,
     verbose=0)",,"TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=5000, min_df=1,
        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,
        stop_words=None, strip_accents=None, sublinear_tf=False,
        token_pattern='(?u)\\b\\w\\w+\\b', tokenizer=None, use_idf=True,
        vocabulary=None)",1.0,50000,1,"{'clf': LinearSVC(C=1.0, class_weight=None, dual=False, fit_intercept=True,
     intercept_scaling=1, loss='squared_hinge', max_iter=1000,
     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,
     verbose=0), 'smp': None, 'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=5000, min_df=1,
        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,
        stop_words=None, strip_accents=None, sublinear_tf=False,
        token_pattern='(?u)\\b\\w\\w+\\b', tokenizer=None, use_idf=True,
        vocabulary=None), 'vec__max_df': 1.0, 'vec__max_features': 50000, 'vec__min_df': 1}",0.2536888211394467,0.25724830667701093,0.25362048756874894,0.2548525384617355,0.0016942936335857923,7,0.8664100590189077,0.8691535638546602,0.865865583807588,0.8671430688937186,0.0014389071790573216
5.157911062240601,0.17133198479115164,0.7627785205841064,0.024754222089417407,"LinearSVC(C=1.0, class_weight=None, dual=False, fit_intercept=True,
     intercept_scaling=1, loss='squared_hinge', max_iter=1000,
     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,
     verbose=0)",,"TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=5000, min_df=1,
        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,
        stop_words=None, strip_accents=None, sublinear_tf=False,
        token_pattern='(?u)\\b\\w\\w+\\b', tokenizer=None, use_idf=True,
        vocabulary=None)",1.0,50000,0.05,"{'clf': LinearSVC(C=1.0, class_weight=None, dual=False, fit_intercept=True,
     intercept_scaling=1, loss='squared_hinge', max_iter=1000,
     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,
     verbose=0), 'smp': None, 'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=5000, min_df=1,
        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,
        stop_words=None, strip_accents=None, sublinear_tf=False,
        token_pattern='(?u)\\b\\w\\w+\\b', tokenizer=None, use_idf=True,
        vocabulary=None), 'vec__max_df': 1.0, 'vec__max_features': 50000, 'vec__min_df': 0.05}",0.10267778640428495,0.10035929409658162,0.09964858164192007,0.10089522071426221,0.001293427843042775,137,0.10108374919704596,0.10103957873285632,0.10121653558779363,0.10111328783923197,7.520119974207232e-05
30.414665778477985,0.3972003528663304,0.892715851465861,0.001317657938335519,"LinearSVC(C=1.0, class_weight=None, dual=False, fit_intercept=True,
     intercept_scaling=1, loss='squared_hinge', max_iter=1000,
     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,
     verbose=0)",,"TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=5000, min_df=1,
        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,
        stop_words=None, strip_accents=None, sublinear_tf=False,
        token_pattern='(?u)\\b\\w\\w+\\b', tokenizer=None, use_idf=True,
        vocabulary=None)",0.95,,1,"{'clf': LinearSVC(C=1.0, class_weight=None, dual=False, fit_intercept=True,
     intercept_scaling=1, loss='squared_hinge', max_iter=1000,
     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,
     verbose=0), 'smp': None, 'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=5000, min_df=1,
        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,
        stop_words=None, strip_accents=None, sublinear_tf=False,
        token_pattern='(?u)\\b\\w\\w+\\b', tokenizer=None, use_idf=True,
        vocabulary=None), 'vec__max_df': 0.95, 'vec__max_features': None, 'vec__min_df': 1}",0.2543345584816768,0.25779769658752194,0.2537318365519675,0.25528803054038873,0.0017915795976805803,5,0.8766216148903059,0.878916854562793,0.875766401063092,0.8771016235053969,0.001330199362593396
5.463089466094971,0.1877331529058036,0.7866112391153971,0.017807917930035357,"LinearSVC(C=1.0, class_weight=None, dual=False, fit_intercept=True,
     intercept_scaling=1, loss='squared_hinge', max_iter=1000,
     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,
     verbose=0)",,"TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=5000, min_df=1,
        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,
        stop_words=None, strip_accents=None, sublinear_tf=False,
        token_pattern='(?u)\\b\\w\\w+\\b', tokenizer=None, use_idf=True,
        vocabulary=None)",0.95,,0.05,"{'clf': LinearSVC(C=1.0, class_weight=None, dual=False, fit_intercept=True,
     intercept_scaling=1, loss='squared_hinge', max_iter=1000,
     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,
     verbose=0), 'smp': None, 'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=5000, min_df=1,
        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,
        stop_words=None, strip_accents=None, sublinear_tf=False,
        token_pattern='(?u)\\b\\w\\w+\\b', tokenizer=None, use_idf=True,
        vocabulary=None), 'vec__max_df': 0.95, 'vec__max_features': None, 'vec__min_df': 0.05}",0.10267778640428495,0.10035929409658162,0.09964858164192007,0.10089522071426221,0.001293427843042775,137,0.10108374919704596,0.10103957873285632,0.10121653558779363,0.10111328783923197,7.520119974207232e-05
14.990273157755533,0.14064038802419299,0.8177211284637451,0.02495850914514913,"LinearSVC(C=1.0, class_weight=None, dual=False, fit_intercept=True,
     intercept_scaling=1, loss='squared_hinge', max_iter=1000,
     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,
     verbose=0)",,"TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=5000, min_df=1,
        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,
        stop_words=None, strip_accents=None, sublinear_tf=False,
        token_pattern='(?u)\\b\\w\\w+\\b', tokenizer=None, use_idf=True,
        vocabulary=None)",0.95,500,1,"{'clf': LinearSVC(C=1.0, class_weight=None, dual=False, fit_intercept=True,
     intercept_scaling=1, loss='squared_hinge', max_iter=1000,
     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,
     verbose=0), 'smp': None, 'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=5000, min_df=1,
        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,
        stop_words=None, strip_accents=None, sublinear_tf=False,
        token_pattern='(?u)\\b\\w\\w+\\b', tokenizer=None, use_idf=True,
        vocabulary=None), 'vec__max_df': 0.95, 'vec__max_features': 500, 'vec__min_df': 1}",0.21456071962917297,0.21938463633903313,0.21949289871003336,0.21781275155941315,0.0022999585419045993,33,0.23282674651041863,0.23105750861463384,0.22874948965225037,0.23087791492576762,0.001669370398619484
4.334494670232137,0.6431328979329002,0.7577840487162272,0.01776133646780687,"LinearSVC(C=1.0, class_weight=None, dual=False, fit_intercept=True,
     intercept_scaling=1, loss='squared_hinge', max_iter=1000,
     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,
     verbose=0)",,"TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=5000, min_df=1,
        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,
        stop_words=None, strip_accents=None, sublinear_tf=False,
        token_pattern='(?u)\\b\\w\\w+\\b', tokenizer=None, use_idf=True,
        vocabulary=None)",0.95,500,0.05,"{'clf': LinearSVC(C=1.0, class_weight=None, dual=False, fit_intercept=True,
     intercept_scaling=1, loss='squared_hinge', max_iter=1000,
     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,
     verbose=0), 'smp': None, 'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=5000, min_df=1,
        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,
        stop_words=None, strip_accents=None, sublinear_tf=False,
        token_pattern='(?u)\\b\\w\\w+\\b', tokenizer=None, use_idf=True,
        vocabulary=None), 'vec__max_df': 0.95, 'vec__max_features': 500, 'vec__min_df': 0.05}",0.10267778640428495,0.10035929409658162,0.09964858164192007,0.10089522071426221,0.001293427843042775,137,0.10108374919704596,0.10103957873285632,0.10121653558779363,0.10111328783923197,7.520119974207232e-05
17.205318609873455,0.21630060558766528,0.8819089730580648,0.01259520376989268,"LinearSVC(C=1.0, class_weight=None, dual=False, fit_intercept=True,
     intercept_scaling=1, loss='squared_hinge', max_iter=1000,
     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,
     verbose=0)",,"TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=5000, min_df=1,
        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,
        stop_words=None, strip_accents=None, sublinear_tf=False,
        token_pattern='(?u)\\b\\w\\w+\\b', tokenizer=None, use_idf=True,
        vocabulary=None)",0.95,5000,1,"{'clf': LinearSVC(C=1.0, class_weight=None, dual=False, fit_intercept=True,
     intercept_scaling=1, loss='squared_hinge', max_iter=1000,
     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,
     verbose=0), 'smp': None, 'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=5000, min_df=1,
        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,
        stop_words=None, strip_accents=None, sublinear_tf=False,
        token_pattern='(?u)\\b\\w\\w+\\b', tokenizer=None, use_idf=True,
        vocabulary=None), 'vec__max_df': 0.95, 'vec__max_features': 5000, 'vec__min_df': 1}",0.25796895092382005,0.2578885571042492,0.25851875974178296,0.25812542258995075,0.0002800611573863987,1,0.4861921378194445,0.48044780988470726,0.47860635512556016,0.481748767609904,0.0032306235746648536
4.4015763600667315,0.0331527946474713,0.7770659128824869,0.015111755532335657,"LinearSVC(C=1.0, class_weight=None, dual=False, fit_intercept=True,
     intercept_scaling=1, loss='squared_hinge', max_iter=1000,
     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,
     verbose=0)",,"TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=5000, min_df=1,
        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,
        stop_words=None, strip_accents=None, sublinear_tf=False,
        token_pattern='(?u)\\b\\w\\w+\\b', tokenizer=None, use_idf=True,
        vocabulary=None)",0.95,5000,0.05,"{'clf': LinearSVC(C=1.0, class_weight=None, dual=False, fit_intercept=True,
     intercept_scaling=1, loss='squared_hinge', max_iter=1000,
     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,
     verbose=0), 'smp': None, 'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=5000, min_df=1,
        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,
        stop_words=None, strip_accents=None, sublinear_tf=False,
        token_pattern='(?u)\\b\\w\\w+\\b', tokenizer=None, use_idf=True,
        vocabulary=None), 'vec__max_df': 0.95, 'vec__max_features': 5000, 'vec__min_df': 0.05}",0.10267778640428495,0.10035929409658162,0.09964858164192007,0.10089522071426221,0.001293427843042775,137,0.10108374919704596,0.10103957873285632,0.10121653558779363,0.10111328783923197,7.520119974207232e-05
28.4199275970459,0.3337849354682928,0.9184463024139404,0.017449958384829493,"LinearSVC(C=1.0, class_weight=None, dual=False, fit_intercept=True,
     intercept_scaling=1, loss='squared_hinge', max_iter=1000,
     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,
     verbose=0)",,"TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=5000, min_df=1,
        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,
        stop_words=None, strip_accents=None, sublinear_tf=False,
        token_pattern='(?u)\\b\\w\\w+\\b', tokenizer=None, use_idf=True,
        vocabulary=None)",0.95,50000,1,"{'clf': LinearSVC(C=1.0, class_weight=None, dual=False, fit_intercept=True,
     intercept_scaling=1, loss='squared_hinge', max_iter=1000,
     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,
     verbose=0), 'smp': None, 'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=5000, min_df=1,
        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,
        stop_words=None, strip_accents=None, sublinear_tf=False,
        token_pattern='(?u)\\b\\w\\w+\\b', tokenizer=None, use_idf=True,
        vocabulary=None), 'vec__max_df': 0.95, 'vec__max_features': 50000, 'vec__min_df': 1}",0.2536888211394467,0.25724830667701093,0.25362048756874894,0.2548525384617355,0.0016942936335857923,7,0.8664100590189077,0.8691535638546602,0.865865583807588,0.8671430688937186,0.0014389071790573216
5.19191296895345,0.04277511903478488,0.7568072477976481,0.01115298000855805,"LinearSVC(C=1.0, class_weight=None, dual=False, fit_intercept=True,
     intercept_scaling=1, loss='squared_hinge', max_iter=1000,
     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,
     verbose=0)",,"TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=5000, min_df=1,
        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,
        stop_words=None, strip_accents=None, sublinear_tf=False,
        token_pattern='(?u)\\b\\w\\w+\\b', tokenizer=None, use_idf=True,
        vocabulary=None)",0.95,50000,0.05,"{'clf': LinearSVC(C=1.0, class_weight=None, dual=False, fit_intercept=True,
     intercept_scaling=1, loss='squared_hinge', max_iter=1000,
     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,
     verbose=0), 'smp': None, 'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=5000, min_df=1,
        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,
        stop_words=None, strip_accents=None, sublinear_tf=False,
        token_pattern='(?u)\\b\\w\\w+\\b', tokenizer=None, use_idf=True,
        vocabulary=None), 'vec__max_df': 0.95, 'vec__max_features': 50000, 'vec__min_df': 0.05}",0.10267778640428495,0.10035929409658162,0.09964858164192007,0.10089522071426221,0.001293427843042775,137,0.10108374919704596,0.10103957873285632,0.10121653558779363,0.10111328783923197,7.520119974207232e-05
33.015069007873535,2.5626420095682785,0.963330348332723,0.03871591711957833,"LinearSVC(C=1.0, class_weight=None, dual=False, fit_intercept=True,
     intercept_scaling=1, loss='squared_hinge', max_iter=1000,
     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,
     verbose=0)","RandomUnderSampler(random_state=1, ratio=None, replacement=False,
          return_indices=False, sampling_strategy='auto')","CountVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=None, min_df=1,
        ngram_range=(1, 1), preprocessor=None, stop_words=None,
        strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)",1.0,,1,"{'clf': LinearSVC(C=1.0, class_weight=None, dual=False, fit_intercept=True,
     intercept_scaling=1, loss='squared_hinge', max_iter=1000,
     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,
     verbose=0), 'smp': RandomUnderSampler(random_state=1, ratio=None, replacement=False,
          return_indices=False, sampling_strategy='auto'), 'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=None, min_df=1,
        ngram_range=(1, 1), preprocessor=None, stop_words=None,
        strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None), 'vec__max_df': 1.0, 'vec__max_features': None, 'vec__min_df': 1}",0.16609027347349656,0.17178977909810356,0.16500364863764239,0.16762790040308084,0.002976140015471585,73,0.47439002599842406,0.49269585656913245,0.4880799594056165,0.48505528065772435,0.007773346313604086
2.953730662663778,0.17910513982109205,0.8190526962280273,0.028512097001239883,"LinearSVC(C=1.0, class_weight=None, dual=False, fit_intercept=True,
     intercept_scaling=1, loss='squared_hinge', max_iter=1000,
     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,
     verbose=0)","RandomUnderSampler(random_state=1, ratio=None, replacement=False,
          return_indices=False, sampling_strategy='auto')","CountVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=None, min_df=1,
        ngram_range=(1, 1), preprocessor=None, stop_words=None,
        strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)",1.0,,0.05,"{'clf': LinearSVC(C=1.0, class_weight=None, dual=False, fit_intercept=True,
     intercept_scaling=1, loss='squared_hinge', max_iter=1000,
     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,
     verbose=0), 'smp': RandomUnderSampler(random_state=1, ratio=None, replacement=False,
          return_indices=False, sampling_strategy='auto'), 'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=None, min_df=1,
        ngram_range=(1, 1), preprocessor=None, stop_words=None,
        strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None), 'vec__max_df': 1.0, 'vec__max_features': None, 'vec__min_df': 0.05}",0.1035611582014731,0.07561287413815306,0.1059911669041555,0.09505506641459388,0.013783453025502468,153,0.10432658978661381,0.07784265649675222,0.10840944090456013,0.09685956239597539,0.013549894238154546
6.600110451380412,0.12723525151608417,0.8075376351674398,0.028267654875487644,"LinearSVC(C=1.0, class_weight=None, dual=False, fit_intercept=True,
     intercept_scaling=1, loss='squared_hinge', max_iter=1000,
     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,
     verbose=0)","RandomUnderSampler(random_state=1, ratio=None, replacement=False,
          return_indices=False, sampling_strategy='auto')","CountVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=None, min_df=1,
        ngram_range=(1, 1), preprocessor=None, stop_words=None,
        strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)",1.0,500,1,"{'clf': LinearSVC(C=1.0, class_weight=None, dual=False, fit_intercept=True,
     intercept_scaling=1, loss='squared_hinge', max_iter=1000,
     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,
     verbose=0), 'smp': RandomUnderSampler(random_state=1, ratio=None, replacement=False,
          return_indices=False, sampling_strategy='auto'), 'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=None, min_df=1,
        ngram_range=(1, 1), preprocessor=None, stop_words=None,
        strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None), 'vec__max_df': 1.0, 'vec__max_features': 500, 'vec__min_df': 1}",0.1622812229072431,0.17111740549319263,0.16910918668597066,0.16750260502880213,0.003782006446224214,75,0.19953454674331672,0.1993095370410474,0.19745977933231673,0.19876795437222694,0.0009295693600845171
2.382697502772013,0.04915218781529661,0.7548824151357015,0.01850733636476182,"LinearSVC(C=1.0, class_weight=None, dual=False, fit_intercept=True,
     intercept_scaling=1, loss='squared_hinge', max_iter=1000,
     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,
     verbose=0)","RandomUnderSampler(random_state=1, ratio=None, replacement=False,
          return_indices=False, sampling_strategy='auto')","CountVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=None, min_df=1,
        ngram_range=(1, 1), preprocessor=None, stop_words=None,
        strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)",1.0,500,0.05,"{'clf': LinearSVC(C=1.0, class_weight=None, dual=False, fit_intercept=True,
     intercept_scaling=1, loss='squared_hinge', max_iter=1000,
     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,
     verbose=0), 'smp': RandomUnderSampler(random_state=1, ratio=None, replacement=False,
          return_indices=False, sampling_strategy='auto'), 'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=None, min_df=1,
        ngram_range=(1, 1), preprocessor=None, stop_words=None,
        strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None), 'vec__max_df': 1.0, 'vec__max_features': 500, 'vec__min_df': 0.05}",0.1035611582014731,0.07561287413815306,0.1059911669041555,0.09505506641459388,0.013783453025502468,153,0.10432658978661381,0.07784265649675222,0.10840944090456013,0.09685956239597539,0.013549894238154546
10.030560890833536,0.6330845053544264,0.8749612172444662,0.045444138176986386,"LinearSVC(C=1.0, class_weight=None, dual=False, fit_intercept=True,
     intercept_scaling=1, loss='squared_hinge', max_iter=1000,
     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,
     verbose=0)","RandomUnderSampler(random_state=1, ratio=None, replacement=False,
          return_indices=False, sampling_strategy='auto')","CountVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=None, min_df=1,
        ngram_range=(1, 1), preprocessor=None, stop_words=None,
        strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)",1.0,5000,1,"{'clf': LinearSVC(C=1.0, class_weight=None, dual=False, fit_intercept=True,
     intercept_scaling=1, loss='squared_hinge', max_iter=1000,
     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,
     verbose=0), 'smp': RandomUnderSampler(random_state=1, ratio=None, replacement=False,
          return_indices=False, sampling_strategy='auto'), 'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=None, min_df=1,
        ngram_range=(1, 1), preprocessor=None, stop_words=None,
        strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None), 'vec__max_df': 1.0, 'vec__max_features': 5000, 'vec__min_df': 1}",0.16518441016702784,0.17076936235217388,0.16503191307538512,0.16699522853152896,0.002669441691046004,81,0.3922051943452591,0.3997886660026019,0.3961201885679299,0.39603801630526364,0.0030964845448771053
2.8392879962921143,0.20263057584319186,0.7824440797170004,0.04511237990496384,"LinearSVC(C=1.0, class_weight=None, dual=False, fit_intercept=True,
     intercept_scaling=1, loss='squared_hinge', max_iter=1000,
     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,
     verbose=0)","RandomUnderSampler(random_state=1, ratio=None, replacement=False,
          return_indices=False, sampling_strategy='auto')","CountVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=None, min_df=1,
        ngram_range=(1, 1), preprocessor=None, stop_words=None,
        strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)",1.0,5000,0.05,"{'clf': LinearSVC(C=1.0, class_weight=None, dual=False, fit_intercept=True,
     intercept_scaling=1, loss='squared_hinge', max_iter=1000,
     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,
     verbose=0), 'smp': RandomUnderSampler(random_state=1, ratio=None, replacement=False,
          return_indices=False, sampling_strategy='auto'), 'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=None, min_df=1,
        ngram_range=(1, 1), preprocessor=None, stop_words=None,
        strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None), 'vec__max_df': 1.0, 'vec__max_features': 5000, 'vec__min_df': 0.05}",0.1035611582014731,0.07561287413815306,0.1059911669041555,0.09505506641459388,0.013783453025502468,153,0.10432658978661381,0.07784265649675222,0.10840944090456013,0.09685956239597539,0.013549894238154546
32.255182107289635,1.8953264398951162,0.9286282062530518,0.023733182583495105,"LinearSVC(C=1.0, class_weight=None, dual=False, fit_intercept=True,
     intercept_scaling=1, loss='squared_hinge', max_iter=1000,
     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,
     verbose=0)","RandomUnderSampler(random_state=1, ratio=None, replacement=False,
          return_indices=False, sampling_strategy='auto')","CountVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=None, min_df=1,
        ngram_range=(1, 1), preprocessor=None, stop_words=None,
        strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)",1.0,50000,1,"{'clf': LinearSVC(C=1.0, class_weight=None, dual=False, fit_intercept=True,
     intercept_scaling=1, loss='squared_hinge', max_iter=1000,
     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,
     verbose=0), 'smp': RandomUnderSampler(random_state=1, ratio=None, replacement=False,
          return_indices=False, sampling_strategy='auto'), 'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=None, min_df=1,
        ngram_range=(1, 1), preprocessor=None, stop_words=None,
        strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None), 'vec__max_df': 1.0, 'vec__max_features': 50000, 'vec__min_df': 1}",0.16706757065624614,0.17131892710041857,0.1669195483695667,0.1684353487087438,0.002039893118938452,71,0.472539087478945,0.49159378676806587,0.4865635592929922,0.48356547784666776,0.00806274397124627
2.749017079671224,0.09096731242859059,0.8634466330210367,0.0874694878200697,"LinearSVC(C=1.0, class_weight=None, dual=False, fit_intercept=True,
     intercept_scaling=1, loss='squared_hinge', max_iter=1000,
     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,
     verbose=0)","RandomUnderSampler(random_state=1, ratio=None, replacement=False,
          return_indices=False, sampling_strategy='auto')","CountVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=None, min_df=1,
        ngram_range=(1, 1), preprocessor=None, stop_words=None,
        strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)",1.0,50000,0.05,"{'clf': LinearSVC(C=1.0, class_weight=None, dual=False, fit_intercept=True,
     intercept_scaling=1, loss='squared_hinge', max_iter=1000,
     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,
     verbose=0), 'smp': RandomUnderSampler(random_state=1, ratio=None, replacement=False,
          return_indices=False, sampling_strategy='auto'), 'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=None, min_df=1,
        ngram_range=(1, 1), preprocessor=None, stop_words=None,
        strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None), 'vec__max_df': 1.0, 'vec__max_features': 50000, 'vec__min_df': 0.05}",0.1035611582014731,0.07561287413815306,0.1059911669041555,0.09505506641459388,0.013783453025502468,153,0.10432658978661381,0.07784265649675222,0.10840944090456013,0.09685956239597539,0.013549894238154546
31.660003980000813,1.2923874769404966,0.926984707514445,0.03758230922672736,"LinearSVC(C=1.0, class_weight=None, dual=False, fit_intercept=True,
     intercept_scaling=1, loss='squared_hinge', max_iter=1000,
     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,
     verbose=0)","RandomUnderSampler(random_state=1, ratio=None, replacement=False,
          return_indices=False, sampling_strategy='auto')","CountVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=None, min_df=1,
        ngram_range=(1, 1), preprocessor=None, stop_words=None,
        strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)",0.95,,1,"{'clf': LinearSVC(C=1.0, class_weight=None, dual=False, fit_intercept=True,
     intercept_scaling=1, loss='squared_hinge', max_iter=1000,
     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,
     verbose=0), 'smp': RandomUnderSampler(random_state=1, ratio=None, replacement=False,
          return_indices=False, sampling_strategy='auto'), 'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=None, min_df=1,
        ngram_range=(1, 1), preprocessor=None, stop_words=None,
        strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None), 'vec__max_df': 0.95, 'vec__max_features': None, 'vec__min_df': 1}",0.16609027347349656,0.17178977909810356,0.16500364863764239,0.16762790040308084,0.002976140015471585,73,0.47439002599842406,0.49269585656913245,0.4880799594056165,0.48505528065772435,0.007773346313604086
2.7844161987304688,0.1696833503958224,0.8460830052693685,0.10034143317858715,"LinearSVC(C=1.0, class_weight=None, dual=False, fit_intercept=True,
     intercept_scaling=1, loss='squared_hinge', max_iter=1000,
     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,
     verbose=0)","RandomUnderSampler(random_state=1, ratio=None, replacement=False,
          return_indices=False, sampling_strategy='auto')","CountVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=None, min_df=1,
        ngram_range=(1, 1), preprocessor=None, stop_words=None,
        strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)",0.95,,0.05,"{'clf': LinearSVC(C=1.0, class_weight=None, dual=False, fit_intercept=True,
     intercept_scaling=1, loss='squared_hinge', max_iter=1000,
     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,
     verbose=0), 'smp': RandomUnderSampler(random_state=1, ratio=None, replacement=False,
          return_indices=False, sampling_strategy='auto'), 'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=None, min_df=1,
        ngram_range=(1, 1), preprocessor=None, stop_words=None,
        strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None), 'vec__max_df': 0.95, 'vec__max_features': None, 'vec__min_df': 0.05}",0.1035611582014731,0.07561287413815306,0.1059911669041555,0.09505506641459388,0.013783453025502468,153,0.10432658978661381,0.07784265649675222,0.10840944090456013,0.09685956239597539,0.013549894238154546
6.2471241156260175,0.32023000406727065,0.8268723487854004,0.021495266029031904,"LinearSVC(C=1.0, class_weight=None, dual=False, fit_intercept=True,
     intercept_scaling=1, loss='squared_hinge', max_iter=1000,
     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,
     verbose=0)","RandomUnderSampler(random_state=1, ratio=None, replacement=False,
          return_indices=False, sampling_strategy='auto')","CountVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=None, min_df=1,
        ngram_range=(1, 1), preprocessor=None, stop_words=None,
        strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)",0.95,500,1,"{'clf': LinearSVC(C=1.0, class_weight=None, dual=False, fit_intercept=True,
     intercept_scaling=1, loss='squared_hinge', max_iter=1000,
     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,
     verbose=0), 'smp': RandomUnderSampler(random_state=1, ratio=None, replacement=False,
          return_indices=False, sampling_strategy='auto'), 'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=None, min_df=1,
        ngram_range=(1, 1), preprocessor=None, stop_words=None,
        strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None), 'vec__max_df': 0.95, 'vec__max_features': 500, 'vec__min_df': 1}",0.1622812229072431,0.17111740549319263,0.16910918668597066,0.16750260502880213,0.003782006446224214,75,0.19953454674331672,0.1993095370410474,0.19745977933231673,0.19876795437222694,0.0009295693600845171
2.466455618540446,0.1797912274055981,0.787517229715983,0.018369350608082962,"LinearSVC(C=1.0, class_weight=None, dual=False, fit_intercept=True,
     intercept_scaling=1, loss='squared_hinge', max_iter=1000,
     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,
     verbose=0)","RandomUnderSampler(random_state=1, ratio=None, replacement=False,
          return_indices=False, sampling_strategy='auto')","CountVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=None, min_df=1,
        ngram_range=(1, 1), preprocessor=None, stop_words=None,
        strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)",0.95,500,0.05,"{'clf': LinearSVC(C=1.0, class_weight=None, dual=False, fit_intercept=True,
     intercept_scaling=1, loss='squared_hinge', max_iter=1000,
     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,
     verbose=0), 'smp': RandomUnderSampler(random_state=1, ratio=None, replacement=False,
          return_indices=False, sampling_strategy='auto'), 'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=None, min_df=1,
        ngram_range=(1, 1), preprocessor=None, stop_words=None,
        strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None), 'vec__max_df': 0.95, 'vec__max_features': 500, 'vec__min_df': 0.05}",0.1035611582014731,0.07561287413815306,0.1059911669041555,0.09505506641459388,0.013783453025502468,153,0.10432658978661381,0.07784265649675222,0.10840944090456013,0.09685956239597539,0.013549894238154546
9.206096649169922,0.4990579480814444,0.9196518262227377,0.05106393747706422,"LinearSVC(C=1.0, class_weight=None, dual=False, fit_intercept=True,
     intercept_scaling=1, loss='squared_hinge', max_iter=1000,
     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,
     verbose=0)","RandomUnderSampler(random_state=1, ratio=None, replacement=False,
          return_indices=False, sampling_strategy='auto')","CountVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=None, min_df=1,
        ngram_range=(1, 1), preprocessor=None, stop_words=None,
        strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)",0.95,5000,1,"{'clf': LinearSVC(C=1.0, class_weight=None, dual=False, fit_intercept=True,
     intercept_scaling=1, loss='squared_hinge', max_iter=1000,
     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,
     verbose=0), 'smp': RandomUnderSampler(random_state=1, ratio=None, replacement=False,
          return_indices=False, sampling_strategy='auto'), 'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=None, min_df=1,
        ngram_range=(1, 1), preprocessor=None, stop_words=None,
        strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None), 'vec__max_df': 0.95, 'vec__max_features': 5000, 'vec__min_df': 1}",0.16518441016702784,0.17076936235217388,0.16503191307538512,0.16699522853152896,0.002669441691046004,81,0.3922051943452591,0.3997886660026019,0.3961201885679299,0.39603801630526364,0.0030964845448771053
2.6632142861684165,0.05966668238852043,0.7724276383717855,0.0335668740152818,"LinearSVC(C=1.0, class_weight=None, dual=False, fit_intercept=True,
     intercept_scaling=1, loss='squared_hinge', max_iter=1000,
     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,
     verbose=0)","RandomUnderSampler(random_state=1, ratio=None, replacement=False,
          return_indices=False, sampling_strategy='auto')","CountVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=None, min_df=1,
        ngram_range=(1, 1), preprocessor=None, stop_words=None,
        strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)",0.95,5000,0.05,"{'clf': LinearSVC(C=1.0, class_weight=None, dual=False, fit_intercept=True,
     intercept_scaling=1, loss='squared_hinge', max_iter=1000,
     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,
     verbose=0), 'smp': RandomUnderSampler(random_state=1, ratio=None, replacement=False,
          return_indices=False, sampling_strategy='auto'), 'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=None, min_df=1,
        ngram_range=(1, 1), preprocessor=None, stop_words=None,
        strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None), 'vec__max_df': 0.95, 'vec__max_features': 5000, 'vec__min_df': 0.05}",0.1035611582014731,0.07561287413815306,0.1059911669041555,0.09505506641459388,0.013783453025502468,153,0.10432658978661381,0.07784265649675222,0.10840944090456013,0.09685956239597539,0.013549894238154546
31.51736370722453,1.3379533910402113,0.9001516501108805,0.0403510136445101,"LinearSVC(C=1.0, class_weight=None, dual=False, fit_intercept=True,
     intercept_scaling=1, loss='squared_hinge', max_iter=1000,
     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,
     verbose=0)","RandomUnderSampler(random_state=1, ratio=None, replacement=False,
          return_indices=False, sampling_strategy='auto')","CountVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=None, min_df=1,
        ngram_range=(1, 1), preprocessor=None, stop_words=None,
        strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)",0.95,50000,1,"{'clf': LinearSVC(C=1.0, class_weight=None, dual=False, fit_intercept=True,
     intercept_scaling=1, loss='squared_hinge', max_iter=1000,
     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,
     verbose=0), 'smp': RandomUnderSampler(random_state=1, ratio=None, replacement=False,
          return_indices=False, sampling_strategy='auto'), 'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=None, min_df=1,
        ngram_range=(1, 1), preprocessor=None, stop_words=None,
        strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None), 'vec__max_df': 0.95, 'vec__max_features': 50000, 'vec__min_df': 1}",0.16706757065624614,0.17131892710041857,0.1669195483695667,0.1684353487087438,0.002039893118938452,71,0.472539087478945,0.49159378676806587,0.4865635592929922,0.48356547784666776,0.00806274397124627
2.6882389386494956,0.04575731648142594,0.7818440596262614,0.004014015501433746,"LinearSVC(C=1.0, class_weight=None, dual=False, fit_intercept=True,
     intercept_scaling=1, loss='squared_hinge', max_iter=1000,
     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,
     verbose=0)","RandomUnderSampler(random_state=1, ratio=None, replacement=False,
          return_indices=False, sampling_strategy='auto')","CountVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=None, min_df=1,
        ngram_range=(1, 1), preprocessor=None, stop_words=None,
        strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)",0.95,50000,0.05,"{'clf': LinearSVC(C=1.0, class_weight=None, dual=False, fit_intercept=True,
     intercept_scaling=1, loss='squared_hinge', max_iter=1000,
     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,
     verbose=0), 'smp': RandomUnderSampler(random_state=1, ratio=None, replacement=False,
          return_indices=False, sampling_strategy='auto'), 'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=None, min_df=1,
        ngram_range=(1, 1), preprocessor=None, stop_words=None,
        strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None), 'vec__max_df': 0.95, 'vec__max_features': 50000, 'vec__min_df': 0.05}",0.1035611582014731,0.07561287413815306,0.1059911669041555,0.09505506641459388,0.013783453025502468,153,0.10432658978661381,0.07784265649675222,0.10840944090456013,0.09685956239597539,0.013549894238154546
9.445324103037516,1.6735798743025028,0.906191349029541,0.027431257110618883,"LinearSVC(C=1.0, class_weight=None, dual=False, fit_intercept=True,
     intercept_scaling=1, loss='squared_hinge', max_iter=1000,
     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,
     verbose=0)","RandomUnderSampler(random_state=1, ratio=None, replacement=False,
          return_indices=False, sampling_strategy='auto')","TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=5000, min_df=1,
        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,
        stop_words=None, strip_accents=None, sublinear_tf=False,
        token_pattern='(?u)\\b\\w\\w+\\b', tokenizer=None, use_idf=True,
        vocabulary=None)",1.0,,1,"{'clf': LinearSVC(C=1.0, class_weight=None, dual=False, fit_intercept=True,
     intercept_scaling=1, loss='squared_hinge', max_iter=1000,
     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,
     verbose=0), 'smp': RandomUnderSampler(random_state=1, ratio=None, replacement=False,
          return_indices=False, sampling_strategy='auto'), 'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=5000, min_df=1,
        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,
        stop_words=None, strip_accents=None, sublinear_tf=False,
        token_pattern='(?u)\\b\\w\\w+\\b', tokenizer=None, use_idf=True,
        vocabulary=None), 'vec__max_df': 1.0, 'vec__max_features': None, 'vec__min_df': 1}",0.17871673719821976,0.18809058803190717,0.18053389073569318,0.1824470719886067,0.004058938017341513,49,0.4721881266353458,0.4902473389331301,0.4847453684658592,0.4823936113447784,0.007557859413507971
2.429315169652303,0.12535424824805488,0.7547484238942465,0.006130656438878158,"LinearSVC(C=1.0, class_weight=None, dual=False, fit_intercept=True,
     intercept_scaling=1, loss='squared_hinge', max_iter=1000,
     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,
     verbose=0)","RandomUnderSampler(random_state=1, ratio=None, replacement=False,
          return_indices=False, sampling_strategy='auto')","TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=5000, min_df=1,
        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,
        stop_words=None, strip_accents=None, sublinear_tf=False,
        token_pattern='(?u)\\b\\w\\w+\\b', tokenizer=None, use_idf=True,
        vocabulary=None)",1.0,,0.05,"{'clf': LinearSVC(C=1.0, class_weight=None, dual=False, fit_intercept=True,
     intercept_scaling=1, loss='squared_hinge', max_iter=1000,
     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,
     verbose=0), 'smp': RandomUnderSampler(random_state=1, ratio=None, replacement=False,
          return_indices=False, sampling_strategy='auto'), 'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=5000, min_df=1,
        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,
        stop_words=None, strip_accents=None, sublinear_tf=False,
        token_pattern='(?u)\\b\\w\\w+\\b', tokenizer=None, use_idf=True,
        vocabulary=None), 'vec__max_df': 1.0, 'vec__max_features': None, 'vec__min_df': 0.05}",0.0956646009161952,0.07294016035839014,0.10409957033234189,0.09090144386897575,0.013159102205281287,185,0.09644568600549863,0.07448320971337322,0.1066895699726076,0.09253948856382649,0.013435182522687526
3.8111223379770913,0.2698084019870202,0.8907284736633301,0.05488087757158441,"LinearSVC(C=1.0, class_weight=None, dual=False, fit_intercept=True,
     intercept_scaling=1, loss='squared_hinge', max_iter=1000,
     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,
     verbose=0)","RandomUnderSampler(random_state=1, ratio=None, replacement=False,
          return_indices=False, sampling_strategy='auto')","TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=5000, min_df=1,
        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,
        stop_words=None, strip_accents=None, sublinear_tf=False,
        token_pattern='(?u)\\b\\w\\w+\\b', tokenizer=None, use_idf=True,
        vocabulary=None)",1.0,500,1,"{'clf': LinearSVC(C=1.0, class_weight=None, dual=False, fit_intercept=True,
     intercept_scaling=1, loss='squared_hinge', max_iter=1000,
     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,
     verbose=0), 'smp': RandomUnderSampler(random_state=1, ratio=None, replacement=False,
          return_indices=False, sampling_strategy='auto'), 'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=5000, min_df=1,
        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,
        stop_words=None, strip_accents=None, sublinear_tf=False,
        token_pattern='(?u)\\b\\w\\w+\\b', tokenizer=None, use_idf=True,
        vocabulary=None), 'vec__max_df': 1.0, 'vec__max_features': 500, 'vec__min_df': 1}",0.1627266173267235,0.17022688701255012,0.1683585005833878,0.16710400164088715,0.0031878767267659418,77,0.1969094765178758,0.1948338404537917,0.19634320898991142,0.19602884198719298,0.0008760465082518009
2.4553439617156982,0.145177862035814,0.7904663880666097,0.02402144432508561,"LinearSVC(C=1.0, class_weight=None, dual=False, fit_intercept=True,
     intercept_scaling=1, loss='squared_hinge', max_iter=1000,
     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,
     verbose=0)","RandomUnderSampler(random_state=1, ratio=None, replacement=False,
          return_indices=False, sampling_strategy='auto')","TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=5000, min_df=1,
        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,
        stop_words=None, strip_accents=None, sublinear_tf=False,
        token_pattern='(?u)\\b\\w\\w+\\b', tokenizer=None, use_idf=True,
        vocabulary=None)",1.0,500,0.05,"{'clf': LinearSVC(C=1.0, class_weight=None, dual=False, fit_intercept=True,
     intercept_scaling=1, loss='squared_hinge', max_iter=1000,
     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,
     verbose=0), 'smp': RandomUnderSampler(random_state=1, ratio=None, replacement=False,
          return_indices=False, sampling_strategy='auto'), 'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=5000, min_df=1,
        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,
        stop_words=None, strip_accents=None, sublinear_tf=False,
        token_pattern='(?u)\\b\\w\\w+\\b', tokenizer=None, use_idf=True,
        vocabulary=None), 'vec__max_df': 1.0, 'vec__max_features': 500, 'vec__min_df': 0.05}",0.0956646009161952,0.07294016035839014,0.10409957033234189,0.09090144386897575,0.013159102205281287,185,0.09644568600549863,0.07448320971337322,0.1066895699726076,0.09253948856382649,0.013435182522687526
4.128265937169393,0.04710574338006807,0.854560375213623,0.007548499861068655,"LinearSVC(C=1.0, class_weight=None, dual=False, fit_intercept=True,
     intercept_scaling=1, loss='squared_hinge', max_iter=1000,
     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,
     verbose=0)","RandomUnderSampler(random_state=1, ratio=None, replacement=False,
          return_indices=False, sampling_strategy='auto')","TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=5000, min_df=1,
        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,
        stop_words=None, strip_accents=None, sublinear_tf=False,
        token_pattern='(?u)\\b\\w\\w+\\b', tokenizer=None, use_idf=True,
        vocabulary=None)",1.0,5000,1,"{'clf': LinearSVC(C=1.0, class_weight=None, dual=False, fit_intercept=True,
     intercept_scaling=1, loss='squared_hinge', max_iter=1000,
     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,
     verbose=0), 'smp': RandomUnderSampler(random_state=1, ratio=None, replacement=False,
          return_indices=False, sampling_strategy='auto'), 'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=5000, min_df=1,
        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,
        stop_words=None, strip_accents=None, sublinear_tf=False,
        token_pattern='(?u)\\b\\w\\w+\\b', tokenizer=None, use_idf=True,
        vocabulary=None), 'vec__max_df': 1.0, 'vec__max_features': 5000, 'vec__min_df': 1}",0.17801197366303884,0.18446838324733686,0.17337821924568358,0.1786195253853531,0.004547876699981857,61,0.3749049274765379,0.3785414064485705,0.373164637388302,0.3755369904378035,0.0022400950979988747
2.3395389715830484,0.1020897876229617,0.7794876098632812,0.008288119331405651,"LinearSVC(C=1.0, class_weight=None, dual=False, fit_intercept=True,
     intercept_scaling=1, loss='squared_hinge', max_iter=1000,
     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,
     verbose=0)","RandomUnderSampler(random_state=1, ratio=None, replacement=False,
          return_indices=False, sampling_strategy='auto')","TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=5000, min_df=1,
        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,
        stop_words=None, strip_accents=None, sublinear_tf=False,
        token_pattern='(?u)\\b\\w\\w+\\b', tokenizer=None, use_idf=True,
        vocabulary=None)",1.0,5000,0.05,"{'clf': LinearSVC(C=1.0, class_weight=None, dual=False, fit_intercept=True,
     intercept_scaling=1, loss='squared_hinge', max_iter=1000,
     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,
     verbose=0), 'smp': RandomUnderSampler(random_state=1, ratio=None, replacement=False,
          return_indices=False, sampling_strategy='auto'), 'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=5000, min_df=1,
        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,
        stop_words=None, strip_accents=None, sublinear_tf=False,
        token_pattern='(?u)\\b\\w\\w+\\b', tokenizer=None, use_idf=True,
        vocabulary=None), 'vec__max_df': 1.0, 'vec__max_features': 5000, 'vec__min_df': 0.05}",0.0956646009161952,0.07294016035839014,0.10409957033234189,0.09090144386897575,0.013159102205281287,185,0.09644568600549863,0.07448320971337322,0.1066895699726076,0.09253948856382649,0.013435182522687526
8.182739019393921,0.1498061356904411,0.9008518060048422,0.03363944355054059,"LinearSVC(C=1.0, class_weight=None, dual=False, fit_intercept=True,
     intercept_scaling=1, loss='squared_hinge', max_iter=1000,
     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,
     verbose=0)","RandomUnderSampler(random_state=1, ratio=None, replacement=False,
          return_indices=False, sampling_strategy='auto')","TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=5000, min_df=1,
        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,
        stop_words=None, strip_accents=None, sublinear_tf=False,
        token_pattern='(?u)\\b\\w\\w+\\b', tokenizer=None, use_idf=True,
        vocabulary=None)",1.0,50000,1,"{'clf': LinearSVC(C=1.0, class_weight=None, dual=False, fit_intercept=True,
     intercept_scaling=1, loss='squared_hinge', max_iter=1000,
     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,
     verbose=0), 'smp': RandomUnderSampler(random_state=1, ratio=None, replacement=False,
          return_indices=False, sampling_strategy='auto'), 'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=5000, min_df=1,
        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,
        stop_words=None, strip_accents=None, sublinear_tf=False,
        token_pattern='(?u)\\b\\w\\w+\\b', tokenizer=None, use_idf=True,
        vocabulary=None), 'vec__max_df': 1.0, 'vec__max_features': 50000, 'vec__min_df': 1}",0.17801672182332764,0.18663503831518283,0.18076487491753326,0.18180554501868124,0.0035945412765296648,51,0.46975372022666007,0.4877886179482638,0.48279972400860544,0.4801140207278431,0.007603689259887814
2.397772947947184,0.09746852906417251,0.7670494715372721,0.008331533882742772,"LinearSVC(C=1.0, class_weight=None, dual=False, fit_intercept=True,
     intercept_scaling=1, loss='squared_hinge', max_iter=1000,
     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,
     verbose=0)","RandomUnderSampler(random_state=1, ratio=None, replacement=False,
          return_indices=False, sampling_strategy='auto')","TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=5000, min_df=1,
        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,
        stop_words=None, strip_accents=None, sublinear_tf=False,
        token_pattern='(?u)\\b\\w\\w+\\b', tokenizer=None, use_idf=True,
        vocabulary=None)",1.0,50000,0.05,"{'clf': LinearSVC(C=1.0, class_weight=None, dual=False, fit_intercept=True,
     intercept_scaling=1, loss='squared_hinge', max_iter=1000,
     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,
     verbose=0), 'smp': RandomUnderSampler(random_state=1, ratio=None, replacement=False,
          return_indices=False, sampling_strategy='auto'), 'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=5000, min_df=1,
        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,
        stop_words=None, strip_accents=None, sublinear_tf=False,
        token_pattern='(?u)\\b\\w\\w+\\b', tokenizer=None, use_idf=True,
        vocabulary=None), 'vec__max_df': 1.0, 'vec__max_features': 50000, 'vec__min_df': 0.05}",0.0956646009161952,0.07294016035839014,0.10409957033234189,0.09090144386897575,0.013159102205281287,185,0.09644568600549863,0.07448320971337322,0.1066895699726076,0.09253948856382649,0.013435182522687526
9.510598738988241,0.2968120839510223,0.9536678791046143,0.0790184741447172,"LinearSVC(C=1.0, class_weight=None, dual=False, fit_intercept=True,
     intercept_scaling=1, loss='squared_hinge', max_iter=1000,
     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,
     verbose=0)","RandomUnderSampler(random_state=1, ratio=None, replacement=False,
          return_indices=False, sampling_strategy='auto')","TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=5000, min_df=1,
        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,
        stop_words=None, strip_accents=None, sublinear_tf=False,
        token_pattern='(?u)\\b\\w\\w+\\b', tokenizer=None, use_idf=True,
        vocabulary=None)",0.95,,1,"{'clf': LinearSVC(C=1.0, class_weight=None, dual=False, fit_intercept=True,
     intercept_scaling=1, loss='squared_hinge', max_iter=1000,
     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,
     verbose=0), 'smp': RandomUnderSampler(random_state=1, ratio=None, replacement=False,
          return_indices=False, sampling_strategy='auto'), 'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=5000, min_df=1,
        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,
        stop_words=None, strip_accents=None, sublinear_tf=False,
        token_pattern='(?u)\\b\\w\\w+\\b', tokenizer=None, use_idf=True,
        vocabulary=None), 'vec__max_df': 0.95, 'vec__max_features': None, 'vec__min_df': 1}",0.17871673719821976,0.18809058803190717,0.18053389073569318,0.1824470719886067,0.004058938017341513,49,0.4721881266353458,0.4902473389331301,0.4847453684658592,0.4823936113447784,0.007557859413507971
2.543245792388916,0.11696595969413248,0.8159723281860352,0.029859918194306938,"LinearSVC(C=1.0, class_weight=None, dual=False, fit_intercept=True,
     intercept_scaling=1, loss='squared_hinge', max_iter=1000,
     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,
     verbose=0)","RandomUnderSampler(random_state=1, ratio=None, replacement=False,
          return_indices=False, sampling_strategy='auto')","TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=5000, min_df=1,
        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,
        stop_words=None, strip_accents=None, sublinear_tf=False,
        token_pattern='(?u)\\b\\w\\w+\\b', tokenizer=None, use_idf=True,
        vocabulary=None)",0.95,,0.05,"{'clf': LinearSVC(C=1.0, class_weight=None, dual=False, fit_intercept=True,
     intercept_scaling=1, loss='squared_hinge', max_iter=1000,
     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,
     verbose=0), 'smp': RandomUnderSampler(random_state=1, ratio=None, replacement=False,
          return_indices=False, sampling_strategy='auto'), 'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=5000, min_df=1,
        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,
        stop_words=None, strip_accents=None, sublinear_tf=False,
        token_pattern='(?u)\\b\\w\\w+\\b', tokenizer=None, use_idf=True,
        vocabulary=None), 'vec__max_df': 0.95, 'vec__max_features': None, 'vec__min_df': 0.05}",0.0956646009161952,0.07294016035839014,0.10409957033234189,0.09090144386897575,0.013159102205281287,185,0.09644568600549863,0.07448320971337322,0.1066895699726076,0.09253948856382649,0.013435182522687526
3.917698303858439,0.21684546025976426,0.945881207784017,0.13911724258066188,"LinearSVC(C=1.0, class_weight=None, dual=False, fit_intercept=True,
     intercept_scaling=1, loss='squared_hinge', max_iter=1000,
     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,
     verbose=0)","RandomUnderSampler(random_state=1, ratio=None, replacement=False,
          return_indices=False, sampling_strategy='auto')","TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=5000, min_df=1,
        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,
        stop_words=None, strip_accents=None, sublinear_tf=False,
        token_pattern='(?u)\\b\\w\\w+\\b', tokenizer=None, use_idf=True,
        vocabulary=None)",0.95,500,1,"{'clf': LinearSVC(C=1.0, class_weight=None, dual=False, fit_intercept=True,
     intercept_scaling=1, loss='squared_hinge', max_iter=1000,
     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,
     verbose=0), 'smp': RandomUnderSampler(random_state=1, ratio=None, replacement=False,
          return_indices=False, sampling_strategy='auto'), 'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=5000, min_df=1,
        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,
        stop_words=None, strip_accents=None, sublinear_tf=False,
        token_pattern='(?u)\\b\\w\\w+\\b', tokenizer=None, use_idf=True,
        vocabulary=None), 'vec__max_df': 0.95, 'vec__max_features': 500, 'vec__min_df': 1}",0.1627266173267235,0.17022688701255012,0.1683585005833878,0.16710400164088715,0.0031878767267659418,77,0.1969094765178758,0.1948338404537917,0.19634320898991142,0.19602884198719298,0.0008760465082518009
2.5405003229777017,0.08526554948686198,0.8167970975240072,0.10259377379185214,"LinearSVC(C=1.0, class_weight=None, dual=False, fit_intercept=True,
     intercept_scaling=1, loss='squared_hinge', max_iter=1000,
     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,
     verbose=0)","RandomUnderSampler(random_state=1, ratio=None, replacement=False,
          return_indices=False, sampling_strategy='auto')","TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=5000, min_df=1,
        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,
        stop_words=None, strip_accents=None, sublinear_tf=False,
        token_pattern='(?u)\\b\\w\\w+\\b', tokenizer=None, use_idf=True,
        vocabulary=None)",0.95,500,0.05,"{'clf': LinearSVC(C=1.0, class_weight=None, dual=False, fit_intercept=True,
     intercept_scaling=1, loss='squared_hinge', max_iter=1000,
     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,
     verbose=0), 'smp': RandomUnderSampler(random_state=1, ratio=None, replacement=False,
          return_indices=False, sampling_strategy='auto'), 'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=5000, min_df=1,
        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,
        stop_words=None, strip_accents=None, sublinear_tf=False,
        token_pattern='(?u)\\b\\w\\w+\\b', tokenizer=None, use_idf=True,
        vocabulary=None), 'vec__max_df': 0.95, 'vec__max_features': 500, 'vec__min_df': 0.05}",0.0956646009161952,0.07294016035839014,0.10409957033234189,0.09090144386897575,0.013159102205281287,185,0.09644568600549863,0.07448320971337322,0.1066895699726076,0.09253948856382649,0.013435182522687526
4.1563780307769775,0.05882474572416157,0.8679614067077637,0.017583801516399367,"LinearSVC(C=1.0, class_weight=None, dual=False, fit_intercept=True,
     intercept_scaling=1, loss='squared_hinge', max_iter=1000,
     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,
     verbose=0)","RandomUnderSampler(random_state=1, ratio=None, replacement=False,
          return_indices=False, sampling_strategy='auto')","TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=5000, min_df=1,
        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,
        stop_words=None, strip_accents=None, sublinear_tf=False,
        token_pattern='(?u)\\b\\w\\w+\\b', tokenizer=None, use_idf=True,
        vocabulary=None)",0.95,5000,1,"{'clf': LinearSVC(C=1.0, class_weight=None, dual=False, fit_intercept=True,
     intercept_scaling=1, loss='squared_hinge', max_iter=1000,
     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,
     verbose=0), 'smp': RandomUnderSampler(random_state=1, ratio=None, replacement=False,
          return_indices=False, sampling_strategy='auto'), 'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=5000, min_df=1,
        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,
        stop_words=None, strip_accents=None, sublinear_tf=False,
        token_pattern='(?u)\\b\\w\\w+\\b', tokenizer=None, use_idf=True,
        vocabulary=None), 'vec__max_df': 0.95, 'vec__max_features': 5000, 'vec__min_df': 1}",0.17801197366303884,0.18446838324733686,0.17337821924568358,0.1786195253853531,0.004547876699981857,61,0.3749049274765379,0.3785414064485705,0.373164637388302,0.3755369904378035,0.0022400950979988747
2.360621372858683,0.017968425222457708,0.7691922982533773,0.029150040734273547,"LinearSVC(C=1.0, class_weight=None, dual=False, fit_intercept=True,
     intercept_scaling=1, loss='squared_hinge', max_iter=1000,
     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,
     verbose=0)","RandomUnderSampler(random_state=1, ratio=None, replacement=False,
          return_indices=False, sampling_strategy='auto')","TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=5000, min_df=1,
        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,
        stop_words=None, strip_accents=None, sublinear_tf=False,
        token_pattern='(?u)\\b\\w\\w+\\b', tokenizer=None, use_idf=True,
        vocabulary=None)",0.95,5000,0.05,"{'clf': LinearSVC(C=1.0, class_weight=None, dual=False, fit_intercept=True,
     intercept_scaling=1, loss='squared_hinge', max_iter=1000,
     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,
     verbose=0), 'smp': RandomUnderSampler(random_state=1, ratio=None, replacement=False,
          return_indices=False, sampling_strategy='auto'), 'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=5000, min_df=1,
        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,
        stop_words=None, strip_accents=None, sublinear_tf=False,
        token_pattern='(?u)\\b\\w\\w+\\b', tokenizer=None, use_idf=True,
        vocabulary=None), 'vec__max_df': 0.95, 'vec__max_features': 5000, 'vec__min_df': 0.05}",0.0956646009161952,0.07294016035839014,0.10409957033234189,0.09090144386897575,0.013159102205281287,185,0.09644568600549863,0.07448320971337322,0.1066895699726076,0.09253948856382649,0.013435182522687526
8.208918333053589,0.17393110989097157,0.9157310326894125,0.03798075605783784,"LinearSVC(C=1.0, class_weight=None, dual=False, fit_intercept=True,
     intercept_scaling=1, loss='squared_hinge', max_iter=1000,
     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,
     verbose=0)","RandomUnderSampler(random_state=1, ratio=None, replacement=False,
          return_indices=False, sampling_strategy='auto')","TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=5000, min_df=1,
        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,
        stop_words=None, strip_accents=None, sublinear_tf=False,
        token_pattern='(?u)\\b\\w\\w+\\b', tokenizer=None, use_idf=True,
        vocabulary=None)",0.95,50000,1,"{'clf': LinearSVC(C=1.0, class_weight=None, dual=False, fit_intercept=True,
     intercept_scaling=1, loss='squared_hinge', max_iter=1000,
     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,
     verbose=0), 'smp': RandomUnderSampler(random_state=1, ratio=None, replacement=False,
          return_indices=False, sampling_strategy='auto'), 'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=5000, min_df=1,
        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,
        stop_words=None, strip_accents=None, sublinear_tf=False,
        token_pattern='(?u)\\b\\w\\w+\\b', tokenizer=None, use_idf=True,
        vocabulary=None), 'vec__max_df': 0.95, 'vec__max_features': 50000, 'vec__min_df': 1}",0.17801672182332764,0.18663503831518283,0.18076487491753326,0.18180554501868124,0.0035945412765296648,51,0.46975372022666007,0.4877886179482638,0.48279972400860544,0.4801140207278431,0.007603689259887814
2.378446658452352,0.17255474228738077,0.6663262844085693,0.18215138545487627,"LinearSVC(C=1.0, class_weight=None, dual=False, fit_intercept=True,
     intercept_scaling=1, loss='squared_hinge', max_iter=1000,
     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,
     verbose=0)","RandomUnderSampler(random_state=1, ratio=None, replacement=False,
          return_indices=False, sampling_strategy='auto')","TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=5000, min_df=1,
        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,
        stop_words=None, strip_accents=None, sublinear_tf=False,
        token_pattern='(?u)\\b\\w\\w+\\b', tokenizer=None, use_idf=True,
        vocabulary=None)",0.95,50000,0.05,"{'clf': LinearSVC(C=1.0, class_weight=None, dual=False, fit_intercept=True,
     intercept_scaling=1, loss='squared_hinge', max_iter=1000,
     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,
     verbose=0), 'smp': RandomUnderSampler(random_state=1, ratio=None, replacement=False,
          return_indices=False, sampling_strategy='auto'), 'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=5000, min_df=1,
        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,
        stop_words=None, strip_accents=None, sublinear_tf=False,
        token_pattern='(?u)\\b\\w\\w+\\b', tokenizer=None, use_idf=True,
        vocabulary=None), 'vec__max_df': 0.95, 'vec__max_features': 50000, 'vec__min_df': 0.05}",0.0956646009161952,0.07294016035839014,0.10409957033234189,0.09090144386897575,0.013159102205281287,185,0.09644568600549863,0.07448320971337322,0.1066895699726076,0.09253948856382649,0.013435182522687526
